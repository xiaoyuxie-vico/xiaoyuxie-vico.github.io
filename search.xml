<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CS61B-Week2-Notes</title>
    <url>/2021/04/24/CS61B-Week2/</url>
    <content><![CDATA[<h1 id="Week-2-Notes"><a href="#Week-2-Notes" class="headerlink" title="Week 2 - Notes"></a>Week 2 - Notes</h1><h2 id="Exercise-B-Level"><a href="#Exercise-B-Level" class="headerlink" title="Exercise B Level"></a>Exercise B Level</h2><ol>
<li>Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method <code>deleteFirst</code>, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above.</li>
<li>Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants.</li>
</ol>
<a id="more"></a>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Code for the question 1 and 2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> edu.princeton.cs.algs4.In;</span><br><span class="line"></span><br><span class="line"><span class="comment">//import java.util.Arrays;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Using sentinel to replace the first</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SLList2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntNode</span> </span>&#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">int</span> item;</span><br><span class="line">        <span class="keyword">public</span> IntNode next;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">IntNode</span><span class="params">(<span class="keyword">int</span> i, IntNode n)</span> </span>&#123;</span><br><span class="line">            item = i;</span><br><span class="line">            next = n;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntNode sentinel;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> size;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SLList2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// initialize with no inputs</span></span><br><span class="line">        sentinel = <span class="keyword">new</span> IntNode(<span class="number">63</span>, <span class="keyword">null</span>);</span><br><span class="line">        size = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SLList2</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// initialize with a integer</span></span><br><span class="line">        sentinel = <span class="keyword">new</span> IntNode(<span class="number">63</span>, <span class="keyword">null</span>);</span><br><span class="line">        sentinel.next = <span class="keyword">new</span> IntNode(x, <span class="keyword">null</span>);</span><br><span class="line">        size = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SLList2</span><span class="params">(<span class="keyword">int</span>[] x)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// initialize with an array</span></span><br><span class="line">        size = <span class="number">0</span>;</span><br><span class="line">        sentinel = <span class="keyword">new</span> IntNode(<span class="number">63</span>, <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; x.length; i++) &#123;</span><br><span class="line">            <span class="comment">// get the item from array inversely</span></span><br><span class="line">            sentinel.next = <span class="keyword">new</span> IntNode(x[x.length-i-<span class="number">1</span>], <span class="keyword">null</span>);</span><br><span class="line">            size += <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Add the first item in the list */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFirst</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        sentinel.next = <span class="keyword">new</span> IntNode(x, sentinel.next);</span><br><span class="line">        size += <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Returns the first item in the list */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getFirst</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sentinel.next.item;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns the last item in the list</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the last item</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getLast</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (sentinel.next == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> sentinel.item;</span><br><span class="line">        &#125;</span><br><span class="line">        sentinel = sentinel.next;</span><br><span class="line">        <span class="keyword">return</span> getLast();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Add an item to a list</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> args int x</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addLast</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        size += <span class="number">1</span>;</span><br><span class="line">        IntNode p = sentinel;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Advance p to the end of the list. */</span></span><br><span class="line">        <span class="keyword">while</span> (p.next != <span class="keyword">null</span>) &#123;</span><br><span class="line">            p = p.next;</span><br><span class="line">        &#125;</span><br><span class="line">        p.next = <span class="keyword">new</span> IntNode(x, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> size;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">deleteFirst</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">/* sentinel.next or sentinel.next.next</span></span><br><span class="line"><span class="comment">        could be null when size == 0 */</span></span><br><span class="line">        <span class="keyword">if</span> (sentinel.next == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        IntNode deleteNode = sentinel.next;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (sentinel.next.next == <span class="keyword">null</span>) &#123;</span><br><span class="line">            sentinel.next = <span class="keyword">new</span> IntNode(-<span class="number">1</span>, <span class="keyword">null</span>);</span><br><span class="line">            <span class="keyword">return</span> deleteNode.item;</span><br><span class="line">        &#125;</span><br><span class="line">        sentinel.next = sentinel.next.next;</span><br><span class="line">        <span class="keyword">return</span> deleteNode.item;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/** Test the constructor that takes in an array of integers*/</span></span><br><span class="line">        <span class="keyword">int</span>[] arr = <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;;</span><br><span class="line">        SLList2 L = <span class="keyword">new</span> SLList2(arr);</span><br><span class="line">        System.out.println(L.getFirst());</span><br><span class="line">      </span><br><span class="line">        SLList2 L = <span class="keyword">new</span> SLList2(<span class="number">1</span>);</span><br><span class="line">        L.addFirst(<span class="number">2</span>);</span><br><span class="line">        System.out.println(L.getFirst());</span><br><span class="line">        L.addFirst(<span class="number">3</span>);</span><br><span class="line">        L.deleteFirst();</span><br><span class="line">        System.out.print(<span class="string">&quot;Final: &quot;</span>);</span><br><span class="line">        System.out.println(L.getFirst());</span><br><span class="line"><span class="comment">//        L.addLast(100);</span></span><br><span class="line"><span class="comment">//        System.out.println(L.getLast());</span></span><br><span class="line"><span class="comment">//        System.out.println(L.size());</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Exercise-A-Level"><a href="#Exercise-A-Level" class="headerlink" title="Exercise A Level"></a>Exercise A Level</h1><p><img src="CS61B-Week2/image-20210425113258793.png" alt="image-20210425113258793"></p>
<p><a href="https://www.kartikkapur.com/documents/mt1.pdf#page=7">Problem Link: Osmosis</a></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IntList2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> first;</span><br><span class="line">    <span class="keyword">public</span> IntList2 rest;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">IntList2</span><span class="params">(<span class="keyword">int</span> f, IntList2 r)</span></span>&#123;</span><br><span class="line">        first = f;</span><br><span class="line">        rest = r;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterative</span></span><br><span class="line">    <span class="comment">// Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addAdjacentIter</span><span class="params">(IntList2 p)</span></span>&#123;</span><br><span class="line">        <span class="comment">/* if p == null, p.rest will no longer execute */</span></span><br><span class="line">        <span class="keyword">if</span> (p.rest == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">/* size &lt;= 1 */</span></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * p.rest != null</span></span><br><span class="line"><span class="comment">         * p ends at the last node finally</span></span><br><span class="line"><span class="comment">         * loop through 1st ~ last 2nd node</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">while</span> (p.rest != <span class="keyword">null</span>) &#123; <span class="comment">/* p ends at the last node */</span></span><br><span class="line">            <span class="keyword">if</span> (p.first == p.rest.first) &#123;</span><br><span class="line">                <span class="comment">/* merge */</span></span><br><span class="line">                p.first *= <span class="number">2</span>;</span><br><span class="line">                p.rest = p.rest.rest; <span class="comment">/* it&#x27;s okay if it is null */</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                p = p.rest;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// recursion</span></span><br><span class="line">    <span class="comment">// Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addAdjacentRec</span><span class="params">(IntList2 p)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (p == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        adj(p, p.rest);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// helper function - pass previous node recursively</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">adj</span><span class="params">(IntList2 prev, IntList2 current)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (current == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">if</span> (prev.first == current.first) &#123;</span><br><span class="line">            prev.first *= <span class="number">2</span>;</span><br><span class="line">            prev.rest = current.rest; <span class="comment">// maybe null</span></span><br><span class="line">            adj(prev, prev.rest); <span class="comment">// I fixed this part that is wrong in the reference link.</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            adj(current, current.rest);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Display an IntList</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">display</span><span class="params">(IntList2 L)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (L.rest != <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.print(L.first);</span><br><span class="line">            System.out.print(<span class="string">&quot;, &quot;</span>);</span><br><span class="line">            L.first = L.rest.first;</span><br><span class="line">            L.rest = L.rest.rest;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(L.first);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        IntList2 L = <span class="keyword">new</span> IntList2(<span class="number">3</span>, <span class="keyword">null</span>);</span><br><span class="line">        L =<span class="keyword">new</span> IntList2(<span class="number">2</span>, L);</span><br><span class="line">        L =<span class="keyword">new</span> IntList2(<span class="number">1</span>, L);</span><br><span class="line">        L =<span class="keyword">new</span> IntList2(<span class="number">1</span>, L);</span><br><span class="line"></span><br><span class="line">      	<span class="comment">// There are two methods.</span></span><br><span class="line">        <span class="comment">// Method 1: Recursive</span></span><br><span class="line">        L.addAdjacentRec(L);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Method 2: Iterative</span></span><br><span class="line"><span class="comment">//        L.addAdjacentIter(L);</span></span><br><span class="line">        System.out.print(<span class="string">&quot;Final: &quot;</span>);</span><br><span class="line">        L.display(L);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Reading-2-3-The-DLList"><a href="#Reading-2-3-The-DLList" class="headerlink" title="Reading 2.3: The DLList"></a>Reading 2.3: The DLList</h1><h2 id="addLast"><a href="#addLast" class="headerlink" title="addLast"></a>addLast</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SLList</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntNode sentinel;</span><br><span class="line">    <span class="keyword">private</span> IntNode last;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> size;    </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addLast</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        last.next = <span class="keyword">new</span> IntNode(x, <span class="keyword">null</span>);</span><br><span class="line">        last = last.next;</span><br><span class="line">        size += <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p><strong>Exercise 2.3.1:</strong> Consider the box and pointer diagram representing the <code>SLList</code> implementation above, which includes the last pointer. Suppose that we’d like to support <code>addLast</code>, <code>getLast</code>, and <code>removeLast</code> operations. Will the structure shown support rapid <code>addLast</code>, <code>getLast</code>, and <code>removeLast</code> operations? If not, which operations are slow?</p>
<p><img src="sllist_last_pointer.png" alt="sllist_last_pointer.png"></p>
<p><strong>Answer 2.3.1:</strong> <code>addLast</code> and <code>getLast</code> will be fast, but <code>removeLast</code> will be slow. That’s because we have no easy way to get the second-to-last node, to update the <code>last</code> pointer, after removing the last node.</p>
<h2 id="Week-2-discussion-1"><a href="#Week-2-discussion-1" class="headerlink" title="Week 2 - discussion 1"></a>Week 2 - discussion 1</h2><p>Implement square and squareMutative which are static methods that both take in an IntList L and return an IntList with its integer values all squared. square does this non-mutatively with recursion by creating new IntLists while squareMutative uses a recursive approach to change the instance variables of the input IntList L.</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> IntList <span class="title">square</span><span class="params">(IntList L)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (L == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> L;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        IntList rest = square(L.rest);</span><br><span class="line">        IntList M = <span class="keyword">new</span> IntList(L.first * L.first, rest);</span><br><span class="line">        <span class="keyword">return</span> M;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> IntList <span class="title">squareMutative</span><span class="params">(IntList L)</span> </span>&#123;</span><br><span class="line">    IntList B = L;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (B != <span class="keyword">null</span>) &#123;</span><br><span class="line">        B.first *= B.first;</span><br><span class="line">        B = B.rest</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> L;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h1><ul>
<li><a href="https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html">CS 61B | Part 1 | List (Linked List &amp; Array List)</a></li>
<li><a href="https://sp18.datastructur.es/materials/discussion/disc02sol.pdf">CS 61B week discussion 1 solution</a></li>
</ul>
]]></content>
      <categories>
        <category>Class notes</category>
      </categories>
      <tags>
        <tag>Online Course</tag>
        <tag>Algorithm</tag>
        <tag>Data structure</tag>
        <tag>Notes</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers</title>
    <url>/2021/02/17/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers/</url>
    <content><![CDATA[<h1 id="Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers"><a href="#Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers" class="headerlink" title="Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers"></a>Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers</h1><p>Recently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field.</p>
<p>Note that please check the lastest verison in <a href="https://github.com/xiaoyuxie-vico/ML_PDE_Resources">github</a>.</p>
<a id="more"></a>

<h1 id="Model-Zoo"><a href="#Model-Zoo" class="headerlink" title="Model Zoo"></a>Model Zoo</h1><table>
<thead>
<tr>
<th>Model</th>
<th>Relevant Papers</th>
<th>Link</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>HiDeNN</td>
<td>Saha, Sourav, et al. “<strong>Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.</strong>“ Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452.</td>
<td><a href="https://www.sciencedirect.com/science/article/pii/S004578252030637X">Paper</a></td>
<td></td>
</tr>
<tr>
<td>HiTSs</td>
<td>Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. “<strong>Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.</strong>“ arXiv preprint arXiv:2008.09768 (2020).</td>
<td><a href="http://arxiv.org/abs/2102.01010">Paper</a>, <a href="https://github.com/luckystarufo/multiscale_HiTS">Code</a>, <a href="https://www.youtube.com/watch?v=Jfl3dIlSTrU">Video</a></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kochkov, Dmitrii, et al. “<strong>Machine learning accelerated computational fluid dynamics.</strong>“ arXiv preprint arXiv:2102.01010 (2021).</td>
<td><a href="http://arxiv.org/abs/2102.01010">Paper</a></td>
<td>Google</td>
</tr>
<tr>
<td>Fourier Neural Operator</td>
<td>Li, Zongyi, et al. “<strong>Fourier neural operator for parametric partial differential equations.</strong>“ arXiv preprint arXiv:2010.08895 (2020).</td>
<td><a href="https://arxiv.org/abs/2010.08895">Paper</a>, <a href="https://github.com/zongyi-li/fourier_neural_operator">Code</a>, <a href="https://www.youtube.com/watch?v=IaS72aHrJKE">Video</a></td>
<td></td>
</tr>
<tr>
<td>PINNs</td>
<td>Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. “<strong>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.</strong>“ Journal of Computational Physics 378 (2019): 686-707;</td>
<td><a href="https://www.sciencedirect.com/science/article/pii/S0021999118307125?casa_token=LpL_wvHQ4CIAAAAA:9xVIgdgQV8GJnbMHbNvP7Kv_gMncbyvEcVFUQI16hhdexW6B7Mzx03LJC4QSr9txfUZ3kI2OEQ">Paper</a>, <a href="https://github.com/maziarraissi/PINNs">Code</a>, <a href="https://www.youtube.com/watch?v=ewaIDXjmRJAhttps://www.youtube.com/watch?v=ewaIDXjmRJA">Video</a></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “<strong>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.</strong>“ <em>Journal of Fluid Mechanics</em> 807 (2016): 155-166.</td>
<td><a href="https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB">Paper</a>, <a href="https://github.com/tbnn/tbnn">Code</a></td>
<td>Pure data</td>
</tr>
<tr>
<td></td>
<td>K. Duraisamy, G. Iaccarino, and H. Xiao, <strong>Turbulence modeling in the age of data</strong>, Annual Review of Fluid Mechanics 51, 357 (2019).</td>
<td><a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-fluid-010518-040547?casa_token=7LvRzlm9it8AAAAA:Fa4hR567KaYqPjKY60YzhlfxiPjg2vsQypN49usrW2ero3lYpVWjIDtq7UObUq3KYvEszTmNYnzs">Paper</a></td>
<td>Pure data, Review</td>
</tr>
<tr>
<td></td>
<td>Maulik, Romit, et al. “<strong>Subgrid modelling for two-dimensional turbulence using neural networks.</strong>“ <em>Journal of Fluid Mechanics</em> 858 (2019): 122-144.</td>
<td><a href="https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/subgrid-modelling-for-twodimensional-turbulence-using-neural-networks/10EDED1AEAA52C35F3E3A3BB6DC218C1">Paper</a></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Beck, Andrea, David Flad, and Claus-Dieter Munz. “<strong>Deep neural networks for data-driven LES closure models.</strong>“ <em>Journal of Computational Physics</em> 398 (2019): 108910.</td>
<td><a href="https://www.sciencedirect.com/science/article/pii/S0021999119306151?casa_token=U_vDCbpgaBQAAAAA:c7WGYiEprQZkdOUhMJaMO4o0qjhklsjLl8ApCjdqsjA4mt-pHTIySAIGcUELOn6Zr6ggOAwoQQ">Paper</a></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. “<strong>Deep learning for universal linear embeddings of nonlinear dynamics.</strong>“ <em>Nature communications</em> 9.1 (2018): 1-10.</td>
<td><a href="https://www.nature.com/articles/s41467-018-07210-0">Paper</a></td>
<td>Nature communications</td>
</tr>
<tr>
<td></td>
<td>Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. “<strong>DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).</strong>“ <em>arXiv preprint arXiv:1911.09145</em> (2019).</td>
<td><a href="https://arxiv.org/abs/1911.09145https://arxiv.org/abs/1911.09145">Paper</a></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Um, Kiwon, et al. “<strong>Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.</strong>“ <em>arXiv preprint arXiv:2007.00016</em> (2020).</td>
<td><a href="https://arxiv.org/abs/2007.00016">Paper</a></td>
<td></td>
</tr>
</tbody></table>
<h1 id="Videos"><a href="#Videos" class="headerlink" title="Videos"></a>Videos</h1><ul>
<li><a href="https://www.youtube.com/watch?v=2Ab-8xTI89c">2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations</a></li>
<li><a href="https://www.youtube.com/watch?v=20vB4MzAbCwv">Steve Brunton: Machine Learning for Fluid Dynamics</a></li>
<li><a href="https://www.youtube.com/watch?v=gv20VsKqgpc">Petros Koumoutsakos: “Machine Learning for Fluid Mechanics”</a></li>
</ul>
<h1 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h1><ul>
<li><a href="https://zongyi-li.github.io/blog/2020/fourier-pde/">Fourier Neural Operator</a></li>
</ul>
<h1 id="Research-Groups"><a href="#Research-Groups" class="headerlink" title="Research Groups"></a>Research Groups</h1><ul>
<li><a href="https://www.eigensteve.com/">Brunton Lab: Data-driven dynamics and control</a></li>
<li><a href="http://tensorlab.cms.caltech.edu/users/anima/group.html">Animashree Anandkumar</a></li>
<li><a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-kam-wing.html">Wing Kam Liu Group</a></li>
</ul>
<h1 id="Contact"><a href="#Contact" class="headerlink" title="Contact"></a>Contact</h1><p>If you like, please star or fork.</p>
<p>Welcome any comments or feedbacks!</p>
<p>Email: <a href="mailto:&#x78;&#105;&#97;&#111;&#x79;&#x75;&#120;&#105;&#101;&#50;&#48;&#x32;&#x30;&#64;&#117;&#x2e;&#x6e;&#111;&#114;&#x74;&#104;&#x77;&#101;&#115;&#116;&#x65;&#114;&#110;&#x2e;&#101;&#100;&#117;">&#x78;&#105;&#97;&#111;&#x79;&#x75;&#120;&#105;&#101;&#50;&#48;&#x32;&#x30;&#64;&#117;&#x2e;&#x6e;&#111;&#114;&#x74;&#104;&#x77;&#101;&#115;&#116;&#x65;&#114;&#110;&#x2e;&#101;&#100;&#117;</a></p>
]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Deep Learning</tag>
        <tag>Fluid Mechanics</tag>
        <tag>Partial Differential Equations</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust and Explainable Image Classification Based on Logits Kernel Density Estimation</title>
    <url>/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/</url>
    <content><![CDATA[<h1 id="Project-Description"><a href="#Project-Description" class="headerlink" title="Project Description"></a>Project Description</h1><p>For a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.</p>
<a id="more"></a>

<p>There are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are.</p>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>The cat and dog dataset used in this project is downloaded from <a href="https://www.kaggle.com/tongpython/cat-and-dog">Kaggel</a>. Several images are shown in the below:</p>
<p>The data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation.</p>
<p>The batch size is 32 and the dataset will be shuﬄed.</p>
<h1 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h1><p>Detailed information about model training can be found in <a href="https://github.com/xiaoyuxie-vico/ExplainableAI/blob/master/trainer_cat_dog.ipynb">trainer_cat_dog.ipynb</a>.</p>
<p>The accuracy in the training set and the test set are 0.9809 and 0.9812 respectively.</p>
<h1 id="Error-analysis-test-set"><a href="#Error-analysis-test-set" class="headerlink" title="Error analysis (test set)"></a>Error analysis (test set)</h1><p>Althought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high.</p>
<p><img src="1.png"></p>
<p>True label is dog, but predicted label is cat and the score is high.</p>
<p><img src="2.png"></p>
<p>The Grad-CAM results for these wrong predicted images are:</p>
<p><img src="3.png"></p>
<p>We can ﬁnd that: </p>
<ol>
<li> Apparent misclassiﬁcations tend to have a larger attention area; </li>
<li>Understandable misclassiﬁcations tend to have a smaller attention area;</li>
</ol>
<h1 id="Distribution-analysis"><a href="#Distribution-analysis" class="headerlink" title="Distribution analysis"></a>Distribution analysis</h1><p>To solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes.</p>
<p>Below, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project.</p>
<p><img src="4.png"></p>
<p>Kernel density estimation for logits is:</p>
<p><img src="5.png"></p>
<h1 id="Analyze-the-robustness-of-classiﬁcation-using-logits-kernel-density-estimation"><a href="#Analyze-the-robustness-of-classiﬁcation-using-logits-kernel-density-estimation" class="headerlink" title="Analyze the robustness of classiﬁcation using logits kernel density estimation"></a>Analyze the robustness of classiﬁcation using logits kernel density estimation</h1><p>It is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations.</p>
<p>For the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization.</p>
<p>If we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not “seen” these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model.</p>
<p><img src="6.png"></p>
<p>In the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions.</p>
<p><img src="7.png"></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul>
<li>The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; </li>
<li>The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; </li>
<li>The proposed approach is explainable and can be understand visually;</li>
</ul>
<h1 id="More-resources"><a href="#More-resources" class="headerlink" title="More resources"></a>More resources</h1><ul>
<li><a href="https://github.com/xiaoyuxie-vico/ExplainableAI">Github</a></li>
<li><a href="https://youtu.be/cIIOdQHTQu4">Youtube Explanation</a></li>
</ul>
<p>Note that this project is my final project for EE475 at Northwestern University in 2020 Fall.</p>
]]></content>
      <categories>
        <category>Course project</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Class Project</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Applications of Machine Learning for Fluid Mechanics</title>
    <url>/2021/02/14/notes-ML-FM/</url>
    <content><![CDATA[<p>This blog is the notes for a video called “<a href="https://www.youtube.com/watch?v=8e3OT2K99Kw&t=84s">Machine Learning for Fluid Mechanics</a>“, which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. “Machine learning for fluid mechanics.” Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.</p>
<a id="more"></a>

<h1 id="What-is-Machine-Learning-ML"><a href="#What-is-Machine-Learning-ML" class="headerlink" title="What is Machine Learning (ML)?"></a>What is Machine Learning (ML)?</h1><p>ML:</p>
<ul>
<li>Models from Data via Optimization</li>
</ul>
<blockquote>
<p>Any sufficiently advanced technology is indistinguishable from magic.</p>
<p>– Arthur C. Clarke</p>
</blockquote>
<p>Fluid dynamics tasks:</p>
<ul>
<li>Reduction</li>
<li>Modeling</li>
<li>Control</li>
<li>Sensing</li>
<li>Closure</li>
</ul>
<p>Optimization problems:</p>
<ul>
<li>High-dimensional</li>
<li>Non-linear</li>
<li>Non-convex</li>
<li>Multiscale</li>
</ul>
<h1 id="What-kind-of-ML-is-needed-in-science-and-engineering"><a href="#What-kind-of-ML-is-needed-in-science-and-engineering" class="headerlink" title="What kind of ML is needed in science and engineering?"></a>What kind of ML is needed in science and engineering?</h1><p>We need Interpretable and Generalizable Machine Learning in science and engineering field.</p>
<blockquote>
<p>Everything should be made as simple as possible, but not simpler.</p>
<p>– Albert Einstein</p>
</blockquote>
<p>How to build a model like $F=ma$?</p>
<p>Features for ML in science and engineering:</p>
<ul>
<li>Sparse</li>
<li>Low-dimensional</li>
<li>Robust</li>
</ul>
<h1 id="Schematic-ML-CFD"><a href="#Schematic-ML-CFD" class="headerlink" title="Schematic: ML + CFD"></a>Schematic: ML + CFD</h1><p><img src="image-20210214173356481.png" alt="Schematic"></p>
<h1 id="Why-ML-could-work"><a href="#Why-ML-could-work" class="headerlink" title="Why ML could work?"></a>Why ML could work?</h1><p>Because patterns exist in fluid flow.</p>
<p><img src="image-20210214173840928.png" alt="pattern_FM"></p>
<h1 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h1><h2 id="Fluid-flow-decomposition"><a href="#Fluid-flow-decomposition" class="headerlink" title="Fluid flow decomposition"></a>Fluid flow decomposition</h2><p>PCA (Shallow, linear) -&gt; Autoencoder (Deep)</p>
<p><img src="image-20210214174003005.png" alt="PCA_FM"></p>
<p><img src="image-20210214175022879.png"></p>
<h2 id="Denoise-for-Fluid-Flow"><a href="#Denoise-for-Fluid-Flow" class="headerlink" title="Denoise for Fluid Flow"></a>Denoise for Fluid Flow</h2><p><img src="image-20210214174108194.png" alt="denoise"></p>
<h2 id="Turbulence-modeling"><a href="#Turbulence-modeling" class="headerlink" title="Turbulence modeling"></a>Turbulence modeling</h2><p>Paper: </p>
<ul>
<li><p>Schlatter, Philipp, et al. “The structure of a turbulent boundary layer studied by numerical simulation.” arXiv preprint arXiv:1010.4000 (2010).</p>
</li>
<li><p>Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. “Turbulence modeling in the age of data.” <em>Annual Review of Fluid Mechanics</em> 51 (2019): 357-377.</p>
</li>
</ul>
<p><img src="image-20210214174310749.png"></p>
<p><img src="image-20210214174422276.png"></p>
<h2 id="ML-CFD-solver"><a href="#ML-CFD-solver" class="headerlink" title="ML_CFD solver"></a>ML_CFD solver</h2><p>Paper:</p>
<ul>
<li>Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.” Journal of Fluid Mechanics 807 (2016): 155-166.</li>
</ul>
<p>Add physical constraints and achieve accurate and pyhsical.</p>
<h2 id="Super-resolution"><a href="#Super-resolution" class="headerlink" title="Super-resolution"></a>Super-resolution</h2><p>Paper: </p>
<ul>
<li>Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. “Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.” arXiv preprint arXiv:1905.10866 (2019).</li>
</ul>
<p>Interpolation and Extrapolation</p>
<p><img src="image-20210214174809740.png"></p>
<p><img src="image-20210214174913278.png"></p>
<h2 id="Solve-PDEs"><a href="#Solve-PDEs" class="headerlink" title="Solve PDEs"></a>Solve PDEs</h2><p><img src="image-20210214175304898.png"></p>
<h1 id="Beyond-understanding-control"><a href="#Beyond-understanding-control" class="headerlink" title="Beyond understanding: control"></a>Beyond understanding: control</h1><p><img src="image-20210214182334199.png"></p>
<h1 id="Inspiration-from-biology"><a href="#Inspiration-from-biology" class="headerlink" title="Inspiration from biology"></a>Inspiration from biology</h1><p><img src="image-20210214182520461.png"></p>
]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Deep Learning</tag>
        <tag>Fluid Mechanics</tag>
      </tags>
  </entry>
  <entry>
    <title>GANs specialization Class 2 Week4 Notes and codes</title>
    <url>/2021/06/19/GANs-specialization-Class-2-Week4-Notes-and-codes/</url>
    <content><![CDATA[<h1 id="Class-2-week-1-assignment"><a href="#Class-2-week-1-assignment" class="headerlink" title="Class 2 week 1 assignment"></a>Class 2 week 1 assignment</h1><h2 id="Evaluating-GANs"><a href="#Evaluating-GANs" class="headerlink" title="Evaluating GANs"></a>Evaluating GANs</h2><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to gain a better understanding of some of the challenges that come with evaluating GANs and a response you can take to alleviate some of them called Fréchet Inception Distance (FID).</p>
<a id="more"></a>

<h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol>
<li>  Understand the challenges associated with evaluating GANs.</li>
<li>  Write code to evaluate the Fréchet Inception Distance.</li>
</ol>
<h2 id="Challenges-With-Evaluating-GANs"><a href="#Challenges-With-Evaluating-GANs" class="headerlink" title="Challenges With Evaluating GANs"></a>Challenges With Evaluating GANs</h2><h4 id="Loss-is-Uninformative-of-Performance"><a href="#Loss-is-Uninformative-of-Performance" class="headerlink" title="Loss is Uninformative of Performance"></a>Loss is Uninformative of Performance</h4><p>One aspect that makes evaluating GANs challenging is that the loss tells us little about their performance. Unlike with classifiers, where a low loss on a test set indicates superior performance, a low loss for the generator or discriminator suggests that learning has stopped. </p>
<h4 id="No-Clear-Non-human-Metric"><a href="#No-Clear-Non-human-Metric" class="headerlink" title="No Clear Non-human Metric"></a>No Clear Non-human Metric</h4><p>If you define the goal of a GAN as “generating images which look real to people” then it’s technically possible to measure this directly: <a href="https://arxiv.org/abs/1904.01121">you can ask people to act as a discriminator</a>. However, this takes significant time and money so ideally you can use a proxy for this. There is also no “perfect” discriminator that can differentiate reals from fakes - if there were, a lot of machine learning tasks would be solved ;)</p>
<p>In this notebook, you will implement Fréchet Inception Distance, one method which aims to solve these issues. </p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>For this notebook, you will again be using <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a>. You will start by loading a pre-trained generator which has been trained on CelebA.</p>
<p>Here, you will import some useful libraries and packages. You will also be provided with the generator and noise code from earlier assignments.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> CelebA</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for our testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (CelebA is rgb, so 3 is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, z_dim=<span class="number">10</span>, im_chan=<span class="number">3</span>, hidden_dim=<span class="number">64</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.z_dim = z_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(z_dim, hidden_dim * <span class="number">8</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">8</span>, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span>(<span class="params">self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN;</span></span><br><span class="line"><span class="string">        a transposed convolution, a batchnorm (except in the final layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, noise</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        x = noise.view(<span class="built_in">len</span>(noise), self.z_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span>(<span class="params">n_samples, z_dim, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device=device)</span><br></pre></td></tr></table></figure>
<h2 id="Loading-the-Pre-trained-Model"><a href="#Loading-the-Pre-trained-Model" class="headerlink" title="Loading the Pre-trained Model"></a>Loading the Pre-trained Model</h2><p>Now, you can set the arguments for the model and load the dataset:</p>
<ul>
<li>  z_dim: the dimension of the noise vector</li>
<li>  image_size: the image size of the input to Inception (more details in the following section)</li>
<li>  device: the device type</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">image_size = <span class="number">299</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(image_size),</span><br><span class="line">    transforms.CenterCrop(image_size),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">in_coursera = <span class="literal">True</span> <span class="comment"># Set this to false if you&#x27;re running this outside Coursera</span></span><br><span class="line"><span class="keyword">if</span> in_coursera:</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    data = torch.Tensor(np.load(<span class="string">&#x27;fid_images_tensor.npz&#x27;</span>, allow_pickle=<span class="literal">True</span>)[<span class="string">&#x27;arr_0&#x27;</span>])</span><br><span class="line">    dataset = torch.utils.data.TensorDataset(data, data)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dataset = CelebA(<span class="string">&quot;.&quot;</span>, download=<span class="literal">True</span>, transform=transform)</span><br></pre></td></tr></table></figure>
<p>Then, you can load and initialize the model with weights from a pre-trained model. This allows you to use the pre-trained model as if you trained it yourself.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen.load_state_dict(torch.load(<span class="string">f&quot;pretrained_celeba.pth&quot;</span>, map_location=torch.device(device))[<span class="string">&quot;gen&quot;</span>])</span><br><span class="line">gen = gen.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<h2 id="Inception-v3-Network"><a href="#Inception-v3-Network" class="headerlink" title="Inception-v3 Network"></a>Inception-v3 Network</h2><p>Inception-V3 is a neural network trained on <a href="http://www.image-net.org/">ImageNet</a> to classify objects. You may recall from the lectures that ImageNet has over 1 million images to train on. As a result, Inception-V3 does a good job detecting features and classifying images. Here, you will load Inception-V3 as <code>inception_model</code>.</p>
<!--  
In the past, people would use a pretrained Inception network to identify the classes of the objects generated by a GAN and measure how similar the distribution of classes generated was to the true image (using KL divergence). This is known as inception score. 

However, there are many problems with this metric. Barratt and Sharma's 2018 "[A Note on the Inception Score](https://arxiv.org/pdf/1801.01973.pdf)" highlights many issues with this approach. Among them, they highlight its instability, its exploitability, and the widespread use of Inception Score on models not trained on ImageNet.  -->





<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> inception_v3</span><br><span class="line">inception_model = inception_v3(pretrained=<span class="literal">False</span>)</span><br><span class="line">inception_model.load_state_dict(torch.load(<span class="string">&quot;inception_v3_google-1a9a5a14.pth&quot;</span>))</span><br><span class="line">inception_model.to(device)</span><br><span class="line">inception_model = inception_model.<span class="built_in">eval</span>() <span class="comment"># Evaluation mode</span></span><br></pre></td></tr></table></figure>
<h2 id="Frechet-Inception-Distance"><a href="#Frechet-Inception-Distance" class="headerlink" title="Fréchet Inception Distance"></a>Fréchet Inception Distance</h2><p>Fréchet Inception Distance (FID) was proposed as an improvement over Inception Score and still uses the Inception-v3 network as part of its calculation. However, instead of using the classification labels of the Inception-v3 network, it uses the output from an earlier layer—the layer right before the labels. This is often called the feature layer. Research has shown that deep convolutional neural networks trained on difficult tasks, like classifying many classes, build increasingly sophisticated representations of features going deeper into the network. For example, the first few layers may learn to detect different kinds of edges and curves, while the later layers may have neurons that fire in response to human faces.</p>
<p>To get the feature layer of a convolutional neural network, you can replace the final fully connected layer with an identity layer that simply returns whatever input it received, unchanged. This essentially removes the final classification layer and leaves you with the intermediate outputs from the layer before.</p>
<details>

<summary>
<font size="3" color="green">
<b>Optional hint for <code><font size="4">inception_model.fc</font></code></b>
</font>
</summary>

<ol>
<li>   You may find <a href="https://pytorch.org/docs/master/generated/torch.nn.Identity.html">torch.nn.Identity()</a> helpful.</li>
</ol>
</details>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: inception_model.fc</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You want to replace the final fully-connected (fc) layer </span></span><br><span class="line"><span class="comment"># with an identity function layer to cut off the classification</span></span><br><span class="line"><span class="comment"># layer and get a feature extractor</span></span><br><span class="line"><span class="comment">#### START CODE HERE ####</span></span><br><span class="line">inception_model.fc = nn.Identity()</span><br><span class="line"><span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_identity_noise = torch.randn(<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line"><span class="keyword">assert</span> torch.equal(test_identity_noise, inception_model.fc(test_identity_noise))</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<h3 id="Frechet-Distance"><a href="#Frechet-Distance" class="headerlink" title="Fréchet Distance"></a>Fréchet Distance</h3><p>Fréchet distance uses the values from the feature layer for two sets of images, say reals and fakes, and compares different statistical properties between them to see how different they are. Specifically, Fréchet distance finds the shortest distance needed to walk along two lines, or two curves, simultaneously. The most intuitive explanation of Fréchet distance is as the “minimum leash distance” between two points. Imagine yourself and your dog, both moving along two curves. If you walked on one curve and your dog, attached to a leash, walked on the other at the same pace, what is the least amount of leash that you can give your dog so that you never need to give them more slack during your walk? Using this, the Fréchet distance measures the similarity between these two curves.</p>
<p>The basic idea is similar for calculating the Fréchet distance between two probability distributions. You’ll start by seeing what this looks like in one-dimensional, also called univariate, space.</p>
<h4 id="Univariate-Frechet-Distance"><a href="#Univariate-Frechet-Distance" class="headerlink" title="Univariate Fréchet Distance"></a>Univariate Fréchet Distance</h4><p>You can calculate the distance between two normal distributions $X$ and $Y$ with means $\mu_X$ and $\mu_Y$ and standard deviations $\sigma_X$ and $\sigma_Y$, as:</p>
<p>$$d(X,Y) = (\mu_X-\mu_Y)^2 + (\sigma_X-\sigma_Y)^2 $$</p>
<p>Pretty simple, right? Now you can see how it can be converted to be used in multi-dimensional, which is also called multivariate, space.</p>
<h4 id="Multivariate-Frechet-Distance"><a href="#Multivariate-Frechet-Distance" class="headerlink" title="Multivariate Fréchet Distance"></a>Multivariate Fréchet Distance</h4><p><strong>Covariance</strong></p>
<p>To find the Fréchet distance between two multivariate normal distributions, you first need to find the covariance instead of the standard deviation. The covariance, which is the multivariate version of variance (the square of standard deviation), is represented using a square matrix where the side length is equal to the number of dimensions. Since the feature vectors you will be using have 2048 values/weights, the covariance matrix will be 2048 x 2048. But for the sake of an example, this is a covariance matrix in a two-dimensional space:</p>
<p>$\Sigma = \left(\begin{array}{cc}<br>1 &amp; 0\<br>0 &amp; 1<br>\end{array}\right)<br>$</p>
<p>The value at location $(i, j)$ corresponds to the covariance of vector $i$ with vector $j$. Since the covariance of $i$ with $j$ and $j$ with $i$ are equivalent, the matrix will always be symmetric with respect to the diagonal. The diagonal is the covariance of that element with itself. In this example, there are zeros everywhere except the diagonal. That means that the two dimensions are independent of one another, they are completely unrelated.</p>
<p>The following code cell will visualize this matrix.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#import os</span></span><br><span class="line"><span class="comment">#os.environ[&#x27;KMP_DUPLICATE_LIB_OK&#x27;]=&#x27;True&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> MultivariateNormal</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment"># This is for visualization</span></span><br><span class="line">mean = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance = torch.Tensor( <span class="comment"># This matrix shows independence - there are only non-zero values on the diagonal</span></span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">independent_dist = MultivariateNormal(mean, covariance)</span><br><span class="line">samples = independent_dist.sample((<span class="number">10000</span>,))</span><br><span class="line">res = sns.jointplot(samples[:, <span class="number">0</span>], samples[:, <span class="number">1</span>], kind=<span class="string">&quot;kde&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_18_0.png" alt="png"></p>
<p>Now, here’s an example of a multivariate normal distribution that has covariance:</p>
<p>$\Sigma = \left(\begin{array}{cc}<br>2 &amp; -1\<br>-1 &amp; 2<br>\end{array}\right)<br>$</p>
<p>And see how it looks:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mean = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">covariance = torch.Tensor(</span><br><span class="line">    [[<span class="number">2</span>, -<span class="number">1</span>],</span><br><span class="line">     [-<span class="number">1</span>, <span class="number">2</span>]]</span><br><span class="line">)</span><br><span class="line">covariant_dist = MultivariateNormal(mean, covariance)</span><br><span class="line">samples = covariant_dist.sample((<span class="number">10000</span>,))</span><br><span class="line">res = sns.jointplot(samples[:, <span class="number">0</span>], samples[:, <span class="number">1</span>], kind=<span class="string">&quot;kde&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_20_0.png" alt="png"></p>
<p><strong>Formula</strong></p>
<p>Based on the paper, “<a href="https://core.ac.uk/reader/82269844">The Fréchet distance between multivariate normal distributions</a>“ by Dowson and Landau (1982), the Fréchet distance between two multivariate normal distributions $X$ and $Y$ is:</p>
<p>$d(X, Y) = \Vert\mu_X-\mu_Y\Vert^2 + \mathrm{Tr}\left(\Sigma_X+\Sigma_Y - 2 \sqrt{\Sigma_X \Sigma_Y}\right)$</p>
<p>Similar to the formula for univariate Fréchet distance, you can calculate the distance between the means and the distance between the standard deviations. However, calculating the distance between the standard deviations changes slightly here, as it includes the matrix product and matrix square root. $\mathrm{Tr}$ refers to the trace, the sum of the diagonal elements of a matrix.</p>
<p>Now you can implement this!</p>
<details>

<summary>
<font size="3" color="green">
<b>Optional hints for <code><font size="4">frechet_distance</font></code></b>
</font>
</summary>

<ol>
<li>  You want to implement the above equation in code.</li>
<li>  You might find the functions <code>torch.norm</code> and <code>torch.trace</code> helpful here.</li>
<li>  A matrix_sqrt function is defined for you above – you need to use it instead of <code>torch.sqrt()</code> which only gets the elementwise square root instead of the matrix square root.</li>
<li>  You can also use the <code>@</code> symbol for matrix multiplication.</details>


</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="comment"># This is the matrix square root function you will be using</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matrix_sqrt</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function that takes in a matrix and returns the square root of that matrix.</span></span><br><span class="line"><span class="string">    For an input matrix A, the output matrix B would be such that B @ B is the matrix A.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        x: a matrix</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    y = x.cpu().detach().numpy()</span><br><span class="line">    y = scipy.linalg.sqrtm(y)</span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(y.real, device=x.device)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: frechet_distance</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">frechet_distance</span>(<span class="params">mu_x, mu_y, sigma_x, sigma_y</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for returning the Fréchet distance between multivariate Gaussians,</span></span><br><span class="line"><span class="string">    parameterized by their means and covariance matrices.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        mu_x: the mean of the first Gaussian, (n_features)</span></span><br><span class="line"><span class="string">        mu_y: the mean of the second Gaussian, (n_features) </span></span><br><span class="line"><span class="string">        sigma_x: the covariance matrix of the first Gaussian, (n_features, n_features)</span></span><br><span class="line"><span class="string">        sigma_y: the covariance matrix of the second Gaussian, (n_features, n_features)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> torch.norm(mu_x-mu_y, <span class="number">2</span>) + torch.trace(sigma_x+sigma_y-<span class="number">2</span>*matrix_sqrt(torch.matmul(sigma_x, sigma_y)))</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"></span><br><span class="line">mean1 = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance1 = torch.Tensor( <span class="comment"># This matrix shows independence - there are only non-zero values on the diagonal</span></span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">dist1 = MultivariateNormal(mean1, covariance1)</span><br><span class="line"></span><br><span class="line">mean2 = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance2 = torch.Tensor( <span class="comment"># This matrix shows dependence </span></span><br><span class="line">    [[<span class="number">2</span>, -<span class="number">1</span>],</span><br><span class="line">     [-<span class="number">1</span>, <span class="number">2</span>]]</span><br><span class="line">)</span><br><span class="line">dist2 = MultivariateNormal(mean2, covariance2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    frechet_distance(</span><br><span class="line">        dist1.mean, dist2.mean,</span><br><span class="line">        dist1.covariance_matrix, dist2.covariance_matrix</span><br><span class="line">    ),</span><br><span class="line">    <span class="number">4</span> - <span class="number">2</span> * torch.sqrt(torch.tensor(<span class="number">3.</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> (frechet_distance(</span><br><span class="line">        dist1.mean, dist1.mean,</span><br><span class="line">        dist1.covariance_matrix, dist1.covariance_matrix</span><br><span class="line">    ).item() == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<h2 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together!"></a>Putting it all together!</h2><p>Now, you can apply FID to your generator from earlier.</p>
<p>You will start by defining a bit of helper code to preprocess the image for the Inception-v3 network:</p>
<!-- This isn't exactly what FID is meant for, since inception scores expect a natural image, but it should give a rough idea of the diversity and quality of your images.  [TODO: move to bottom since image net is trained on nature (cat, dog) images, fidelity (quality)] -->


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">img</span>):</span></span><br><span class="line">    img = torch.nn.functional.interpolate(img, size=(<span class="number">299</span>, <span class="number">299</span>), mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>
<p>Then, you’ll define a function to calculate the covariance of the features that returns a covariance matrix given a list of values:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_covariance</span>(<span class="params">features</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(np.cov(features.detach().numpy(), rowvar=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure>
<p>Finally, you can use the pre-trained Inception-v3 model to compute features of the real and fake images. With these features, you can then get the covariance and means of these features across many samples. </p>
<p>First, you get the features of the real and fake images using the Inception-v3 model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fake_features_list = []</span><br><span class="line">real_features_list = []</span><br><span class="line"></span><br><span class="line">gen.<span class="built_in">eval</span>()</span><br><span class="line">n_samples = <span class="number">512</span> <span class="comment"># The total number of samples</span></span><br><span class="line">batch_size = <span class="number">4</span> <span class="comment"># Samples per iteration</span></span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">cur_samples = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad(): <span class="comment"># You don&#x27;t need to calculate gradients here, so you do this to save memory</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> real_example, _ <span class="keyword">in</span> tqdm(dataloader, total=n_samples // batch_size): <span class="comment"># Go by batch</span></span><br><span class="line">            real_samples = real_example</span><br><span class="line">            real_features = inception_model(real_samples.to(device)).detach().to(<span class="string">&#x27;cpu&#x27;</span>) <span class="comment"># Move features to CPU</span></span><br><span class="line">            real_features_list.append(real_features)</span><br><span class="line"></span><br><span class="line">            fake_samples = get_noise(<span class="built_in">len</span>(real_example), z_dim).to(device)</span><br><span class="line">            fake_samples = preprocess(gen(fake_samples))</span><br><span class="line">            fake_features = inception_model(fake_samples.to(device)).detach().to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">            fake_features_list.append(fake_features)</span><br><span class="line">            cur_samples += <span class="built_in">len</span>(real_samples)</span><br><span class="line">            <span class="keyword">if</span> cur_samples &gt;= n_samples:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">&quot;Error in loop&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>HBox(children=(FloatProgress(value=0.0, max=128.0), HTML(value=&#39;&#39;)))</code></pre>
<p>Then, you can combine all of the values that you collected for the reals and fakes into large tensors:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Needed as is for autograding</span></span><br><span class="line">fake_features_all = torch.cat(fake_features_list)</span><br><span class="line">real_features_all = torch.cat(real_features_list)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fake_features_all.shape</span><br></pre></td></tr></table></figure>



<pre><code>torch.Size([512, 2048])</code></pre>
<p>And calculate the covariance and means of these real and fake features:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the covariance matrix for the fake and real features</span></span><br><span class="line"><span class="comment"># and also calculate the means of the feature over the batch (for each feature dimension mean)</span></span><br><span class="line"><span class="comment">#### START CODE HERE ####</span></span><br><span class="line">mu_fake = torch.mean(fake_features_all, <span class="number">0</span>)</span><br><span class="line">mu_real = torch.mean(real_features_all, <span class="number">0</span>)</span><br><span class="line">sigma_fake = get_covariance(fake_features_all)</span><br><span class="line">sigma_real = get_covariance(real_features_all)</span><br><span class="line"><span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(sigma_fake.shape) == (fake_features_all.shape[<span class="number">1</span>], fake_features_all.shape[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> torch.<span class="built_in">abs</span>(sigma_fake[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">2.5e-2</span>) &lt; <span class="number">1e-2</span> <span class="keyword">and</span> torch.<span class="built_in">abs</span>(sigma_fake[-<span class="number">1</span>, -<span class="number">1</span>] - <span class="number">5e-2</span>) &lt; <span class="number">1e-2</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(sigma_real.shape) == (real_features_all.shape[<span class="number">1</span>], real_features_all.shape[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> torch.<span class="built_in">abs</span>(sigma_real[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">3.5768e-2</span>) &lt; <span class="number">1e-4</span> <span class="keyword">and</span> torch.<span class="built_in">abs</span>(sigma_real[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">5.3236e-4</span>) &lt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(mu_fake.shape) == (fake_features_all.shape[<span class="number">1</span>],)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(mu_real.shape) == (real_features_all.shape[<span class="number">1</span>],)</span><br><span class="line"><span class="keyword">assert</span> torch.<span class="built_in">abs</span>(mu_real[<span class="number">0</span>] - <span class="number">0.3099</span>) &lt; <span class="number">0.01</span> <span class="keyword">and</span> torch.<span class="built_in">abs</span>(mu_real[<span class="number">1</span>] - <span class="number">0.2721</span>) &lt; <span class="number">0.01</span></span><br><span class="line"><span class="keyword">assert</span> torch.<span class="built_in">abs</span>(mu_fake[<span class="number">0</span>] - <span class="number">0.37</span>) &lt; <span class="number">0.05</span> <span class="keyword">and</span> torch.<span class="built_in">abs</span>(mu_real[<span class="number">1</span>] - <span class="number">0.27</span>) &lt; <span class="number">0.05</span></span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<p>At this point, you can also visualize what the pairwise multivariate distributions of the inception features look like!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">indices = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">fake_dist = MultivariateNormal(mu_fake[indices], sigma_fake[indices][:, indices])</span><br><span class="line">fake_samples = fake_dist.sample((<span class="number">5000</span>,))</span><br><span class="line">real_dist = MultivariateNormal(mu_real[indices], sigma_real[indices][:, indices])</span><br><span class="line">real_samples = real_dist.sample((<span class="number">5000</span>,))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df_fake = pd.DataFrame(fake_samples.numpy(), columns=indices)</span><br><span class="line">df_real = pd.DataFrame(real_samples.numpy(), columns=indices)</span><br><span class="line">df_fake[<span class="string">&quot;is_real&quot;</span>] = <span class="string">&quot;no&quot;</span></span><br><span class="line">df_real[<span class="string">&quot;is_real&quot;</span>] = <span class="string">&quot;yes&quot;</span></span><br><span class="line">df = pd.concat([df_fake, df_real])</span><br><span class="line">sns.pairplot(df, plot_kws=&#123;<span class="string">&#x27;alpha&#x27;</span>: <span class="number">0.1</span>&#125;, hue=<span class="string">&#x27;is_real&#x27;</span>)</span><br></pre></td></tr></table></figure>



<pre><code>&lt;seaborn.axisgrid.PairGrid at 0x7f3ea941ed30&gt;</code></pre>
<p><img src="output_38_1.png" alt="png"></p>
<p>Lastly, you can use your earlier <code>frechet_distance</code> function to calculate the FID and evaluate your GAN. You can see how similar/different the features of the generated images are to the features of the real images. The next cell might take five minutes or so to run in Coursera.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print(frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake).item())</span><br></pre></td></tr></table></figure>
<p>You’ll notice this model gets a pretty high FID, likely over 30. Since lower is better, and the best models on CelebA get scores in the single-digits, there’s clearly a long way to go with this model. You can use FID to compare different models, as well as different stages of training of the same model. </p>
]]></content>
      <categories>
        <category>Class notes</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Online course</tag>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title>GANs specialization Week2: Notes and codes</title>
    <url>/2021/05/06/GANs-specialization-Week2-Notes-and-codes/</url>
    <content><![CDATA[<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><p>In this week, I learnt how to implement DCGAN and TGA. The code are very helpful due to a lot of suggestions.</p>
<p>There is an very useful interactive blog to illustrate the checkboard effects in transposed convolution. The link is <a href="https://distill.pub/2016/deconv-checkerboard/">here</a>.</p>
<a id="more"></a>

<h2 id="Transposed-convolution"><a href="#Transposed-convolution" class="headerlink" title="Transposed convolution"></a>Transposed convolution</h2><p><img src="33EBFA00-A891-4948-801C-DDA4E0550EB3.png" alt="Transposed conv"></p>
<h2 id="Checkboard-pattern"><a href="#Checkboard-pattern" class="headerlink" title="Checkboard pattern"></a>Checkboard pattern</h2><p><img src="0C581697-8793-E64F-9CA9-120B7B3B6B8F.png" alt="Checkboard pattern"></p>
<h1 id="Week-2-Assignment-Deep-Convolutional-GAN-DCGAN"><a href="#Week-2-Assignment-Deep-Convolutional-GAN-DCGAN" class="headerlink" title="Week 2 - Assignment - Deep Convolutional GAN (DCGAN)"></a>Week 2 - Assignment - Deep Convolutional GAN (DCGAN)</h1><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><p>In this notebook, you’re going to create another GAN using the MNIST dataset. You will implement a Deep Convolutional GAN (DCGAN), a very successful and influential GAN model developed in 2015.</p>
<p><em>Note: <a href="https://arxiv.org/pdf/1511.06434v1.pdf">here</a> is the paper if you are interested! It might look dense now, but soon you’ll be able to understand many parts of it :)</em></p>
<h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol>
<li>  Get hands-on experience making a widely used GAN: Deep Convolutional GAN (DCGAN).</li>
<li>  Train a powerful generative model.</li>
</ol>
<p><img src="dcgan-gen.png" alt="Generator architecture"></p>
<p>Figure: Architectural drawing of a generator from DCGAN from <a href="https://arxiv.org/pdf/1511.06434v1.pdf">Radford et al (2016)</a>.</p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h4 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h4><p>Here are the main features of DCGAN (don’t worry about memorizing these, you will be guided through the implementation!): </p>
<!-- ```
Architecture guidelines for stable Deep Convolutional GANs
• Replace any pooling layers with strided convolutions (discriminator) and fractional-strided
convolutions (generator).
• Use BatchNorm in both the generator and the discriminator.
• Remove fully connected hidden layers for deeper architectures.
• Use ReLU activation in generator for all layers except for the output, which uses Tanh.
• Use LeakyReLU activation in the discriminator for all layers.

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">*   Use convolutions without any pooling layers</span><br><span class="line">*   Use batchnorm in both the generator and the discriminator</span><br><span class="line">*   Don&#39;t use fully connected hidden layers</span><br><span class="line">*   Use ReLU activation in the generator for all layers except for the output, which uses a Tanh activation.</span><br><span class="line">*   Use LeakyReLU activation in the discriminator for all layers except for the output, which does not use an activation</span><br><span class="line"></span><br><span class="line">You will begin by importing some useful packages and data that will help you create your GAN. You are also provided a visualizer function to help see the images your GAN will create.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">​&#96;&#96;&#96;python</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from tqdm.auto import tqdm</span><br><span class="line">from torchvision import transforms</span><br><span class="line">from torchvision.datasets import MNIST</span><br><span class="line">from torchvision.utils import make_grid</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">torch.manual_seed(0) # Set for testing purposes, please do not change!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def show_tensor_images(image_tensor, num_images&#x3D;25, size&#x3D;(1, 28, 28)):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Function for visualizing images: Given a tensor of images, number of images, and</span><br><span class="line">    size per image, plots and prints the images in an uniform grid.</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    image_tensor &#x3D; (image_tensor + 1) &#x2F; 2</span><br><span class="line">    image_unflat &#x3D; image_tensor.detach().cpu()</span><br><span class="line">    image_grid &#x3D; make_grid(image_unflat[:num_images], nrow&#x3D;5)</span><br><span class="line">    plt.imshow(image_grid.permute(1, 2, 0).squeeze())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>The first component you will make is the generator. You may notice that instead of passing in the image dimension, you will pass the number of image channels to the generator. This is because with DCGAN, you use convolutions which don’t depend on the number of pixels on an image. However, the number of channels is important to determine the size of the filters.</p>
<p>You will build a generator using 4 layers (3 hidden layers + 1 output layer). As before, you will need to write a function to create a single block for the generator’s neural network.</p>
<!-- From the paper, we know to "[u]se batchnorm in both the generator and the discriminator" and "[u]se ReLU activation in generator for all layers except for the output, which uses Tanh." --> 
<p>Since in DCGAN the activation function will be different for the output layer, you will need to check what layer is being created. You are supplied with some tests following the code cell so you can see if you’re on the right track!</p>
<p>At the end of the generator class, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network. You are also given a function to create a noise vector. These functions are the same as the ones from the last assignment.</p>
<details>
<summary>
<font size="3" color="green">
<b>Optional hint for <code><font size="4">make_gen_block</font></code></b>
</font>
</summary>


<ol>
<li>You’ll find <a href="https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html">nn.ConvTranspose2d</a> and <a href="https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html">nn.BatchNorm2d</a> useful!</details>


</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Generator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, z_dim=<span class="number">10</span>, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.z_dim = z_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(z_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span>(<span class="params">self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN, </span></span><br><span class="line"><span class="string">        corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#     Steps:</span></span><br><span class="line">        <span class="comment">#       1) Do a transposed convolution using the given parameters.</span></span><br><span class="line">        <span class="comment">#       2) Do a batchnorm, except for the last layer.</span></span><br><span class="line">        <span class="comment">#       3) Follow each batchnorm with a ReLU activation.</span></span><br><span class="line">        <span class="comment">#       4) If its the final layer, use a Tanh activation after the deconvolution.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build the neural block</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">                <span class="comment"># step 1</span></span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                <span class="comment"># step 2</span></span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                <span class="comment"># step 3</span></span><br><span class="line">                nn.ReLU()</span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># Final Layer</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">                <span class="comment"># step 4</span></span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh()</span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unsqueeze_noise</span>(<span class="params">self, noise</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns a copy of that noise with width and height = 1 and channels = z_dim.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> noise.view(<span class="built_in">len</span>(noise), self.z_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, noise</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        x = self.unsqueeze_noise(noise)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span>(<span class="params">n_samples, z_dim, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device=device)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Test your make_gen_block() function</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">gen = Generator()</span><br><span class="line">num_test = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the hidden block</span></span><br><span class="line">test_hidden_noise = get_noise(num_test, gen.z_dim)</span><br><span class="line">test_hidden_block = gen.make_gen_block(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>)</span><br><span class="line">test_uns_noise = gen.unsqueeze_noise(test_hidden_noise)</span><br><span class="line">hidden_output = test_hidden_block(test_uns_noise)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that it works with other strides</span></span><br><span class="line">test_hidden_block_stride = gen.make_gen_block(<span class="number">20</span>, <span class="number">20</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">test_final_noise = get_noise(num_test, gen.z_dim) * <span class="number">20</span></span><br><span class="line">test_final_block = gen.make_gen_block(<span class="number">10</span>, <span class="number">20</span>, final_layer=<span class="literal">True</span>)</span><br><span class="line">test_final_uns_noise = gen.unsqueeze_noise(test_final_noise)</span><br><span class="line">final_output = test_final_block(test_final_uns_noise)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the whole thing:</span></span><br><span class="line">test_gen_noise = get_noise(num_test, gen.z_dim)</span><br><span class="line">test_uns_gen_noise = gen.unsqueeze_noise(test_gen_noise)</span><br><span class="line">gen_output = gen(test_uns_gen_noise)</span><br></pre></td></tr></table></figure>
<p>Here’s the test for your generator block:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNIT TESTS</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(hidden_output.shape) == (num_test, <span class="number">20</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">assert</span> hidden_output.<span class="built_in">max</span>() &gt; <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.<span class="built_in">min</span>() == <span class="number">0</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &gt; <span class="number">0.2</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &lt; <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &gt; <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(test_hidden_block_stride(hidden_output).shape) == (num_test, <span class="number">20</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> final_output.<span class="built_in">max</span>().item() == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> final_output.<span class="built_in">min</span>().item() == -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(gen_output.shape) == (num_test, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="keyword">assert</span> gen_output.std() &gt; <span class="number">0.5</span></span><br><span class="line"><span class="keyword">assert</span> gen_output.std() &lt; <span class="number">0.8</span></span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>The second component you need to create is the discriminator.</p>
<p>You will use 3 layers in your discriminator’s neural network. Like with the generator, you will need create the function to create a single neural network block for the discriminator.</p>
<!-- From the paper, we know that we need to "[u]se LeakyReLU activation in the discriminator for all layers." And for the LeakyReLUs, "the slope of the leak was set to 0.2" in DCGAN. -->
<p>There are also tests at the end for you to use.</p>
<details>
<summary>
<font size="3" color="green">
<b>Optional hint for <code><font size="4">make_disc_block</font></code></b>
</font>
</summary>


<ol>
<li>You’ll find <a href="https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html">nn.Conv2d</a>, <a href="https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html">nn.BatchNorm2d</a>, and <a href="https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html">nn.LeakyReLU</a> useful!</details>


</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Discriminator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">    hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">16</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            self.make_disc_block(im_chan, hidden_dim),</span><br><span class="line">            self.make_disc_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_disc_block(hidden_dim * <span class="number">2</span>, <span class="number">1</span>, final_layer=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_disc_block</span>(<span class="params">self, input_channels, output_channels, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, final_layer=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a discriminator block of DCGAN, </span></span><br><span class="line"><span class="string">        corresponding to a convolution, a batchnorm (except for in the last layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment">#     Steps:</span></span><br><span class="line">        <span class="comment">#       1) Add a convolutional layer using the given parameters.</span></span><br><span class="line">        <span class="comment">#       2) Do a batchnorm, except for the last layer.</span></span><br><span class="line">        <span class="comment">#       3) Follow each batchnorm with a LeakyReLU activation with slope 0.2.</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Build the neural block</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE #### #</span></span><br><span class="line">                <span class="comment"># step 1</span></span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                <span class="comment"># step 2</span></span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                <span class="comment"># step 3</span></span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># Final Layer</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE #### #</span></span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, image</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        disc_pred = self.disc(image)</span><br><span class="line">        <span class="keyword">return</span> disc_pred.view(<span class="built_in">len</span>(disc_pred), -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Test your make_disc_block() function</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">num_test = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">gen = Generator()</span><br><span class="line">disc = Discriminator()</span><br><span class="line">test_images = gen(get_noise(num_test, gen.z_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the hidden block</span></span><br><span class="line">test_hidden_block = disc.make_disc_block(<span class="number">1</span>, <span class="number">5</span>, kernel_size=<span class="number">6</span>, stride=<span class="number">3</span>)</span><br><span class="line">hidden_output = test_hidden_block(test_images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the final block</span></span><br><span class="line">test_final_block = disc.make_disc_block(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">5</span>, final_layer=<span class="literal">True</span>)</span><br><span class="line">final_output = test_final_block(test_images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the whole thing:</span></span><br><span class="line">disc_output = disc(test_images)</span><br></pre></td></tr></table></figure>
<p>Here’s a test for your discriminator block:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Test the hidden block</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(hidden_output.shape) == (num_test, <span class="number">5</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line"><span class="comment"># Because of the LeakyReLU slope</span></span><br><span class="line"><span class="keyword">assert</span> -hidden_output.<span class="built_in">min</span>() / hidden_output.<span class="built_in">max</span>() &gt; <span class="number">0.15</span></span><br><span class="line"><span class="keyword">assert</span> -hidden_output.<span class="built_in">min</span>() / hidden_output.<span class="built_in">max</span>() &lt; <span class="number">0.25</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &gt; <span class="number">0.5</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &lt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the final block</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(final_output.shape) == (num_test, <span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">assert</span> final_output.<span class="built_in">max</span>() &gt; <span class="number">1.0</span></span><br><span class="line"><span class="keyword">assert</span> final_output.<span class="built_in">min</span>() &lt; -<span class="number">1.0</span></span><br><span class="line"><span class="keyword">assert</span> final_output.std() &gt; <span class="number">0.3</span></span><br><span class="line"><span class="keyword">assert</span> final_output.std() &lt; <span class="number">0.6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the whole thing:</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(disc_output.shape) == (num_test, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> disc_output.std() &gt; <span class="number">0.25</span></span><br><span class="line"><span class="keyword">assert</span> disc_output.std() &lt; <span class="number">0.5</span></span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Now you can put it all together!<br>Remember that these are your parameters:</p>
<ul>
<li>  criterion: the loss function</li>
<li>  n_epochs: the number of times you iterate through the entire dataset when training</li>
<li>  z_dim: the dimension of the noise vector</li>
<li>  display_step: how often to display/visualize the images</li>
<li>  batch_size: the number of images per forward/backward pass</li>
<li>  lr: the learning rate</li>
<li>  beta_1, beta_2: the momentum term</li>
<li>  device: the device type</li>
</ul>
<!-- In addition, be warned that **this runs very slowly on the default CPU**. One way to run this more quickly is to download the .ipynb and upload it to Google Drive, then open it with Google Colab, click on `Runtime -> Change runtime type` and set hardware accelerator to GPU and replace
`device = "cpu"`
with
`device = "cuda"`. The code should then run without any more changes, over 1,000 times faster.  -->



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># A learning rate of 0.0002 works well on DCGAN</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># These parameters control the optimizer&#x27;s momentum, which you can read more about here:</span></span><br><span class="line"><span class="comment"># https://distill.pub/2017/momentum/ but you don’t need to worry about it for this course!</span></span><br><span class="line">beta_1 = <span class="number">0.5</span> </span><br><span class="line">beta_2 = <span class="number">0.999</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can tranform the image values to be between -1 and 1 (the range of the tanh activation)</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">&#x27;.&#x27;</span>, download=<span class="literal">False</span>, transform=transform),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>Then, you can initialize your generator, discriminator, and optimizers.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))</span><br><span class="line">disc = Discriminator().to(device) </span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># You initialize the weights to the normal distribution</span></span><br><span class="line"><span class="comment"># with mean 0 and standard deviation 0.02</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d) <span class="keyword">or</span> <span class="built_in">isinstance</span>(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">gen = gen.apply(weights_init)</span><br><span class="line">disc = disc.apply(weights_init)</span><br></pre></td></tr></table></figure>
<p>Finally, you can train your GAN!<br>For each epoch, you will process the entire dataset in batches. For every batch, you will update the discriminator and generator. Then, you can see DCGAN’s results!</p>
<p>Here’s roughly the progression you should be expecting. On GPU this takes about 30 seconds per thousand steps. On CPU, this can take about 8 hours per thousand steps. You might notice that in the image of Step 5000, the generator is disproprotionately producing things that look like ones. If the discriminator didn’t learn to detect this imbalance quickly enough, then the generator could just produce more ones. As a result, it may have ended up tricking the discriminator so well that there would be no more improvement, known as mode collapse:<br><img src="MNIST_DCGAN_Progression.png" alt="MNIST Digits Progression"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_epochs = <span class="number">50</span></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">mean_generator_loss = <span class="number">0</span></span><br><span class="line">mean_discriminator_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = <span class="built_in">len</span>(real)</span><br><span class="line">        real = real.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Update discriminator ##</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line">        fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        fake = gen(fake_noise)</span><br><span class="line">        disc_fake_pred = disc(fake.detach())</span><br><span class="line">        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span><br><span class="line">        disc_real_pred = disc(real)</span><br><span class="line">        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span><br><span class="line">        disc_loss = (disc_fake_loss + disc_real_loss) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">        mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class="line">        <span class="comment"># Update gradients</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># Update optimizer</span></span><br><span class="line">        disc_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Update generator ##</span></span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line">        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        fake_2 = gen(fake_noise_2)</span><br><span class="line">        disc_fake_pred = disc(fake_2)</span><br><span class="line">        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span><br><span class="line">        gen_loss.backward()</span><br><span class="line">        gen_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">        mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Visualization code ##</span></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f&quot;Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>&quot;</span>)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            mean_generator_loss = <span class="number">0</span></span><br><span class="line">            mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">        cur_step += <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))</code></pre>
<p>​    </p>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))


Step 500: Generator loss: 0.9676521250009537, discriminator loss: 0.5025659155845642</code></pre>
<p><img src="output_20_4.png" alt="png"></p>
<p><img src="output_20_5.png" alt="png"></p>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))


Step 23000: Generator loss: 0.7005814965963376, discriminator loss: 0.6960778114795685</code></pre>
<p><img src="output_20_235.png" alt="png"></p>
<p><img src="output_20_236.png" alt="png"></p>
]]></content>
      <categories>
        <category>Class-notes</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Deep Learning</tag>
        <tag>Online course</tag>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title>GANs specialization Week4-1: Notes and codes</title>
    <url>/2021/05/23/GANs-specialization-Week4-Notes-and-codes/</url>
    <content><![CDATA[<h1 id="Assignment-1-Build-a-Conditional-GAN"><a href="#Assignment-1-Build-a-Conditional-GAN" class="headerlink" title="Assignment 1: Build a Conditional GAN"></a>Assignment 1: Build a Conditional GAN</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to make a conditional GAN in order to generate hand-written images of digits, conditioned on the digit to be generated (the class vector). This will let you choose what digit you want to generate.</p>
<p>You’ll then do some exploration of the generated images to visualize what the noise and class vectors mean.  </p>
<a id="more"></a>

<h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol>
<li>  Learn the technical difference between a conditional and unconditional GAN.</li>
<li>  Understand the distinction between the class and noise vector in a conditional GAN.</li>
</ol>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>For this assignment, you will be using the MNIST dataset again, but there’s nothing stopping you from applying this generator code to produce images of animals conditioned on the species or pictures of faces conditioned on facial characteristics.</p>
<p>Note that this assignment requires no changes to the architectures of the generator or discriminator, only changes to the data passed to both. The generator will no longer take <code>z_dim</code> as an argument, but  <code>input_dim</code> instead, since you need to pass in both the noise and class vectors. In addition to good variable naming, this also means that you can use the generator and discriminator code you have previously written with different parameters.</p>
<p>You will begin by importing the necessary libraries and building the generator and discriminator.</p>
<h4 id="Packages-and-Visualization"><a href="#Packages-and-Visualization" class="headerlink" title="Packages and Visualization"></a>Packages and Visualization</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for our testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span>(<span class="params">image_tensor, num_images=<span class="number">25</span>, size=(<span class="params"><span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span></span>), nrow=<span class="number">5</span>, show=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu()</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    <span class="keyword">if</span> show:</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="Generator-and-Noise"><a href="#Generator-and-Noise" class="headerlink" title="Generator and Noise"></a>Generator and Noise</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_dim=<span class="number">10</span>, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(input_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span>(<span class="params">self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN;</span></span><br><span class="line"><span class="string">        a transposed convolution, a batchnorm (except in the final layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, noise</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, input_dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        x = noise.view(<span class="built_in">len</span>(noise), self.input_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span>(<span class="params">n_samples, input_dim, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, input_dim, device=device)</span><br></pre></td></tr></table></figure>
<h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">      im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">            (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">      hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            self.make_disc_block(im_chan, hidden_dim),</span><br><span class="line">            self.make_disc_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_disc_block(hidden_dim * <span class="number">2</span>, <span class="number">1</span>, final_layer=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_disc_block</span>(<span class="params">self, input_channels, output_channels, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, final_layer=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; </span></span><br><span class="line"><span class="string">        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, image</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_chan)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        disc_pred = self.disc(image)</span><br><span class="line">        <span class="keyword">return</span> disc_pred.view(<span class="built_in">len</span>(disc_pred), -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Class-Input"><a href="#Class-Input" class="headerlink" title="Class Input"></a>Class Input</h2><p>In conditional GANs, the input vector for the generator will also need to include the class information. The class is represented using a one-hot encoded vector where its length is the number of classes and each index represents a class. The vector is all 0’s and a 1 on the chosen class. Given the labels of multiple images (e.g. from a batch) and number of classes, please create one-hot vectors for each label. There is a class within the PyTorch functional library that can help you.</p>
<details>

<summary>
<font size="3" color="green">
<b>Optional hints for <code><font size="4">get_one_hot_labels</font></code></b>
</font>
</summary>

<ol>
<li>  This code can be done in one line.</li>
<li>  The documentation for <a href="https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.one_hot">F.one_hot</a> may be helpful.</li>
</ol>
</details>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_one_hot_labels</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_hot_labels</span>(<span class="params">labels, n_classes</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        labels: tensor of labels from the dataloader, size (?)</span></span><br><span class="line"><span class="string">        n_classes: the total number of classes in the dataset, an integer scalar</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> F.one_hot(labels, n_classes).<span class="built_in">float</span>()</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">assert</span> (</span><br><span class="line">    get_one_hot_labels(</span><br><span class="line">        labels=torch.Tensor([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]]).long(),</span><br><span class="line">        n_classes=<span class="number">3</span></span><br><span class="line">    ).tolist() == </span><br><span class="line">    [[</span><br><span class="line">      [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], </span><br><span class="line">      [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], </span><br><span class="line">      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">    ]]</span><br><span class="line">)</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<p>Next, you need to be able to concatenate the one-hot class vector to the noise vector before giving it to the generator. You will also need to do this when adding the class channels to the discriminator.</p>
<p>To do this, you will need to write a function that combines two vectors. Remember that you need to ensure that the vectors are the same type: floats. Again, you can look to the PyTorch library for help.</p>
<details>
<summary>
<font size="3" color="green">
<b>Optional hints for <code><font size="4">combine_vectors</font></code></b>
</font>
</summary>

<ol>
<li>  This code can also be written in one line.</li>
<li>  The documentation for <a href="https://pytorch.org/docs/master/generated/torch.cat.html">torch.cat</a> may be helpful.</li>
<li>  Specifically, you might want to look at what the <code>dim</code> argument of <code>torch.cat</code> does.</li>
</ol>
</details>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: combine_vectors</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_vectors</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">      x: (n_samples, ?) the first vector. </span></span><br><span class="line"><span class="string">        In this assignment, this will be the noise vector of shape (n_samples, z_dim), </span></span><br><span class="line"><span class="string">        but you shouldn&#x27;t need to know the second dimension&#x27;s size.</span></span><br><span class="line"><span class="string">      y: (n_samples, ?) the second vector.</span></span><br><span class="line"><span class="string">        Once again, in this assignment this will be the one-hot class vector </span></span><br><span class="line"><span class="string">        with the shape (n_samples, n_classes), but you shouldn&#x27;t assume this in your code.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># Note: Make sure this function outputs a float no matter what inputs it receives</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    combined = torch.cat((x, y), <span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">combined = combine_vectors(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]));</span><br><span class="line"><span class="comment"># Check exact order of elements</span></span><br><span class="line"><span class="keyword">assert</span> torch.<span class="built_in">all</span>(combined == torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>]]))</span><br><span class="line"><span class="comment"># Tests that items are of float type</span></span><br><span class="line"><span class="keyword">assert</span> (<span class="built_in">type</span>(combined[<span class="number">0</span>][<span class="number">0</span>].item()) == <span class="built_in">float</span>)</span><br><span class="line"><span class="comment"># Check shapes</span></span><br><span class="line">combined = combine_vectors(torch.randn(<span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>), torch.randn(<span class="number">1</span>, <span class="number">8</span>, <span class="number">5</span>));</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(combined.shape) == (<span class="number">1</span>, <span class="number">12</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">tuple</span>(combine_vectors(torch.randn(<span class="number">1</span>, <span class="number">10</span>, <span class="number">12</span>).long(), torch.randn(<span class="number">1</span>, <span class="number">20</span>, <span class="number">12</span>).long()).shape) == (<span class="number">1</span>, <span class="number">30</span>, <span class="number">12</span>)</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Now you can start to put it all together!<br>First, you will define some new parameters:</p>
<ul>
<li>  mnist_shape: the number of pixels in each MNIST image, which has dimensions 28 x 28 and one channel (because it’s black-and-white) so 1 x 28 x 28</li>
<li>  n_classes: the number of classes in MNIST (10, since there are the digits from 0 to 9)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mnist_shape = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">n_classes = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>And you also include the same parameters from previous assignments:</p>
<ul>
<li>  criterion: the loss function</li>
<li>  n_epochs: the number of times you iterate through the entire dataset when training</li>
<li>  z_dim: the dimension of the noise vector</li>
<li>  display_step: how often to display/visualize the images</li>
<li>  batch_size: the number of images per forward/backward pass</li>
<li>  lr: the learning rate</li>
<li>  device: the device type</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">n_epochs = <span class="number">200</span></span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">&#x27;.&#x27;</span>, download=<span class="literal">False</span>, transform=transform),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>Then, you can initialize your generator, discriminator, and optimizers. To do this, you will need to update the input dimensions for both models. For the generator, you will need to calculate the size of the input vector; recall that for conditional GANs, the generator’s input is the noise vector concatenated with the class vector. For the discriminator, you need to add a channel for every class.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_input_dimensions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_input_dimensions</span>(<span class="params">z_dim, mnist_shape, n_classes</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for getting the size of the conditional input dimensions </span></span><br><span class="line"><span class="string">    from z_dim, the image shape, and number of classes.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28)</span></span><br><span class="line"><span class="string">        n_classes: the total number of classes in the dataset, an integer scalar</span></span><br><span class="line"><span class="string">                (10 for MNIST)</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        generator_input_dim: the input dimensionality of the conditional generator, </span></span><br><span class="line"><span class="string">                          which takes the noise and class vectors</span></span><br><span class="line"><span class="string">        discriminator_im_chan: the number of input channels to the discriminator</span></span><br><span class="line"><span class="string">                            (e.g. C x 28 x 28 for MNIST)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    generator_input_dim = z_dim + n_classes</span><br><span class="line">    discriminator_im_chan = mnist_shape[<span class="number">0</span>] + n_classes</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> generator_input_dim, discriminator_im_chan</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_input_dims</span>():</span></span><br><span class="line">    gen_dim, disc_dim = get_input_dimensions(<span class="number">23</span>, (<span class="number">12</span>, <span class="number">23</span>, <span class="number">52</span>), <span class="number">9</span>)</span><br><span class="line">    <span class="keyword">assert</span> gen_dim == <span class="number">32</span></span><br><span class="line">    <span class="keyword">assert</span> disc_dim == <span class="number">21</span></span><br><span class="line">test_input_dims()</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)</span><br><span class="line"></span><br><span class="line">gen = Generator(input_dim=generator_input_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">disc = Discriminator(im_chan=discriminator_im_chan).to(device)</span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d) <span class="keyword">or</span> <span class="built_in">isinstance</span>(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">gen = gen.apply(weights_init)</span><br><span class="line">disc = disc.apply(weights_init)</span><br></pre></td></tr></table></figure>
<p>Now to train, you would like both your generator and your discriminator to know what class of image should be generated. There are a few locations where you will need to implement code.</p>
<p>For example, if you’re generating a picture of the number “1”, you would need to:</p>
<ol>
<li>  Tell that to the generator, so that it knows it should be generating a “1”</li>
<li>  Tell that to the discriminator, so that it knows it should be looking at a “1”. If the discriminator is told it should be looking at a 1 but sees something that’s clearly an 8, it can guess that it’s probably fake</li>
</ol>
<p>There are no explicit unit tests here – if this block of code runs and you don’t change any of the other variables, then you’ve done it correctly!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL</span></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">generator_losses = []</span><br><span class="line">discriminator_losses = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#UNIT TEST <span class="doctag">NOTE:</span> Initializations needed for grading</span></span><br><span class="line">noise_and_labels = <span class="literal">False</span></span><br><span class="line">fake = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">fake_image_and_labels = <span class="literal">False</span></span><br><span class="line">real_image_and_labels = <span class="literal">False</span></span><br><span class="line">disc_fake_pred = <span class="literal">False</span></span><br><span class="line">disc_real_pred = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="comment"># Dataloader returns the batches and the labels</span></span><br><span class="line">    <span class="keyword">for</span> real, labels <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = <span class="built_in">len</span>(real)</span><br><span class="line">        <span class="comment"># Flatten the batch of real images from the dataset</span></span><br><span class="line">        real = real.to(device)</span><br><span class="line"></span><br><span class="line">        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)</span><br><span class="line">        image_one_hot_labels = one_hot_labels[:, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">        image_one_hot_labels = image_one_hot_labels.repeat(<span class="number">1</span>, <span class="number">1</span>, mnist_shape[<span class="number">1</span>], mnist_shape[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update discriminator ###</span></span><br><span class="line">        <span class="comment"># Zero out the discriminator gradients</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line">        <span class="comment"># Get noise corresponding to the current batch_size </span></span><br><span class="line">        fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Now you can get the images from the generator</span></span><br><span class="line">        <span class="comment"># Steps: 1) Combine the noise vectors and the one-hot labels for the generator</span></span><br><span class="line">        <span class="comment">#        2) Generate the conditioned fake images</span></span><br><span class="line">       </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        noise_and_labels = torch.cat((fake_noise,one_hot_labels), <span class="number">1</span>)</span><br><span class="line">        fake = gen(noise_and_labels)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Make sure that enough images were generated</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(fake) == <span class="built_in">len</span>(real)</span><br><span class="line">        <span class="comment"># Check that correct tensors were combined</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">tuple</span>(noise_and_labels.shape) == (cur_batch_size, fake_noise.shape[<span class="number">1</span>] + one_hot_labels.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># It comes from the correct generator</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">tuple</span>(fake.shape) == (<span class="built_in">len</span>(real), <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Now you can get the predictions from the discriminator</span></span><br><span class="line">        <span class="comment"># Steps: 1) Create the input for the discriminator</span></span><br><span class="line">        <span class="comment">#           a) Combine the fake images with image_one_hot_labels, </span></span><br><span class="line">        <span class="comment">#              remember to detach the generator (.detach()) so you do not backpropagate through it</span></span><br><span class="line">        <span class="comment">#           b) Combine the real images with image_one_hot_labels</span></span><br><span class="line">        <span class="comment">#        2) Get the discriminator&#x27;s prediction on the fakes as disc_fake_pred</span></span><br><span class="line">        <span class="comment">#        3) Get the discriminator&#x27;s prediction on the reals as disc_real_pred</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        fake_image_and_labels = torch.cat((fake.detach(), image_one_hot_labels), <span class="number">1</span>)</span><br><span class="line">        real_image_and_labels = torch.cat((real.detach(), image_one_hot_labels), <span class="number">1</span>)</span><br><span class="line">        disc_fake_pred = disc(fake_image_and_labels)</span><br><span class="line">        disc_real_pred = disc(real_image_and_labels)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Make sure shapes are correct </span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">tuple</span>(fake_image_and_labels.shape) == (<span class="built_in">len</span>(real), fake.detach().shape[<span class="number">1</span>] + image_one_hot_labels.shape[<span class="number">1</span>], <span class="number">28</span> ,<span class="number">28</span>)</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">tuple</span>(real_image_and_labels.shape) == (<span class="built_in">len</span>(real), real.shape[<span class="number">1</span>] + image_one_hot_labels.shape[<span class="number">1</span>], <span class="number">28</span> ,<span class="number">28</span>)</span><br><span class="line">        <span class="comment"># Make sure that enough predictions were made</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(disc_real_pred) == <span class="built_in">len</span>(real)</span><br><span class="line">        <span class="comment"># Make sure that the inputs are different</span></span><br><span class="line">        <span class="keyword">assert</span> torch.<span class="built_in">any</span>(fake_image_and_labels != real_image_and_labels)</span><br><span class="line">        <span class="comment"># Shapes must match</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">tuple</span>(fake_image_and_labels.shape) == <span class="built_in">tuple</span>(real_image_and_labels.shape)</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">tuple</span>(disc_fake_pred.shape) == <span class="built_in">tuple</span>(disc_real_pred.shape)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span><br><span class="line">        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span><br><span class="line">        disc_loss = (disc_fake_loss + disc_real_loss) / <span class="number">2</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">        disc_opt.step() </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">        discriminator_losses += [disc_loss.item()]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update generator ###</span></span><br><span class="line">        <span class="comment"># Zero out the generator gradients</span></span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)</span><br><span class="line">        <span class="comment"># This will error if you didn&#x27;t concatenate your labels to your image correctly</span></span><br><span class="line">        disc_fake_pred = disc(fake_image_and_labels)</span><br><span class="line">        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span><br><span class="line">        gen_loss.backward()</span><br><span class="line">        gen_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the generator losses</span></span><br><span class="line">        generator_losses += [gen_loss.item()]</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            gen_mean = <span class="built_in">sum</span>(generator_losses[-display_step:]) / display_step</span><br><span class="line">            disc_mean = <span class="built_in">sum</span>(discriminator_losses[-display_step:]) / display_step</span><br><span class="line">            print(<span class="string">f&quot;Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;gen_mean&#125;</span>, discriminator loss: <span class="subst">&#123;disc_mean&#125;</span>&quot;</span>)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            step_bins = <span class="number">20</span></span><br><span class="line">            x_axis = <span class="built_in">sorted</span>([i * step_bins <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(generator_losses) // step_bins)] * step_bins)</span><br><span class="line">            num_examples = (<span class="built_in">len</span>(generator_losses) // step_bins) * step_bins</span><br><span class="line">            plt.plot(</span><br><span class="line">                <span class="built_in">range</span>(num_examples // step_bins), </span><br><span class="line">                torch.Tensor(generator_losses[:num_examples]).view(-<span class="number">1</span>, step_bins).mean(<span class="number">1</span>),</span><br><span class="line">                label=<span class="string">&quot;Generator Loss&quot;</span></span><br><span class="line">            )</span><br><span class="line">            plt.plot(</span><br><span class="line">                <span class="built_in">range</span>(num_examples // step_bins), </span><br><span class="line">                torch.Tensor(discriminator_losses[:num_examples]).view(-<span class="number">1</span>, step_bins).mean(<span class="number">1</span>),</span><br><span class="line">                label=<span class="string">&quot;Discriminator Loss&quot;</span></span><br><span class="line">            )</span><br><span class="line">            plt.legend()</span><br><span class="line">            plt.show()</span><br><span class="line">        <span class="keyword">elif</span> cur_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&quot;Congratulations! If you&#x27;ve gotten here, it&#x27;s working. Please let this train until you&#x27;re happy with how the generated numbers look, and then go on to the exploration!&quot;</span>)</span><br><span class="line">        cur_step += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<pre><code>Step 87500: Generator loss: 0.9753798842430115, discriminator loss: 0.6099519475102425</code></pre>
<p><img src="output_24_1070.png" alt="png"></p>
<p><img src="output_24_1071.png" alt="png"></p>
<p><img src="output_24_1072.png" alt="png"></p>
<h2 id="Exploration"><a href="#Exploration" class="headerlink" title="Exploration"></a>Exploration</h2><p>You can do a bit of exploration now!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Before you explore, you should put the generator</span></span><br><span class="line"><span class="comment"># in eval mode, both in general and so that batch norm</span></span><br><span class="line"><span class="comment"># doesn&#x27;t cause you issues and is using its eval statistics</span></span><br><span class="line">gen = gen.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<h4 id="Changing-the-Class-Vector"><a href="#Changing-the-Class-Vector" class="headerlink" title="Changing the Class Vector"></a>Changing the Class Vector</h4><p>You can generate some numbers with your new model! You can add interpolation as well to make it more interesting.</p>
<p>So starting from a image, you will produce intermediate images that look more and more like the ending image until you get to the final image. Your’re basically morphing one image into another. You can choose what these two images will be using your conditional GAN.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">n_interpolation = <span class="number">9</span> <span class="comment"># Choose the interpolation: how many intermediate images you want + 2 (for the start and end image)</span></span><br><span class="line">interpolation_noise = get_noise(<span class="number">1</span>, z_dim, device=device).repeat(n_interpolation, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">interpolate_class</span>(<span class="params">first_number, second_number</span>):</span></span><br><span class="line">    first_label = get_one_hot_labels(torch.Tensor([first_number]).long(), n_classes)</span><br><span class="line">    second_label = get_one_hot_labels(torch.Tensor([second_number]).long(), n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the interpolation vector between the two labels</span></span><br><span class="line">    percent_second_label = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, n_interpolation)[:, <span class="literal">None</span>]</span><br><span class="line">    interpolation_labels = first_label * (<span class="number">1</span> - percent_second_label) + second_label * percent_second_label</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Combine the noise and the labels</span></span><br><span class="line">    noise_and_labels = combine_vectors(interpolation_noise, interpolation_labels.to(device))</span><br><span class="line">    fake = gen(noise_and_labels)</span><br><span class="line">    show_tensor_images(fake, num_images=n_interpolation, nrow=<span class="built_in">int</span>(math.sqrt(n_interpolation)), show=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">start_plot_number = <span class="number">1</span> <span class="comment"># Choose the start digit</span></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">end_plot_number = <span class="number">5</span> <span class="comment"># Choose the end digit</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">interpolate_class(start_plot_number, end_plot_number)</span><br><span class="line">_ = plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Uncomment the following lines of code if you would like to visualize a set of pairwise class </span></span><br><span class="line"><span class="comment">## interpolations for a collection of different numbers, all in a single grid of interpolations.</span></span><br><span class="line"><span class="comment">## You&#x27;ll also see another visualization like this in the next code block!</span></span><br><span class="line">plot_numbers = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">7</span>]</span><br><span class="line">n_numbers = <span class="built_in">len</span>(plot_numbers)</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i, first_plot_number <span class="keyword">in</span> <span class="built_in">enumerate</span>(plot_numbers):</span><br><span class="line">    <span class="keyword">for</span> j, second_plot_number <span class="keyword">in</span> <span class="built_in">enumerate</span>(plot_numbers):</span><br><span class="line">        plt.subplot(n_numbers, n_numbers, i * n_numbers + j + <span class="number">1</span>)</span><br><span class="line">        interpolate_class(first_plot_number, second_plot_number)</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.subplots_adjust(top=<span class="number">1</span>, bottom=<span class="number">0</span>, left=<span class="number">0</span>, right=<span class="number">1</span>, hspace=<span class="number">0.1</span>, wspace=<span class="number">0</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure>

<p><img src="output_28_0.png" alt="png"></p>
<p><img src="output_28_1.png" alt="png"></p>
<h4 id="Changing-the-Noise-Vector"><a href="#Changing-the-Noise-Vector" class="headerlink" title="Changing the Noise Vector"></a>Changing the Noise Vector</h4><p>Now, what happens if you hold the class constant, but instead you change the noise vector? You can also interpolate the noise vector and generate an image at each step.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_interpolation = <span class="number">9</span> <span class="comment"># How many intermediate images you want + 2 (for the start and end image)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This time you&#x27;re interpolating between the noise instead of the labels</span></span><br><span class="line">interpolation_label = get_one_hot_labels(torch.Tensor([<span class="number">5</span>]).long(), n_classes).repeat(n_interpolation, <span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">interpolate_noise</span>(<span class="params">first_noise, second_noise</span>):</span></span><br><span class="line">    <span class="comment"># This time you&#x27;re interpolating between the noise instead of the labels</span></span><br><span class="line">    percent_first_noise = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, n_interpolation)[:, <span class="literal">None</span>].to(device)</span><br><span class="line">    interpolation_noise = first_noise * percent_first_noise + second_noise * (<span class="number">1</span> - percent_first_noise)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Combine the noise and the labels again</span></span><br><span class="line">    noise_and_labels = combine_vectors(interpolation_noise, interpolation_label.to(device))</span><br><span class="line">    fake = gen(noise_and_labels)</span><br><span class="line">    show_tensor_images(fake, num_images=n_interpolation, nrow=<span class="built_in">int</span>(math.sqrt(n_interpolation)), show=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate noise vectors to interpolate between</span></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">n_noise = <span class="number">5</span> <span class="comment"># Choose the number of noise examples in the grid</span></span><br><span class="line">plot_noises = [get_noise(<span class="number">1</span>, z_dim, device=device) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_noise)]</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i, first_plot_noise <span class="keyword">in</span> <span class="built_in">enumerate</span>(plot_noises):</span><br><span class="line">    <span class="keyword">for</span> j, second_plot_noise <span class="keyword">in</span> <span class="built_in">enumerate</span>(plot_noises):</span><br><span class="line">        plt.subplot(n_noise, n_noise, i * n_noise + j + <span class="number">1</span>)</span><br><span class="line">        interpolate_noise(first_plot_noise, second_plot_noise)</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.subplots_adjust(top=<span class="number">1</span>, bottom=<span class="number">0</span>, left=<span class="number">0</span>, right=<span class="number">1</span>, hspace=<span class="number">0.1</span>, wspace=<span class="number">0</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure>

<p><img src="output_30_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Class notes</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Online course</tag>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title>GANs specialization Week1: Notes and codes</title>
    <url>/2021/04/26/GANs-specialization-Week1-Notes-and-codes/</url>
    <content><![CDATA[<p>This is week 1 notes for the first course (<a href="https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans">Build Basic Generative Adversarial Networks (GANs)</a>)  in the <a href="https://www.coursera.org/specializations/generative-adversarial-networks-gans">GANs specialization</a>. I actually leant a lot about some pratical things in doing the assignment, like how to set some hyperparameters and the usage of truncation to balance the diversity and quality.  I really like DeepLearning.AI’s courses. The lectures and assignments are always well-designed.</p>
<a id="more"></a>

<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><ul>
<li><p>Generator needs more time steps to train</p>
<blockquote>
<p> This can be a much harder task than discrimination. Typically, you will need the generator to take multiple steps to improve itself for every step the discriminator takes.</p>
</blockquote>
</li>
<li><p>Noise vector 𝑧</p>
<blockquote>
<p> The noise vector 𝑧 has the important role of making sure the images generated from the same class 𝑦 don’t all look the same—think of it as a random seed. You generate it randomly, usually by sampling random numbers either between 0 and 1 uniformly, or from the normal distribution, which you can denote 𝑧 ~ 𝑁(0,1). The zero means the normal distribution has a mean of zero, and the 1 means that the normal distribution has a variance of 1.</p>
</blockquote>
<blockquote>
<p>In reality, 𝑧 is usually larger than just 1 value to allow for more combinations of what 𝑧 could be. There’s no special number that determines what works, but 100 is standard. Some researchers might use a power of 2, like 128 or 512, but again, nothing special about the number itself, just that it’s large enough to contain a lot of possibilities. As a result, you would sample 𝑧 from that many different dimensions (constituting multiple normal distributions).</p>
</blockquote>
<blockquote>
<p>Fun Fact: this is also called a spherical normal and denoted 𝑧 ~ 𝑁(0,𝐼) where the 𝐼 represents the identity matrix and means the variance is 1 in all dimensions.*</p>
</blockquote>
</li>
<li><p>Truncation trick</p>
<blockquote>
<p>So now that you’re a bit familiar with noise vectors, here’s another cool concept that people use to tune their outputs. It’s called the truncation trick. I like to think of the truncation trick as a way of trading off fidelity (quality) and diversity in the samples. It works like this: when you randomly sample your noise vector 𝑧, you can choose to keep that random 𝑧 or you can sample another one.</p>
</blockquote>
<blockquote>
<p>Why would you want to sample another one?</p>
</blockquote>
<blockquote>
<p>Well, since I’m sampling 𝑧 from a normal distribution, my model will see more of those 𝑧 values within a standard deviation from the mean than those at the tails of the distribution—and this happens during training. This means that while the model is training, it’s likely to be familiar with certain noise vectors and as a result model those areas coming from familiar noise vector regions. In these areas, my model will likely have much more realistic results, but nothing too funky, it’s not taking as many risks in those regions mapped from those familiar noise vectors. This is the trade-off between fidelity (realistic, high quality images) and diversity (variety in images).</p>
</blockquote>
<p><img src="image-20210426120534698.png" alt="image-20210426120534698"></p>
<blockquote>
<p>Image Credit: Modelica</p>
</blockquote>
<blockquote>
<p>What the truncation trick does is resamples the noise vector 𝑧 until it falls within some bounds of the normal distribution. In fact, it samples 𝑧 from a truncated normal distribution where the tails are cut off at different values (red line in graph is truncated normal, blue is original). You can tune these values and thus tune fidelity/diversity. Recall that having a lot of fidelity is not always the goal—one failure mode of that is that you get one really real image but nothing else (no diversity), and that’s not very interesting or successful from a model that’s supposed to model the realm of all possible human faces or that of all possible coconuts—including that of a cat pouncing after a flying coconut (but with extremely low probability).</p>
</blockquote>
<blockquote>
<p><strong>truncation</strong>: The positive truncation value. 1 is low truncation (high diversity) and 0 is all truncation except for the mean (high quality/fidelity). A lower value increases fidelity and decreases diversity, and vice versa.</p>
</blockquote>
<p><img src="image-20210426120448703.png" alt="image-20210426120448703"></p>
</li>
<li><p>Playing with code</p>
<p>interpolation</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z_dim = Gs.input_shape[<span class="number">1</span>]</span><br><span class="line">first_noise = rnd.randn(<span class="number">1</span>, z_dim)</span><br><span class="line">second_noise = rnd.randn(<span class="number">1</span>, z_dim)</span><br><span class="line">percent_first_noise = np.linspace(<span class="number">0</span>, <span class="number">1</span>, n_interpolation)[:, <span class="literal">None</span>]</span><br><span class="line">interpolation_noise = first_noise * percent_first_noise + second_noise * (<span class="number">1</span> - percent_first_noise)</span><br></pre></td></tr></table></figure></li>
<li><p>Random vector</p>
<blockquote>
<p>Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. torch.ones(3, 3, device=device), or move it onto the target device using torch.ones(3, 3).to(device). You do not need to do this if you’re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as torch.ones_like. In general, use torch.ones_like and torch.zeros_like instead of torch.ones or torch.zeros where possible.</p>
</blockquote>
</li>
<li><p>HW</p>
<blockquote>
<p>Since the generator is needed when calculating the discriminator’s loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!</p>
</blockquote>
</li>
</ul>
<h1 id="Week-1-Assignment-Your-First-GAN"><a href="#Week-1-Assignment-Your-First-GAN" class="headerlink" title="Week 1 Assignment: Your First GAN"></a>Week 1 Assignment: Your First GAN</h1><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><p>In this notebook, you’re going to create your first generative adversarial network (GAN) for this course! Specifically, you will build and train a GAN that can generate hand-written images of digits (0-9). You will be using PyTorch in this specialization, so if you’re not familiar with this framework, you may find the <a href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a> useful. The hints will also often include links to relevant documentation.</p>
<h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol>
<li>  Build the generator and discriminator components of a GAN from scratch.</li>
<li>  Create generator and discriminator loss functions.</li>
<li>  Train your GAN and visualize the generated images.</li>
</ol>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>You will begin by importing some useful packages and the dataset you will use to build and train your GAN. You are also provided with a visualizer function to help you investigate the images your GAN will create.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST <span class="comment"># Training dataset</span></span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span>(<span class="params">image_tensor, num_images=<span class="number">25</span>, size=(<span class="params"><span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span></span>)</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in a uniform grid.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu().view(-<span class="number">1</span>, *size)</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="MNIST-Dataset"><a href="#MNIST-Dataset" class="headerlink" title="MNIST Dataset"></a>MNIST Dataset</h4><p>The training images your discriminator will be using is from a dataset called <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>. It contains 60,000 images of handwritten digits, from 0 to 9, like these:</p>
<p>You may notice that the images are quite pixelated – this is because they are all only 28 x 28! The small size of its images makes MNIST ideal for simple training. Additionally, these images are also in black-and-white so only one dimension, or “color channel”, is needed to represent them (more on this later in the course).</p>
<h4 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h4><p>You will represent the data using <a href="https://pytorch.org/docs/stable/tensors.html">tensors</a>. Tensors are a generalization of matrices: for example, a stack of three matrices with the amounts of red, green, and blue at different locations in a 64 x 64 pixel image is a tensor with the shape 3 x 64 x 64.</p>
<p>Tensors are easy to manipulate and supported by <a href="https://pytorch.org/">PyTorch</a>, the machine learning library you will be using. Feel free to explore them more, but you can imagine these as multi-dimensional matrices or vectors!</p>
<h4 id="Batches"><a href="#Batches" class="headerlink" title="Batches"></a>Batches</h4><p>While you could train your model after generating one image, it is extremely inefficient and leads to less stable training. In GANs, and in machine learning in general, you will process multiple images per training step. These are called batches.</p>
<p>This means that your generator will generate an entire batch of images and receive the discriminator’s feedback on each before updating the model. The same goes for the discriminator, it will calculate its loss on the entire batch of generated images as well as on the reals before the model is updated.</p>
<h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>The first step is to build the generator component.</p>
<p>You will start by creating a function to make a single layer/block for the generator’s neural network. Each block should include a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">linear transformation</a> to map to another shape, a <a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html">batch normalization</a> for stabilization, and finally a non-linear activation function (you use a <a href="https://pytorch.org/docs/master/generated/torch.nn.ReLU.html">ReLU here</a>) so the output can be transformed in complex ways. You will learn more about activations and batch normalization later in the course.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_generator_block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_generator_block</span>(<span class="params">input_dim, output_dim</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for returning a block of the generator&#x27;s neural network</span></span><br><span class="line"><span class="string">    given input and output dimensions.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        output_dim: the dimension of the output vector, a scalar</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        a generator neural network layer, with a linear transformation </span></span><br><span class="line"><span class="string">          followed by a batch normalization and then a relu activation</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        <span class="comment"># Hint: Replace all of the &quot;None&quot; with the appropriate dimensions.</span></span><br><span class="line">        <span class="comment"># The documentation may be useful if you&#x27;re less familiar with PyTorch:</span></span><br><span class="line">        <span class="comment"># https://pytorch.org/docs/stable/nn.html.</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        nn.Linear(input_dim, output_dim),</span><br><span class="line">        nn.BatchNorm1d(output_dim),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Verify the generator block function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_block</span>(<span class="params">in_features, out_features, num_test=<span class="number">1000</span></span>):</span></span><br><span class="line">    block = get_generator_block(in_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check the three parts</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(block) == <span class="number">3</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">type</span>(block[<span class="number">0</span>]) == nn.Linear</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">type</span>(block[<span class="number">1</span>]) == nn.BatchNorm1d</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">type</span>(block[<span class="number">2</span>]) == nn.ReLU</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check the output shape</span></span><br><span class="line">    test_input = torch.randn(num_test, in_features)</span><br><span class="line">    test_output = block(test_input)</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">tuple</span>(test_output.shape) == (num_test, out_features)</span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &gt; <span class="number">0.55</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &lt; <span class="number">0.65</span></span><br><span class="line"></span><br><span class="line">test_gen_block(<span class="number">25</span>, <span class="number">12</span>)</span><br><span class="line">test_gen_block(<span class="number">15</span>, <span class="number">28</span>)</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<p>Now you can build the generator class. It will take 3 values:</p>
<ul>
<li>  The noise vector dimension</li>
<li>  The image dimension</li>
<li>  The initial hidden dimension</li>
</ul>
<p>Using these values, the generator will build a neural network with 5 layers/blocks. Beginning with the noise vector, the generator will apply non-linear transformations via the block function until the tensor is mapped to the size of the image to be outputted (the same size as the real images from MNIST). You will need to fill in the code for final layer since it is different than the others. The final layer does not need a normalization or activation function, but does need to be scaled with a <a href="https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html">sigmoid function</a>. </p>
<p>Finally, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network.</p>
<details>


<summary>
<font size="3" color="green">
<b>Optional hints for <code><font size="4">Generator</font></code></b>
</font>
</summary>

<ol>
<li>The output size of the final linear transformation should be im_dim, but remember you need to scale the outputs between 0 and 1 using the sigmoid function.</li>
<li><a href="https://pytorch.org/docs/master/generated/torch.nn.Linear.html">nn.Linear</a> and <a href="https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html">nn.Sigmoid</a> will be useful here. </details>



</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Generator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_dim: the dimension of the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">          (MNIST images are 28 x 28 = 784 so that is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, z_dim=<span class="number">10</span>, im_dim=<span class="number">784</span>, hidden_dim=<span class="number">128</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            get_generator_block(z_dim, hidden_dim),</span><br><span class="line">            get_generator_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            get_generator_block(hidden_dim * <span class="number">2</span>, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            get_generator_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">8</span>),</span><br><span class="line">            <span class="comment"># There is a dropdown with hints if you need them! </span></span><br><span class="line">            <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">            nn.Linear(hidden_dim * <span class="number">8</span>, im_dim),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">            <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, noise</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> self.gen(noise)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Needed for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_gen</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the sequential model</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> self.gen</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Verify the generator class</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_generator</span>(<span class="params">z_dim, im_dim, hidden_dim, num_test=<span class="number">10000</span></span>):</span></span><br><span class="line">    gen = Generator(z_dim, im_dim, hidden_dim).get_gen()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check there are six modules in the sequential part</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(gen) == <span class="number">6</span></span><br><span class="line">    test_input = torch.randn(num_test, z_dim)</span><br><span class="line">    test_output = gen(test_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check that the output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">tuple</span>(test_output.shape) == (num_test, im_dim)</span><br><span class="line">    <span class="keyword">assert</span> test_output.<span class="built_in">max</span>() &lt; <span class="number">1</span>, <span class="string">&quot;Make sure to use a sigmoid&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.<span class="built_in">min</span>() &gt; <span class="number">0</span>, <span class="string">&quot;Make sure to use a sigmoid&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.<span class="built_in">min</span>() &lt; <span class="number">0.5</span>, <span class="string">&quot;Don&#x27;t use a block in your solution&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &gt; <span class="number">0.05</span>, <span class="string">&quot;Don&#x27;t use batchnorm here&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &lt; <span class="number">0.15</span>, <span class="string">&quot;Don&#x27;t use batchnorm here&quot;</span></span><br><span class="line"></span><br><span class="line">test_generator(<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">test_generator(<span class="number">20</span>, <span class="number">8</span>, <span class="number">24</span>)</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<h2 id="Noise"><a href="#Noise" class="headerlink" title="Noise"></a>Noise</h2><p>To be able to use your generator, you will need to be able to create noise vectors. The noise vector z has the important role of making sure the images generated from the same class don’t all look the same – think of it as a random seed. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, you will generate all the noise vectors at once.</p>
<p>Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. <code>torch.ones(3, 3, device=device)</code>, or move it onto the target device using <code>torch.ones(3, 3).to(device)</code>. You do not need to do this if you’re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as <code>torch.ones_like</code>. In general, use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code> where possible.</p>
<details>


<summary>
<font size="3" color="green">
<b>Optional hint for <code><font size="4">get_noise</font></code></b>
</font>
</summary>

<ol>
<li>You will probably find <a href="https://pytorch.org/docs/master/generated/torch.randn.html">torch.randn</a> useful here.</details>


</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_noise</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span>(<span class="params">n_samples, z_dim, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> To use this on GPU with device=&#x27;cuda&#x27;, make sure to pass the device </span></span><br><span class="line">    <span class="comment"># argument to the function you use to generate the noise.</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device=device)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Verify the noise vector function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_get_noise</span>(<span class="params">n_samples, z_dim, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span></span><br><span class="line">    noise = get_noise(n_samples, z_dim, device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make sure a normal distribution was used</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">tuple</span>(noise.shape) == (n_samples, z_dim)</span><br><span class="line">    <span class="keyword">assert</span> torch.<span class="built_in">abs</span>(noise.std() - torch.tensor(<span class="number">1.0</span>)) &lt; <span class="number">0.01</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">str</span>(noise.device).startswith(device)</span><br><span class="line"></span><br><span class="line">test_get_noise(<span class="number">1000</span>, <span class="number">100</span>, <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    test_get_noise(<span class="number">1000</span>, <span class="number">32</span>, <span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>The second component that you need to construct is the discriminator. As with the generator component, you will start by creating a function that builds a neural network block for the discriminator.</p>
<p><em>Note: You use leaky ReLUs to prevent the “dying ReLU” problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient. You will learn more about this in the following lectures!</em> </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_discriminator_block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_discriminator_block</span>(<span class="params">input_dim, output_dim</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Discriminator Block</span></span><br><span class="line"><span class="string">    Function for returning a neural network of the discriminator given input and output dimensions.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        output_dim: the dimension of the output vector, a scalar</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        a discriminator neural network layer, with a linear transformation </span></span><br><span class="line"><span class="string">          followed by an nn.LeakyReLU activation with negative slope of 0.2 </span></span><br><span class="line"><span class="string">          (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        nn.Linear(input_dim, output_dim),</span><br><span class="line">        nn.LeakyReLU(negative_slope=<span class="number">0.2</span>)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Verify the discriminator block function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_disc_block</span>(<span class="params">in_features, out_features, num_test=<span class="number">10000</span></span>):</span></span><br><span class="line">    block = get_discriminator_block(in_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check there are two parts</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(block) == <span class="number">2</span></span><br><span class="line">    test_input = torch.randn(num_test, in_features)</span><br><span class="line">    test_output = block(test_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check that the shape is right</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">tuple</span>(test_output.shape) == (num_test, out_features)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check that the LeakyReLU slope is about 0.2</span></span><br><span class="line">    <span class="keyword">assert</span> -test_output.<span class="built_in">min</span>() / test_output.<span class="built_in">max</span>() &gt; <span class="number">0.1</span></span><br><span class="line">    <span class="keyword">assert</span> -test_output.<span class="built_in">min</span>() / test_output.<span class="built_in">max</span>() &lt; <span class="number">0.3</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &gt; <span class="number">0.3</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &lt; <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">test_disc_block(<span class="number">25</span>, <span class="number">12</span>)</span><br><span class="line">test_disc_block(<span class="number">15</span>, <span class="number">28</span>)</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<p>Now you can use these blocks to make a discriminator! The discriminator class holds 2 values:</p>
<ul>
<li>  The image dimension</li>
<li>  The hidden dimension</li>
</ul>
<p>The discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator’s neural network you are given a forward pass function that takes in an image tensor to be classified.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Discriminator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        im_dim: the dimension of the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">            (MNIST images are 28x28 = 784 so that is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, im_dim=<span class="number">784</span>, hidden_dim=<span class="number">128</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            get_discriminator_block(im_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            get_discriminator_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            get_discriminator_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            <span class="comment"># Hint: You want to transform the final output into a single value,</span></span><br><span class="line">            <span class="comment">#       so add one more linear map.</span></span><br><span class="line">            <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">            nn.Linear(hidden_dim, <span class="number">1</span>)</span><br><span class="line">            <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, image</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> self.disc(image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Needed for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_disc</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the sequential model</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> self.disc</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Verify the discriminator class</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_discriminator</span>(<span class="params">z_dim, hidden_dim, num_test=<span class="number">100</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    disc = Discriminator(z_dim, hidden_dim).get_disc()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check there are three parts</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(disc) == <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check the linear layer is correct</span></span><br><span class="line">    test_input = torch.randn(num_test, z_dim)</span><br><span class="line">    test_output = disc(test_input)</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">tuple</span>(test_output.shape) == (num_test, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Don&#x27;t use a block</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(disc[-<span class="number">1</span>], nn.Sequential)</span><br><span class="line"></span><br><span class="line">test_discriminator(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">test_discriminator(<span class="number">20</span>, <span class="number">8</span>)</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Now you can put it all together!<br>First, you will set your parameters:</p>
<ul>
<li>  criterion: the loss function</li>
<li>  n_epochs: the number of times you iterate through the entire dataset when training</li>
<li>  z_dim: the dimension of the noise vector</li>
<li>  display_step: how often to display/visualize the images</li>
<li>  batch_size: the number of images per forward/backward pass</li>
<li>  lr: the learning rate</li>
<li>  device: the device type, here using a GPU (which runs CUDA), not CPU</li>
</ul>
<p>Next, you will load the MNIST dataset as tensors using a dataloader.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set your parameters</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Notes for nn.BCEWithLogitsLoss():</span></span><br><span class="line"><span class="string">This loss combines a Sigmoid layer and the BCELoss in one single class. </span></span><br><span class="line"><span class="string">This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, </span></span><br><span class="line"><span class="string">by combining the operations into one layer, </span></span><br><span class="line"><span class="string">we take advantage of the log-sum-exp trick for numerical stability.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">n_epochs = <span class="number">200</span></span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load MNIST dataset as tensors</span></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">&#x27;.&#x27;</span>, download=<span class="literal">False</span>, transform=transforms.ToTensor()),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### DO NOT EDIT ###</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span></span><br></pre></td></tr></table></figure>
<p>Now, you can initialize your generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">disc = Discriminator().to(device) </span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br></pre></td></tr></table></figure>
<p>Before you train your GAN, you will need to create functions to calculate the discriminator’s loss and the generator’s loss. This is how the discriminator and generator will know how they are doing and improve themselves. <strong>Since the generator is needed when calculating the discriminator’s loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!</strong></p>
<p>Remember that you have already defined a loss function earlier (<code>criterion</code>) and you are encouraged to use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code>. If you use <code>torch.ones</code> or <code>torch.zeros</code>, you’ll need to pass <code>device=device</code> to them.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_disc_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_disc_loss</span>(<span class="params">gen, disc, criterion, real, num_images, z_dim, device</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Return the loss of the discriminator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        gen: the generator model, which returns an image given z-dimensional noise</span></span><br><span class="line"><span class="string">        disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span></span><br><span class="line"><span class="string">        criterion: the loss function, which should be used to compare </span></span><br><span class="line"><span class="string">               the discriminator&#x27;s predictions to the ground truth reality of the images </span></span><br><span class="line"><span class="string">               (e.g. fake = 0, real = 1)</span></span><br><span class="line"><span class="string">        real: a batch of real images</span></span><br><span class="line"><span class="string">        num_images: the number of images the generator should produce, </span></span><br><span class="line"><span class="string">                which is also the length of the real images</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        disc_loss: a torch scalar loss value for the current batch</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#     These are the steps you will need to complete:</span></span><br><span class="line">    <span class="comment">#       1) Create noise vectors and generate a batch (num_images) of fake images. </span></span><br><span class="line">    <span class="comment">#            Make sure to pass the device argument to the noise.</span></span><br><span class="line">    <span class="comment">#       2) Get the discriminator&#x27;s prediction of the fake image </span></span><br><span class="line">    <span class="comment">#            and calculate the loss. Don&#x27;t forget to detach the generator!</span></span><br><span class="line">    <span class="comment">#            (Remember the loss function you set earlier -- criterion. You need a </span></span><br><span class="line">    <span class="comment">#            &#x27;ground truth&#x27; tensor in order to calculate the loss. </span></span><br><span class="line">    <span class="comment">#            For example, a ground truth tensor for a fake image is all zeros.)</span></span><br><span class="line">    <span class="comment">#       3) Get the discriminator&#x27;s prediction of the real image and calculate the loss.</span></span><br><span class="line">    <span class="comment">#       4) Calculate the discriminator&#x27;s loss by averaging the real and fake loss</span></span><br><span class="line">    <span class="comment">#            and set it to disc_loss.</span></span><br><span class="line">    <span class="comment">#     Note: Please do not use concatenation in your solution. The tests are being updated to </span></span><br><span class="line">    <span class="comment">#           support this, but for now, average the two losses as described in step (4).</span></span><br><span class="line">    <span class="comment">#     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="comment"># step 1</span></span><br><span class="line">    noise = get_noise(num_images, z_dim, device)</span><br><span class="line">    fake_imgs = gen(noise)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># step 2</span></span><br><span class="line">    disc_output_fake = disc(fake_imgs.detach())</span><br><span class="line">    ground_truth_fake = torch.zeros_like(disc_output_fake)</span><br><span class="line">    loss_fake = criterion(disc_output_fake, ground_truth_fake)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># step 3</span></span><br><span class="line">    disc_output_real = disc(real)</span><br><span class="line">    ground_truth_real = torch.ones_like(disc_output_real)</span><br><span class="line">    loss_real = criterion(disc_output_real, ground_truth_real)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3tep 4</span></span><br><span class="line">    disc_loss = (loss_fake + loss_real) / <span class="number">2.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> disc_loss</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_disc_reasonable</span>(<span class="params">num_images=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="comment"># Don&#x27;t use explicit casts to cuda - use the device argument</span></span><br><span class="line">    <span class="keyword">import</span> inspect, re</span><br><span class="line">    lines = inspect.getsource(get_disc_loss)</span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r&quot;to\(.cuda.\)&quot;</span>, lines)) <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r&quot;\.cuda\(\)&quot;</span>, lines)) <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = torch.zeros_like</span><br><span class="line">    disc = <span class="keyword">lambda</span> x: x.mean(<span class="number">1</span>)[:, <span class="literal">None</span>]</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    real = torch.ones(num_images, z_dim)</span><br><span class="line">    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.<span class="built_in">all</span>(torch.<span class="built_in">abs</span>(disc_loss.mean() - <span class="number">0.5</span>) &lt; <span class="number">1e-5</span>)</span><br><span class="line">    </span><br><span class="line">    gen = torch.ones_like</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    real = torch.zeros(num_images, z_dim)</span><br><span class="line">    <span class="keyword">assert</span> torch.<span class="built_in">all</span>(torch.<span class="built_in">abs</span>(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class="string">&#x27;cpu&#x27;</span>)) &lt; <span class="number">1e-5</span>)</span><br><span class="line">    </span><br><span class="line">    gen = <span class="keyword">lambda</span> x: torch.ones(num_images, <span class="number">10</span>)</span><br><span class="line">    disc = <span class="keyword">lambda</span> x: x.mean(<span class="number">1</span>)[:, <span class="literal">None</span>] + <span class="number">10</span></span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    real = torch.zeros(num_images, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.<span class="built_in">all</span>(torch.<span class="built_in">abs</span>(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class="string">&#x27;cpu&#x27;</span>).mean() - <span class="number">5</span>) &lt; <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">    gen = torch.ones_like</span><br><span class="line">    disc = nn.Linear(<span class="number">64</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    real = torch.ones(num_images, <span class="number">64</span>) * <span class="number">0.5</span></span><br><span class="line">    disc.weight.data = torch.ones_like(disc.weight.data) * <span class="number">0.5</span></span><br><span class="line">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line">    criterion = <span class="keyword">lambda</span> x, y: torch.<span class="built_in">sum</span>(x) + torch.<span class="built_in">sum</span>(y)</span><br><span class="line">    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class="string">&#x27;cpu&#x27;</span>).mean()</span><br><span class="line">    disc_loss.backward()</span><br><span class="line">    <span class="keyword">assert</span> torch.isclose(torch.<span class="built_in">abs</span>(disc.weight.grad.mean() - <span class="number">11.25</span>), torch.tensor(<span class="number">3.75</span>))</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_disc_loss</span>(<span class="params">max_tests = <span class="number">10</span></span>):</span></span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = Generator(z_dim).to(device)</span><br><span class="line">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">    disc = Discriminator().to(device) </span><br><span class="line">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line">    num_steps = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> dataloader:</span><br><span class="line">        cur_batch_size = <span class="built_in">len</span>(real)</span><br><span class="line">        real = real.view(cur_batch_size, -<span class="number">1</span>).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update discriminator ###</span></span><br><span class="line">        <span class="comment"># Zero out the gradient before backpropagation</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate discriminator loss</span></span><br><span class="line">        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)</span><br><span class="line">        <span class="keyword">assert</span> (disc_loss - <span class="number">0.68</span>).<span class="built_in">abs</span>() &lt; <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update gradients</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check that they detached correctly</span></span><br><span class="line">        <span class="keyword">assert</span> gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.grad <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update optimizer</span></span><br><span class="line">        old_weight = disc.disc[<span class="number">0</span>][<span class="number">0</span>].weight.data.clone()</span><br><span class="line">        disc_opt.step()</span><br><span class="line">        new_weight = disc.disc[<span class="number">0</span>][<span class="number">0</span>].weight.data</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check that some discriminator weights changed</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.<span class="built_in">all</span>(torch.eq(old_weight, new_weight))</span><br><span class="line">        num_steps += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> num_steps &gt;= max_tests:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">test_disc_reasonable()</span><br><span class="line">test_disc_loss()</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gen_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_loss</span>(<span class="params">gen, disc, criterion, num_images, z_dim, device</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Return the loss of the generator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        gen: the generator model, which returns an image given z-dimensional noise</span></span><br><span class="line"><span class="string">        disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span></span><br><span class="line"><span class="string">        criterion: the loss function, which should be used to compare </span></span><br><span class="line"><span class="string">               the discriminator&#x27;s predictions to the ground truth reality of the images </span></span><br><span class="line"><span class="string">               (e.g. fake = 0, real = 1)</span></span><br><span class="line"><span class="string">        num_images: the number of images the generator should produce, </span></span><br><span class="line"><span class="string">                which is also the length of the real images</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        gen_loss: a torch scalar loss value for the current batch</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#     These are the steps you will need to complete:</span></span><br><span class="line">    <span class="comment">#       1) Create noise vectors and generate a batch of fake images. </span></span><br><span class="line">    <span class="comment">#           Remember to pass the device argument to the get_noise function.</span></span><br><span class="line">    <span class="comment">#       2) Get the discriminator&#x27;s prediction of the fake image.</span></span><br><span class="line">    <span class="comment">#       3) Calculate the generator&#x27;s loss. Remember the generator wants</span></span><br><span class="line">    <span class="comment">#          the discriminator to think that its fake images are real</span></span><br><span class="line">    <span class="comment">#     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="comment"># step 1</span></span><br><span class="line">    noise = get_noise(num_images, z_dim, device)</span><br><span class="line">    gen_output_fake = gen(noise)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># step 2</span></span><br><span class="line">    disc_output_fake = disc(gen_output_fake)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># step 3</span></span><br><span class="line">    ground_truth_fake = torch.ones_like(disc_output_fake)</span><br><span class="line">    gen_loss = criterion(disc_output_fake, ground_truth_fake)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> gen_loss</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_reasonable</span>(<span class="params">num_images=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="comment"># Don&#x27;t use explicit casts to cuda - use the device argument</span></span><br><span class="line">    <span class="keyword">import</span> inspect, re</span><br><span class="line">    lines = inspect.getsource(get_gen_loss)</span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r&quot;to\(.cuda.\)&quot;</span>, lines)) <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r&quot;\.cuda\(\)&quot;</span>, lines)) <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = torch.zeros_like</span><br><span class="line">    disc = nn.Identity()</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.<span class="built_in">all</span>(torch.<span class="built_in">abs</span>(gen_loss_tensor) &lt; <span class="number">1e-5</span>)</span><br><span class="line">    <span class="comment">#Verify shape. Related to gen_noise parametrization</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">tuple</span>(gen_loss_tensor.shape) == (num_images, z_dim)</span><br><span class="line"></span><br><span class="line">    gen = torch.ones_like</span><br><span class="line">    disc = nn.Identity()</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    real = torch.zeros(num_images, <span class="number">1</span>)</span><br><span class="line">    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.<span class="built_in">all</span>(torch.<span class="built_in">abs</span>(gen_loss_tensor - <span class="number">1</span>) &lt; <span class="number">1e-5</span>)</span><br><span class="line">    <span class="comment">#Verify shape. Related to gen_noise parametrization</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">tuple</span>(gen_loss_tensor.shape) == (num_images, z_dim)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_loss</span>(<span class="params">num_images</span>):</span></span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = Generator(z_dim).to(device)</span><br><span class="line">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">    disc = Discriminator().to(device) </span><br><span class="line">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line">    </span><br><span class="line">    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check that the loss is reasonable</span></span><br><span class="line">    <span class="keyword">assert</span> (gen_loss - <span class="number">0.7</span>).<span class="built_in">abs</span>() &lt; <span class="number">0.1</span></span><br><span class="line">    gen_loss.backward()</span><br><span class="line">    old_weight = gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.clone()</span><br><span class="line">    gen_opt.step()</span><br><span class="line">    new_weight = gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight</span><br><span class="line">    <span class="keyword">assert</span> <span class="keyword">not</span> torch.<span class="built_in">all</span>(torch.eq(old_weight, new_weight))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test_gen_reasonable(<span class="number">10</span>)</span><br><span class="line">test_gen_loss(<span class="number">18</span>)</span><br><span class="line">print(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!</code></pre>
<p>Finally, you can put everything together! For each epoch, you will process the entire dataset in batches. For every batch, you will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. </p>
<p>It’s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It’s important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.</p>
<p>After you’ve submitted a working version with the original architecture, feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. For example, consider changing the size of the hidden dimension, or making the networks shallower or deeper by changing the number of layers.</p>
<!-- In addition, be warned that this runs very slowly on a CPU. One way to run this more quickly is to use Google Colab: 

1.   Download the .ipynb
2.   Upload it to Google Drive and open it with Google Colab
3.   Make the runtime type GPU (under “Runtime” -> “Change runtime type” -> Select “GPU” from the dropdown)
4.   Replace `device = "cpu"` with `device = "cuda"`
5.   Make sure your `get_noise` function uses the right device -->

<p>But remember, don’t expect anything spectacular: this is only the first lesson. The results will get better with later lessons as you learn methods to help keep your generator and discriminator at similar levels.</p>
<p>You should roughly expect to see this progression. On a GPU, this should take about 15 seconds per 500 steps, on average, while on CPU it will take roughly 1.5 minutes:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: </span></span><br><span class="line"></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">mean_generator_loss = <span class="number">0</span></span><br><span class="line">mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">test_generator = <span class="literal">True</span> <span class="comment"># Whether the generator should be tested</span></span><br><span class="line">gen_loss = <span class="literal">False</span></span><br><span class="line">error = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = <span class="built_in">len</span>(real)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Flatten the batch of real images from the dataset</span></span><br><span class="line">        real = real.view(cur_batch_size, -<span class="number">1</span>).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update discriminator ###</span></span><br><span class="line">        <span class="comment"># Zero out the gradients before backpropagation</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate discriminator loss</span></span><br><span class="line">        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update gradients</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update optimizer</span></span><br><span class="line">        disc_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For testing purposes, to keep track of the generator weights</span></span><br><span class="line">        <span class="keyword">if</span> test_generator:</span><br><span class="line">            old_generator_weights = gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.detach().clone()</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update generator ###</span></span><br><span class="line">        <span class="comment">#     Hint: This code will look a lot like the discriminator updates!</span></span><br><span class="line">        <span class="comment">#     These are the steps you will need to complete:</span></span><br><span class="line">        <span class="comment">#       1) Zero out the gradients.</span></span><br><span class="line">        <span class="comment">#       2) Calculate the generator loss, assigning it to gen_loss.</span></span><br><span class="line">        <span class="comment">#       3) Backprop through the generator: update the gradients and optimizer.</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        <span class="comment"># step 1</span></span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># step 2</span></span><br><span class="line">        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># step 3</span></span><br><span class="line">        gen_loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">        gen_opt.step()</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># For testing purposes, to check that your code changes the generator weights</span></span><br><span class="line">        <span class="keyword">if</span> test_generator:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">assert</span> lr &gt; <span class="number">0.0000002</span> <span class="keyword">or</span> (gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.grad.<span class="built_in">abs</span>().<span class="built_in">max</span>() &lt; <span class="number">0.0005</span> <span class="keyword">and</span> epoch == <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">assert</span> torch.<span class="built_in">any</span>(gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.detach().clone() != old_generator_weights)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                error = <span class="literal">True</span></span><br><span class="line">                print(<span class="string">&quot;Runtime tests have failed&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">        mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">        mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Visualization code ###</span></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>&quot;</span>)</span><br><span class="line">            fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">            fake = gen(fake_noise)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            mean_generator_loss = <span class="number">0</span></span><br><span class="line">            mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">        cur_step += <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))

HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))


Epoch 1, step 500: Generator loss: 1.537445880770684, discriminator loss: 0.4010176688432697</code></pre>
<p><img src="output_31_4.png" alt="png"></p>
<p><img src="output_31_5.png" alt="png"></p>
<p> <strong>Notes that there are only two epoch’s results.</strong></p>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))


Epoch 199, step 93500: Generator loss: 1.095844237446786, discriminator loss: 0.5419520480632782</code></pre>
<p><img src="output_31_958.png" alt="png"></p>
<p><img src="output_31_959.png" alt="png"></p>
]]></content>
      <categories>
        <category>Class notes</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Online course</tag>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title>A Survey for Vision Transformers (By Mar. 2021)</title>
    <url>/2021/04/25/Survey-vision-transformer/</url>
    <content><![CDATA[<p><strong>ABSTRACT:</strong> Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback.</p>
<a id="more"></a>

<p><strong>KEYWORDS:</strong> <em>Self-attention, transformer, convolution neural networks, deep neural networks.</em></p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.  Introduction"></a>1.  Introduction</h1><p>Convolutional neural networks (CNN) has dominated computer vision field since the impressive success of a ImageNet Large Scale Visual Recognition Challenge in 2014 (Russakovsky et al., 2015). Various CNN architectures (He et al., 2015; Szegedy et al., 2016; G. Huang et al., 2016; Hu et al., 2017) springs up like mushrooms since that time. These carefully designed CNN architectures have created significant progress in various visual fields, like image classification (Howard et al., 2017; Szegedy et al., 2014; X. Wang et al., 2018), object detection (Redmon and Farhadi, 2018; Ren et al., 2016), semantic segmentation (Z. Huang et al., 2019; P. Wang et al., 2018; S. Zheng et al., 2020), image generation(X. Chen et al., 2016; Goodfellow, 2016), etc. Most of CNN architectures (He et al., 2015; Simonyan and Zisserman, 2014) heavily benefit from different convolution operations like 3×3 convolutions. These convolution operations can learn local information from visual inputs. Deep stacked convolutions then help to gather high level information (Goodfellow et al., 2016), which help to do different image understanding tasks. In about less than a decade, CNN has become the main design paradigm in many modern deep learning systems.</p>
<p>Despite the great success of CNN, there are some limitations. First, while convolution operations can process a local neighbor in space and time, convolution is hard to capture long-distance dependencies. These dependencies are vital to semantic concepts. For example, a bird break in different angles have to be treated with different convolution operations due to the changes of break position. Generally, we can use different convolution operations, increase kernel sizes, increase model depth, or dilated convolutions to solve this problem. But the computation cost can be high and require careful design. Second, convolution treats all information as equal, but in fact not all pixels should be regarded as equal. For example, when we classify dogs and cats, we pay more attention on their ears and noses and ignore some useless background information. That is to say, image classification should give priority to important foreground objects instead of background. But convolution operations process all local neighborhood information with equal weights and do not have a concept of importance. This also causes inefficiency in computation. </p>
<p>At the same time, transformer shows its power in natural language processing (NLP) tasks and almost become the standard architecture in NLP tasks. One of important reason is that transformer is good at modeling long distance information. In 2017, Vaswani et at. (Vaswani et al., 2017) first replaced convolutions and recurrence with transformer which only based on attention modules. The architecture allows to train model in a more parallelizable way and significantly decrease the training time. In 2019, Devlin et al. (Devlin et al., 2019) proposed a new language representation model called BERT, which aims to train a deep learning model from unlabeled text by jointly conditioning on all context in all layers. One of the most impressive things is BERT model can be easily transferred to other tasks by just fine tuning one additional output layer. In 2020, Brown et al. (Brown et al., 2020) proposed an autoregressive language model GPT-3 which achieves impressive performance on various NLP tasks, like machine translation and question-answering system. </p>
<p>Inspired by the significant improvement of transformer in natural language processing (NLP) tasks, a growing number of researchers are trying to take advantage of transformer in computer vision (Carion et al., 2020; Dosovitskiy et al., 2020; W. Wang et al., 2021; Srinivas et al., 2021). Chen et al. (M. Chen et al., 2020) firstly tried to explore the application of transformer in images. They trained a sequence transformer model to predict pixels and found that the model can learn strong image representations. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed a fully-transformer model called Vision Transformer (ViT) that can be directly used in image classification. Vit treats each image as a set of fixed-size patches (14×14 or 16×16) that will be feed into a transformer encoder to make classification. Though ViT achieves excellent results compared to stat-of-the-art CNN models, it heavily relies on a large-size dataset like JFT-300M (C. Sun et al., 2017) (300 million images) and have relative worse performance on a medium size dataset like ImageNet (Deng et al., 2009).  To tackle this problem and make models train effectively on a medium size dataset, Touvron et al. (Touvron et al., 2021) also proposed an efficient convolution-free transformer model called DeiT. DeiT can be trained on a single 8-GPU node in about three days by leveraging a new distillation token and a specific teacher-student strategy. In the early of 2021, various explorations in vision transformer has been proposed, including Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), Transformer in Transformer (TiT) (Han et al., 2021), and Tokens-to-token ViT (Yuan et al., 2021). </p>
<p>Even though the application of transformer has dominated a variety of NLP tasks since 2017, the application of transformer in CV is just rapidly developed especially since the late 2020. There is still no standard architecture in vision transformer and different models have their own advantages and disadvantages. Almost in every month, there are new proposed architectures about vision transformer. Thus, this paper tries to do a comprehensive survey about the recent vision transformer and compare different architectures in different applications. Specifically, this paper tries to answer four questions in vision transformer: 1) what is transformer? 2) why we need transformer in CV? 3) how transformer is applied in CV? 4) what are existing difficulties and potential development? </p>
<p>The structure of the paper is as follows. In section 2, foundations and preliminary knowledge about transformer will be introduced. Some close related ideas like conventional CNN architectures, channel attention, self-attention, multi-head self-attention, and non-local network will be discussed. This section tries to discuss the evolution of the design of vision models from convolution to transformer. The first aforementioned question will be discussed in this section. In section 3, the core topic transformer will be discussed in detailed. The structure of transformer and different applications of transformer will be introduced, including image classification, object detection, segmentation, etc. This section will show the power of transformer in vision fields, which will also clarify the second and third question in depth. In section 4, vision transformer will be classified into three categories, including transformer with global attention, transformer plus CNN, and transformer with efficient attention. Section 3 and section 4 will help to answer the third aforementioned question. In section 5, existing problems and difficulties in vision transformer will be discussed. Some potential and valuable future work will also be discussed in this section. This section will answer the fourth aforementioned question. In the last section, I will conclude the development of vision transformer and summary the paper.</p>
<h1 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2.  Related work"></a>2.  Related work</h1><p>Visio transformer closely relates to convolution operation and self-attention. Thus, in this section I will firstly go through the milestone vision architectures since 2012 to explain how researchers designed models and improved performance. Then, I will give an introduction about self-attention, which is a core module in transformer. I will also introduce non-local network and two Fourier-based networks to explain there are other ways to capture long-range dependencies instead of self-attention.</p>
<h2 id="2-1-CNN"><a href="#2-1-CNN" class="headerlink" title="2.1 CNN"></a>2.1 CNN</h2><p>There are a lot of CNN model taking advantage from a deeper neural network. Going deeper for a neural network is always the first choice for complex tasks. In 1989, LeNet (LeCun et al., 1989) , one of the earliest CNN, was proposed by LeCun et al. It only has 7 layers, including 3 convolutional layers, 2 pooling layers, and 1 fully-connected layer. It defines the fundamental modules for CNN. Trained with backpropagation algorithms, it can be successfully identified handwritten zip code numbers. </p>
<p>In 2012, AlexNet (Krizhevsky et al., 2012) increased the model depth to 8 layers. It achieved a great improvement (a top-5 error of 15.3%, more than 10.8 percentage points than second one) in ImageNet (Deng et al., 2009). AlexNet shows the power of increase model depth and makes it feasible when use GPU during training. After AlexNet, there are countless number of researchers trying to use Deep CNN to improve performance in their fields. This also greatly improve the development of deep learning in all fields. </p>
<p>In 2015, VGG (Simonyan and Zisserman, 2014) achieved the state-of-the-art performance on ILSVRC classiﬁcation and localisation tasks. It heavily uses 3×3 convolutions and increases the depth by adding more convolutional layers. </p>
<p>Considering the significance of the depth, Residual Network (ResNet) (He et al., 2015) tried to explore how to increase the depth and avoid vanishing/exploding gradients (Bengio et al., 1994; Glorot and Bengio, 2010). This is because He et al. found that directly stacked more layers cannot have a better performance. They proposed a residual block and let these layers fit a residual mapping. They increased the depth from 18 layers to 152 layers and increase performance, which is the first time to surpass 100-layer barrier. ResNet has also been a popular backbone in various vision tasks since then. </p>
<p>In 2016, Densely Connected Convolutional Networks (DenseNet) not only creates short paths from early layers to later layers, it also connects all layers directly with each other.  So, it uses direct connections between any two layers with the same feature-map size to implement this idea. This method also helps to solve the gradient vanishing problem. The maximum depth of DenseNet reached 264. The architecture of DenseNet is shown in Figure 1.</p>
<p><img src="clip_image001.png" alt="img"></p>
<p>Figure 1. DenseNet (G. Huang et al., 2016)</p>
<p>It is needed to note that convolution operations have dominated various visual recognition tasks with a deep neural network since 2012. There are several reasons behind its popular. First, convolution layers can capture favorable visual context using filters. These filters shared weights over the entire image space, so convolution operations have translation equivariance. Second, convolution operation can take advantage of the rapid development of computational resource like GPU because it is inherent parallelable. And GPU allows to train a very deep CNN with fewer time, which helps to train on large-scale image dataset, like ImageNet. Third, multiple kernel paths help to achieve a competitive performance. GoogleNet (Szegedy et al., 2014) carefully designs a Inception module with 1×1 convolution, 3×3 convolution, and 5×5 convolution to increasing the depth and width of the network while keeping the computational budget constant. Fourth, CNN can use hierarchical structure to learn high-level information. This is because spatial and channel-wise information can be fused in a local receptive filed in convolutional layers. The fused information can then be passed to non-linear activation functions or/and down-sampling layers to learn a better representation that is vital to capture global theoretical receptive ﬁelds. </p>
<h2 id="2-2-Attention-in-CNN"><a href="#2-2-Attention-in-CNN" class="headerlink" title="2.2 Attention in CNN"></a>2.2 Attention in CNN</h2><p>Considering the importance of channel relationship, SENet (Hu et al., 2017) uses a Squeeze-and-Excitation (SE) block to explicitly model the interdependencies between different channels. The architecture can learn different weights for different channels in SE block. It is an effective and lightweight gating mechanism to self-recalibrate the feature map. There are three important operations. First, in the squeeze operation, SE block will shrink features through spatial dimensions. Second, in the excitation operation, it will learn weights to explicitly model channel association. Third, it reweights the feature maps by the learned weights. The SE block is shown in Figure 2. </p>
<p><img src="clip_image002.png" alt="img"></p>
<p>Figure 2. A Squeeze-and-Excitation block. (Hu et al., 2017)</p>
<p>Inspired the SENet’s channel recalibration mechanism and multi-branch convolutional networks (He et al., 2015; Srivastava et al., 2015), SKNet uses a non-linear approach to aggregate multiple scale feature and constructs a dynamic selection mechanism in CNN instead of fixed receptive fields in standard CNN. It has three basic operators – Split, Fuse, and Select, which are shown in Figure 3. Split operator means that they use two filters with different filter sizes. Fuse operator aims to enable neurons to adaptively adjust receptive fields based on the stimulus content. Select operator is a soft attention across channels, which is similar to SENet. This architecture improves the efﬁciency and effectiveness of object recognition. </p>
<p><img src="clip_image003.png" alt="img"></p>
<p>Figure 3. Selective Kernel Convolution. (X. Li et al., 2019)</p>
<p>Some researchers tried to not only use channel attention, but also take into account spatial attention as well. For example, Bottleneck Attention Module (BAM) (Park et al., 2018) and Convolutional Block Attention Module (CBAM) (Woo et al., 2018) were proposed to infer an attention map along two separate dimensions, channel and spatial. But BAM’s attention computes in parallel, while CBAM computes attention sequentially. </p>
<p>Other researchers only used spatial attention and did not consider channel attention. For example, Non-local Network (NLNet) (X. Wang et al., 2018) tries to capture long-range dependencies by avoiding messages delivered back and forth between distant positions. They proposed non-local operations to have spatial attention. This operation can use a weighted sum of the features to compute the response at a position. They applied NLNet in video classification and achieved competitive performance. The non-local block is shown in Figure 4. </p>
<p><img src="clip_image004.png" alt="img"></p>
<p>Figure 4. Non-local block. (X. Wang et al., 2018)</p>
<p>Although NLNet presents a way to capture long-range information, global contexts are almost the same for different query positions (Cao et al., 2019). So, construct a global context network (GCNet) is proposed to solve this problem by unify the basic idea of SNNet and NLNet, which means it takes into account of spatial and channel information at the same time. </p>
<p>To summary the application of attention in CNN, the comparison of NLNet, SENet, SKNet, and GCNet, etc. are shown in Table 1.</p>
<p>Table 1. Model comparison for the application of attention in CNN.</p>
<table>
<thead>
<tr>
<th><strong>Model name</strong></th>
<th><strong>Characteristics</strong></th>
</tr>
</thead>
<tbody><tr>
<td>SENet</td>
<td>Spatial aggregation, channel attention</td>
</tr>
<tr>
<td>SKNet</td>
<td>Adaptive receptive filed, channel attention</td>
</tr>
<tr>
<td>BAM</td>
<td>Channel attention and spatial attention in parallel</td>
</tr>
<tr>
<td>CBAM</td>
<td>Channel attention and spatial attention sequentially</td>
</tr>
<tr>
<td>NLNet</td>
<td>Spatial weighted sum per pixel</td>
</tr>
<tr>
<td>GCNet</td>
<td>Spatial weighted sum, channel attention</td>
</tr>
</tbody></table>
<h2 id="2-3-Self-attention"><a href="#2-3-Self-attention" class="headerlink" title="2.3 Self-attention"></a>2.3 Self-attention</h2><p>Self-attention (Vaswani et al., 2017) can transform an input to a better representation by allowing inputs to interact with each other. It will compute three hidden representations called query, key, value firstly by three learnable matrixes. After that, it will compute attention weights by the dot products of query and key. The weights will be divided by the sqrt root of the key’s dimension and obtain the weighted value as output. So, the output of self-attention operation is aggregates of the interactions between all inputs. Figure 5 shows how self-attention works. </p>
<p><img src="clip_image005.png" alt="img"></p>
<p>Figure 5. Self-attention operation.</p>
<p>The calculation of self-attention can be formulated as a single function: </p>
<table>
<thead>
<tr>
<th><img src="clip_image007.png" alt="img"></th>
<th>(1)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>The first important thing to understand the mechanism of self-attention is remembering the dimension of the input and output of self-attention is the same. That is to say, our goal is to have a new and better representation for our inputs, and the output needs to be the same dimension. So, what is a good representation for the input? When we are doing machine translation, we understand that even for a same word it can have different or even opposite meaning. What make the word has different meaning? The answer is context. So, we want to consider the whole context and capture the different relationships between different words. Self-attention is designed to do a similar thing. The dot product of <img src="clip_image009.png" alt="img"> and <img src="clip_image011.png" alt="img"> aims to calculate the similarity/distance between two different vectors. The score of <img src="clip_image013.png" alt="img"> can be used to determine how much weights need to be put in a certain input. But before we calculate a weighted value as the output, we need to do some normalization, which is shown in Equation 1. </p>
<p>It is also need to note that the difference of the encoder-decoder attention layer (Luong et al., 2015) between self-attention layer is where the key matrix <img src="clip_image011.png" alt="img">,value matrix <img src="clip_image016.png" alt="img"> and query matrix <img src="clip_image009.png" alt="img"> come from. For the encoder-decoder attention layer, <img src="clip_image019.png" alt="img">and <img src="clip_image016.png" alt="img"> come from the encoder module and <img src="clip_image009.png" alt="img"> comes from the previous layer. Other operations are the same. </p>
<p>The limitation for self-attention layer is that it cannot give different weights for different parts of the input. One way to solve that is to use multi-head attention instead of one single-head attention. The structure of multi-head attention is shown in Figure 6. It helps to boost the performance of the vanilla self-attention layer by giving attention layers different representation subspace. Different query, key, and value matrix are used for different heads So, they can learn different representation in training. For example, several heads are good at learning local information. Others are good at learning global information. Specifically, the operation of multi-head attention can be formulated as below:</p>
<table>
<thead>
<tr>
<th><img src="clip_image023.png" alt="img"></th>
<th>(2)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>where <img src="clip_image025.png" alt="img">, <img src="clip_image027.png" alt="img">, <img src="clip_image029.png" alt="img">, and <img src="clip_image031.png" alt="img">are the concatenation of <img src="clip_image033.png" alt="img">. </p>
<p><img src="clip_image034.png" alt="img"></p>
<p>Figure 6. Multi-head attention. (Vaswani et al., 2017)</p>
<h2 id="2-4-Fourier-based-network"><a href="#2-4-Fourier-based-network" class="headerlink" title="2.4 Fourier-based network"></a>2.4 Fourier-based network</h2><p>Convolution and attention are not the only choice to improve model performance. Fourier analysis tells us that any signal can be represented as a linear combination of sin and cos functions with different coefficients regardless of signal’s dimension. Some researcher recently tried to integrate Short Time Fourier Transformation (STFT) in to vision architectures. For example, Fast Fourier Convolution (FFC) (Lu Chi, 2020) adds a global branch that manipulates image-level spectrum and a semi-global branch to process stacked image patches. This design help to capture non-local receptive fields and cross-scale fusion. </p>
<p>Another network is call Fourier Neural Operator (FNO) (Z. Li et al., 2020). Although FNO is designed to solve partial differential equations in engineering and science, it is a pure neural network and does not have close relationship with some numerical methods, like Finite Element Analysis or Finite Difference Method. The basic idea is that they build a Fourier Layer/ Block and stacked many Fourier Layers to get a better performance. Specifically, it has two branches. The first branch uses STFT to change hidden representations in spatial domain to frequency domain. Then high frequency modes are removed, which means only low frequency modes remains. The coefficients for each low frequency are learned by the Neural Network. And the frequency information is inversed to the original spatial domain by inverse STFT. The second is 1×1 convolution operation which is designed to let the model learn the residual and help to alleviate the gradient vanishing problem. Figure 7 show the structure of Fourier Layer.</p>
<p><img src="clip_image035.png" alt="img"></p>
<p>Figure 7. Fourier Layer. (Z. Li et al., 2020)</p>
<p>There are several reasons for researchers to use Fourier Analysis in Neural Network. First, the computation is very efficient in a quasi-linear level due to the use of STFT. Second, it helps to capture coarse and global in a more efficient way by using learnable coefficients for different frequencies. Thus, in some applications, we may not need to use stacked convolutions with different size. We can use Fourier Layer to learn a good representation in frequency domain.</p>
<h1 id="3-Topic-and-its-applications"><a href="#3-Topic-and-its-applications" class="headerlink" title="3.  Topic and its applications"></a>3.  Topic and its applications</h1><p>In this section, I will firstly introduce the details of transformer and then provide a comprehensive review of its applications in different visual tasks, including image classification, object detection, segmentation, etc. </p>
<h2 id="3-1-Transformer"><a href="#3-1-Transformer" class="headerlink" title="3.1 Transformer"></a>3.1 Transformer</h2><p>In the previous section, I have introduced the core module in transformer, single-head and multi-head attention layer. I will go through other modules and introduce the whole structure of transformer, which is shown in Figure 8. </p>
<p><img src="clip_image036.png" alt="img"></p>
<p>Figure 8. The architecture for transformer. (Vaswani et al., 2017)</p>
<p>The first thing is the positional encoding. Self-attention operation is irrelevant to the position of each input. So, it cannot capture the positional information of inputs. To solve this potential problem, they proposed a positional encoding that is added to the input embedding. </p>
<p>The second thing is the Feed-forward neural network (FFNN). A FFNN is used after a self-attention layer in each encoder and decoder. This FFNN has two linear transformation layers and a ReLU activation function (Nair and Hinton, 2010). It can be expressed as:</p>
<table>
<thead>
<tr>
<th><img src="clip_image038.png" alt="img">,</th>
<th>(3)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>where <img src="clip_image040.png" alt="img">and <img src="clip_image042.png" alt="img"> are weights for the two linear transformation layers, <img src="clip_image044.png" alt="img"> is ReLU activation function. </p>
<p>The third thing is Add &amp; Norm module. To strength the information flow in a deep neural network, a residual connection is added in each module, including FFNN and self-attention. This is a similar design in ResNet (He et al., 2015). To boost the training, a layer-normalization (Cheng et al., 2016) is followed by the residual connection. </p>
<p>The last thing is the output layer. After passing through <img src="clip_image046.png" alt="img">decoders, the output will be passed to a linear layer and a softmax layer, which help to transform logits vector into word probabilities.</p>
<p>To summarize, Transformer, an encoder-decoder architecture, is designed to handle sequential data.</p>
<h2 id="3-2-Applications"><a href="#3-2-Applications" class="headerlink" title="3.2 Applications"></a>3.2 Applications</h2><p>Since 2017, Transformer has achieved great success in various NLP tasks. For example, after pretraining, BERT (Devlin et al., 2019) can be easily used in a wide range of downstream tasks by fine-tuning one newly added output layer. Generative Pre-Trained Transformer models (GPT (Alec Radford et al., 2018), GPT-2 (A. Radford et al., 2019), and GPT-3 (Brown et al., 2020)) are another popular pretrained models in NLP. GPT-3 is designed to directly do multiple tasks without fine-tuning, but it has 175 billion parameters and requires 45 TB compressed plaintext data to train.</p>
<p>Inspired by the success of transformer in NLP, there are growing number of vision transformers being proposed (Beal et al., 2020; Dosovitskiy et al., 2020; Yang et al., 2020; Zhang et al., 2020; S. Zheng et al., 2020). Researchers also applied vision transformer in different fields. </p>
<h2 id="3-2-1-Image-classification"><a href="#3-2-1-Image-classification" class="headerlink" title="3.2.1  Image classification"></a>3.2.1  Image classification</h2><p>ViT (Dosovitskiy et al., 2020) is a pure transformer applied directly to image patches. The architecture is shown in Figure 9. They reshape an image <img src="clip_image048.png" alt="img"> as a sequence of flattened 2D patches <img src="clip_image048.png" alt="img">, where H = 14 or 16. Then, a 1D positional embedding added with patch embedding are fed into transformer encoder.  The feature obtained from the first position is regarded as the global image representation and is used to do classification. ViT only uses the encoder of transformer and only has minimal changes compared to a standard transformer in NLP. </p>
<p><img src="clip_image050.png" alt="img"></p>
<p>Figure 9. Vision Transformer (ViT). (Dosovitskiy et al., 2020)</p>
<p>ViT achieved excellent results when pre-trained on a large dataset JFT-300M (300 million images), but it does not generalize well in mid-sized datasets like ImageNet (1.2 million images). One of the reasons is that transformer lack inductive biases inherent to CNN, such as translation equivariance and locality. So, vision transformer needs sufficient data to have a good generalization, which is similar in NLP transformer. Moreover, they compared ViT with a hybrid model that feeds intermediate feature maps into ViT by leveraging CNN model. They found that hybrid model only has better performance than ViT at small computational budgets and the difference vanishes given sufficient data. This shows that CNN may not be a necessary part for a large model. Furthermore, they analyzed the relationship between network depth and mean attention distance, which is shown in Figure 10. They found that some heads can attend at global information even at the lowest layer. </p>
<p><img src="clip_image051.png" alt="img"></p>
<p>Figure 10. Attention distance in ViT. (Dosovitskiy et al., 2020)</p>
<p>ViT needs to be trained on millions of images, which severely hinders its application in different fields. In the early of 2021, DeiT (Touvron et al., 2021) uses a teacher-student strategy and add a distillation token. It is a convolution-free architecture and similar to ViT, but it uses CNN as a teacher and transformer as a student. It is much more efficient and only needs to be trained about 3 days on a single 8-GPU node. To facilitate the training, it uses data augmentation and regularization strategies. These helps to greatly reduces the requirement of data and make it feasible to train on ImageNet instead of JFT-300M. They achieved better performance than ViT and EfficientNet. </p>
<p>Another vision transformer called Tokens-to-Token ViT (T2T) (Yuan et al., 2021) was proposed in 2021. The architecture of T2T is shown in Figure 11 (a). T2T wants to train model in a more efficient way and avoid the reliance on a large-size dataset. They pointed out two problems in ViT. First, splitting each image into a sequence of tokens fails to capture key local structures like edges or lines. Second, the backbone of ViT is not suitable for vision tasks due to the redundancy in the attention module. From Figure 11 (b) , we can find that T2T can capture local structure features (green box), but ViT capture invalid feature maps (red box). Thus, they proposed a progressive tokenization module to replace the tokenization method in ViT. Their method helps to capture local structure information from neighbor tokens and achieve length reduction of tokens. They also compared different backbone from existing CNN architectures and found that “deep-narrow” architecture design with fewer channels but more layers in ViT is the best one.</p>
<p><img src="clip_image053.png" alt="img"></p>
<p>Figure 11. T2T architecture and feature visualization. (Yuan et al., 2021)</p>
<p>Viewing images as a sequence of patches ignore the structure information between patches. To solve this problem, Transformer in Transformer (TNT), a pure transformer network, handles images in patch-level and pixel level. The architecture of TNT is shown in Figure 12. TNT uses two transformer blocks. The first one is an outer transformer block which is used to process patch embedding. The other one is an inner transformer block which is designed to caoture local features from pixel level. In a similar computational cost, TNT’s performance is 1.5% higher than that of DeiT on ImageNet. They also compared visualization of learned features between DeiT and TNT. Compared to DeiT, TNT can preserve better local information and have more diverse features. </p>
<p><img src="clip_image054.png" alt="img"></p>
<p>Figure 12. TNT block and TNT framework. (Han et al., 2021)</p>
<p>Bottleneck Transformers Network (BoTNet) is also a backbone architecture. It only modifies the final three bottleneck blocks of ResNet with global self-attention, which is shown in Figure 13. It has two main stages. In the first stage, they use convolution operations to learn high-level feature maps. In the second stage, they use global self-attention to aggregate information. Based on such design, it is easy to handle large images (1024×1024) in an efficient way. They pointed out that the designed block can be regarded as transformer block. BoTNet is quite similar to NLNet. The first difference is that BoTNet uses multi-head self-attention but NLNet does not always use it in the implementation. The second difference is that non-local block is used as an additional block, while bottleneck block is used to replace convolution block.</p>
<p><img src="clip_image055.png" alt="img"></p>
<p>Figure 13. A comparison of ResNet bottleneck and bottleneck Transformer. (Srinivas et al., 2021)</p>
<h2 id="3-2-2-Object-detection"><a href="#3-2-2-Object-detection" class="headerlink" title="3.2.2  Object detection"></a>3.2.2  Object detection</h2><p>Compared to image classification, object detection requires models to predict object labels and their corresponding bounding boxes. Detection Transformer (DETR) is the first end-to-end transformer architecture used in object detection. It removes the many traditional hand-designed components like region proposal network (RPN) and non-maximal suppression (NMS). It also can predict all objects at once, which is different to RNN that predicts sequence elements one by one. DETR uses a set-based global loss which performs bipartite matching between predicted and ground-truth objects to train the model. It does not require any customized layers, which makes it easily reproduced in any framework. It achieves comparable results to Faster R-CNN (Ren et al., 2016). But it has two main limitations. First, it takes a long time to converge. This is because the attention module has uniform attention at the start of training and needs more epochs to update to optimal attention weights. Second, it easily fails to detect small objects due to limited feature spatial resolution. </p>
<p><img src="clip_image056.png" alt="img"></p>
<p>Figure 14. The architecture for DETR. (Carion et al., 2020)</p>
<p>To solve the drawbacks of DETR, Deformable DETR uses more efficient attention module that only attend on some key sampling points around the reference point. In this way, it greatly reduces the computational cost. Also, the deformable attention module cam fuse multi-scale features. It achieves better performance with 10 times less training epochs compared to the original DETR model.</p>
<p>Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), a recent pure transformer backbone, also aims to capture multi-scale features maps for dense predictions. Compared to ViT, PVT is trained on dense partition of images (the patch size is 4) to get high resolution outputs. To prevent the high computational costs, PVT uses a progressive shrinking pyramid and adopts a spatial -reduction attention layer. They applied PVT in image classification, object detection, and sematic segmentation and achieved a competitive performance. PVT also can be easily combined with DETR. The combination model is called PVT+DETR, which is an entirely convolution-free network. It has better performance than DETR plus ResNet50. </p>
<p>Adaptive Clustering Transformer (ACT) (M. Zheng et al., 2020) is also designed to solve DETR’s high computational cost in training and testing. ACT cluster the query features using Locality Sensitive Hashing (LSH) and use the prototype-key interaction to approximate the query-key relationship. ACT greatly reduce the computation cost while just reduce little accuracy. </p>
<p>Sun et al. (Z. Sun et al., 2020) pointed out that the slow convergence of the original DETR model comes from the Hungarian loss and the Transformer cross-attention. They proposed two possible solutions: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). These two models achieve better performance than the original DETR model.</p>
<h2 id="3-2-3-Segmentation"><a href="#3-2-3-Segmentation" class="headerlink" title="3.2.3  Segmentation"></a>3.2.3  Segmentation</h2><p>Segmentation requires model to have pixel-level understanding, which is different from image classification and object detection. Video Instance Segmentation with Transformers (VisTR) (Y. Wang et al., 2020)  is an end-to-end parallel sequence prediction network. The pipeline of VisTR is shown in Figure 15. VisTR takes a sequence of images from a video as input and make instance segmentation prediction for each image directly.  VisTR is designed to learn pixel-level and instance-level similarity. It can track instances seamlessly and naturally. A CNN model is used to extract feature maps from a sequence of images. After adding with positional encoding, extracted feature maps will be fed into encoder and decoder. Finally, VisTR will output the final mask sequences by an instance sequence segmentation. </p>
<p><img src="clip_image057.png" alt="img"></p>
<p>Figure 15. The architecture for VisTR. (Y. Wang et al., 2020)</p>
<p>Point Transformer (Zhao et al., 2020) is a backbone architecture for 3D point cloud understanding tasks, including semantic scene segmentation, object part segmentation, and object classiﬁcation. They designed a Point Transformer layer which is invariant to permutation and cardinality. It outperforms the state-of-the-art model in S3DIS dataset for large -scale semantic scene segmentation. Axial-DeepLab (H. Wang, Zhu, Green, et al., 2020) is proposed to convert 2D self-attention as two 1D axial-attention layers which are applied to height and width sequentially. This design greatly reduces the computation and allow to capture non-local interactions easily. There is also a model called Attention-Based Transformers that is designed to instance segmentation of cells in microstructures. It uses long skip connections between the CNN encoder and CNN decoder. It achieves fast and accurate cell instance segmentation. </p>
<h1 id="4-Classification-and-comparison-of-various-methods"><a href="#4-Classification-and-comparison-of-various-methods" class="headerlink" title="4.  Classification and comparison of various methods"></a>4.  Classification and comparison of various methods</h1><p>Transformer structure uses self-attention module to long-range information, while convolutions use hierarchical structure to learn high-level features. Generally, researchers try to explore architectures in these two different ways and take advantage from each other. Another important category is efficient transformer because transformer normally requires more data than standard CNN. </p>
<table>
<thead>
<tr>
<th><strong>Task</strong></th>
<th><strong>Category</strong></th>
<th><strong>Method</strong></th>
<th><strong>Characteristics</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Image classification</td>
<td>Transformer + global attention</td>
<td>ViT   (Dosovitskiy  et al., 2020)</td>
<td>Only transformer encoder, patching embedding, and  positional encoding;  High computational cost, heavily rely on large-size dataset</td>
</tr>
<tr>
<td>Image classification</td>
<td>Transformer + CNN</td>
<td>DeiT   (Touvron  et al., 2021)</td>
<td>Teacher-student strategy, distillation token, data  augmentation and regularization strategies;  Rely on the CNN model to be a teacher</td>
</tr>
<tr>
<td>Image classification</td>
<td>Transformer + global attention</td>
<td>T2T-ViT   (Yuan et  al., 2021)</td>
<td>Progressively structurize images to tokens, deep-narrow  structure, train from scratch on ImageNet</td>
</tr>
<tr>
<td>Image classification</td>
<td>Transformer + global attention</td>
<td>TNT   (Han et  al., 2021)</td>
<td>Operate on patch-level and pixel-level, more diverse  feature extraction</td>
</tr>
<tr>
<td>Object detection</td>
<td>Transformer + CNN</td>
<td>DETR   (Carion  et al., 2020)</td>
<td>End-to-end learning, set-based prediction, remove  hand-design modules;  Struggle on small objects, slow to converge</td>
</tr>
<tr>
<td>Object detection</td>
<td>Transformer + CNN + efficient attention</td>
<td>Deformable DETR   (Zhu et  al., 2020)</td>
<td>Better performance on small objects, faster convergence  than DETR;  Two stage detector architecture, use data augmentation in  test time</td>
</tr>
<tr>
<td>Segmentation</td>
<td>Transformer + CNN</td>
<td>MaX-DeepLab (H. Wang,  Zhu, Adam, et al., 2020)</td>
<td>ﬁrst end-to-end model for panoptic segmentation, mask  transformer, bipartite matching, dual-path architecture</td>
</tr>
<tr>
<td>Segmentation</td>
<td>Transformer + CNN</td>
<td>VisTR   (Y. Wang  et al., 2020)</td>
<td>instance sequence matching and segmentation, direct  end-to-end parallel sequence prediction</td>
</tr>
<tr>
<td>Image processing</td>
<td>Transformer + CNN</td>
<td>Image processing transformer  (IPT) (H. Chen  et al., 2020)</td>
<td>The multi-head and multi-trail structure, generate a large  amount of corrupted image pairs images by add noising, rain streak, or down-sampling;  can be used in different tasks in one model after ﬁne-tuning</td>
</tr>
<tr>
<td>Image colorization</td>
<td>Transformer + global attention</td>
<td>Colorization Transformer (ColTran) (Kumar et  al., 2020)</td>
<td>Conditional transformer layers, two parallel networks to upsample  low resolution images</td>
</tr>
<tr>
<td>Segmentation</td>
<td>Transformer + efficient attention</td>
<td>Criss-Cross Network (CCNet)</td>
<td>Criss-Cross Attention block, high efficiency and low  computation, The state-of-the-art performance,</td>
</tr>
<tr>
<td>NLP, architecture exploration</td>
<td>Transformer + efficient attention</td>
<td>ConvBERT (Jiang et al., 2020)</td>
<td>Span-based dynamic  convolution, mixed attention block to capture both global and local dependencies</td>
</tr>
</tbody></table>
<h1 id="5-Future-work"><a href="#5-Future-work" class="headerlink" title="5.  Future work"></a>5.  Future work</h1><p>Transformer is still a young field and has different architectures proposed in almost every week. There are several important problems or difficulties that are worth to be noticed. </p>
<p>The first one is to discover the mechanism of transformer. Since Google proposed transformer architecture in 2017, countless papers have been using this idea in different fields, from NLP to CV or to medical researches. But recently, Google also published a paper which points out that “Attention is not all you need: pure attention loses rank doubly exponentially with depth”. They decompose the outputs of self-attention layer into a sum of smaller terms and found that skip connections or multi-layer perceptron are vital to the final performance. We need to understand why and how transformer works. Only when we have a better understanding about that, we can know the direction of improvement and development. </p>
<p>The second one is to build efficient vision transformer. There are several ways to accomplish that. For example, we can take advantages from convolution operations which has inductive biases, including translation invariance, weight sharing, scale invariance. Or we can build effective attention block to capture different scale information in a better way. Or we can discover the efficient to use training strategies (data augmentation or regularization) to avoid the reliance on large-size dataset. This problem is the core research direction for vision transformer for a long time since in practical we generally cannot find such large dataset. </p>
<p>The third one is to reduce model complexity. Small and efficient model can be easily extended in different fields. Distillation maybe a choice to make transformer models more feasible in practice. Some useful techniques such as selective or sparse attention, low-rank matrix in attention, and efficient cluster attention may give some insights to solve this problem.</p>
<p>The fourth one is to explore more backbone architecture for visual inputs instead of directly using transformer structure in NLP tasks. The last one is to discover more models that can do multiple tasks in one model. </p>
<h1 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6.  Conclusion"></a>6.  Conclusion</h1><p>In this survey, I give a comprehensive review about vision transformer. I introduce the development of the depth exploration in CNN and go through the researches in channel and spatial attention. I also introduce the all related modules in transformer structure, including self-attention, MLP, FFNN, and layer normalization. Then the application of vision transformer and their classification and comparison are discussed. I also list some potential and useful future research directions at the end.</p>
<h1 id="7-Course-feedbacks"><a href="#7-Course-feedbacks" class="headerlink" title="7.  Course feedbacks"></a>7.  Course feedbacks</h1><p>This course is a great graduate-level class. There are several reasons for that:</p>
<p>\1.   <strong>Rich course content.</strong> From PCA to Bayesian, all important topics are covered in this 9-week course. It is amazing and really give me a good overview in this field. </p>
<p>\2.   <strong>Excellent lectures.</strong> Even though I toke class by watching video, I can feel that professor try to explain concept in detail rather quickly go through various concepts and equations. For some hard concepts or math-heavy part, professor is always willing to draw some lines or circles in patient, which really help me to understand them.</p>
<p>\3.   <strong>Novel and creative illustration</strong>. Even though I think I have learned PCA from other course or videos, I am surprised to learn new things from professor’s explanation. More importantly, most of other courses only cover PCA and do not introduce LDA. Only after this class, I finally understand that when do I need to choose PCA or LDA. Another example is backpropagation. This is my first time to learn sensitivity in backpropagation, which really help me understand more than other courses. I never heard it, although I have learned some course or read some books before.</p>
<p>\4.   <strong>Excellent reading recommendation</strong>. Professor have kindly given us some reading list in each lecture to help us learn more things. They are really helpful and help me to understand deeply.</p>
<p>\5.   For every lecture, professor always pointed out the “<strong>take away home message</strong>”. Personally, I really love this part. This short message really helps me to focus on the most important part and avoid loss from some relative trivial concepts.</p>
<p>\6.   <strong>The slides are clear and well-organized.</strong> I really love this class’s slides. The have a lot of sentence and organized very well, which help to in the lecture and review.</p>
<h1 id="8-Reference"><a href="#8-Reference" class="headerlink" title="8.  Reference"></a>8.  Reference</h1><p>[1] Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., and Kislyuk, D. (2020). Toward Transformer-Based Object Detection. <em>ArXiv:2012.09958 [Cs]</em>. <a href="http://arxiv.org/abs/2012.09958">http://arxiv.org/abs/2012.09958</a></p>
<p>[2] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. <em>IEEE Transactions on Neural Networks</em>, <em>5</em>(2), 157–166. <a href="https://doi.org/10.1109/72.279181">https://doi.org/10.1109/72.279181</a></p>
<p>[3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). <em>Language Models are Few-Shot Learners</em>. <a href="https://arxiv.org/abs/2005.14165v4">https://arxiv.org/abs/2005.14165v4</a></p>
<p>[4] Cao, Y., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). <em>GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond</em>. 0–0. <a href="https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html">https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html</a></p>
<p>[5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-End Object Detection with Transformers. <em>ArXiv:2005.12872 [Cs]</em>. <a href="http://arxiv.org/abs/2005.12872">http://arxiv.org/abs/2005.12872</a></p>
<p>[6] Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. (2020). Pre-Trained Image Processing Transformer. <em>ArXiv:2012.00364 [Cs]</em>. <a href="http://arxiv.org/abs/2012.00364">http://arxiv.org/abs/2012.00364</a></p>
<p>[7] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative Pretraining From Pixels. <em>International Conference on Machine Learning</em>, 1691–1703. <a href="http://proceedings.mlr.press/v119/chen20s.html">http://proceedings.mlr.press/v119/chen20s.html</a></p>
<p>[8] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. <em>ArXiv:1606.03657 [Cs, Stat]</em>. <a href="http://arxiv.org/abs/1606.03657">http://arxiv.org/abs/1606.03657</a></p>
<p>[9] Cheng, J., Dong, L., and Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. <em>ArXiv:1601.06733 [Cs]</em>. <a href="http://arxiv.org/abs/1601.06733">http://arxiv.org/abs/1601.06733</a></p>
<p>[10]         Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–255.</p>
<p>[11]         Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <em>ArXiv:1810.04805 [Cs]</em>. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a></p>
<p>[12]         Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. <em>ArXiv:2010.11929 [Cs]</em>. <a href="http://arxiv.org/abs/2010.11929">http://arxiv.org/abs/2010.11929</a></p>
<p>[13]         Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 249–256. <a href="http://proceedings.mlr.press/v9/glorot10a.html">http://proceedings.mlr.press/v9/glorot10a.html</a></p>
<p>[14]         Goodfellow, I. (2016). <em>Generative Adversarial Networks (GANs)</em>. 86.</p>
<p>[15]         Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p>
<p>[16]         Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in Transformer. <em>ArXiv:2103.00112 [Cs]</em>. <a href="http://arxiv.org/abs/2103.00112">http://arxiv.org/abs/2103.00112</a></p>
<p>[17]         He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. <em>ArXiv:1512.03385 [Cs]</em>. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a></p>
<p>[18]         Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. <em>ArXiv:1704.04861 [Cs]</em>. <a href="http://arxiv.org/abs/1704.04861">http://arxiv.org/abs/1704.04861</a></p>
<p>[19]         Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2017). Squeeze-and-Excitation Networks. <em>ArXiv:1709.01507 [Cs]</em>. <a href="http://arxiv.org/abs/1709.01507">http://arxiv.org/abs/1709.01507</a></p>
<p>[20]         Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. <em>ArXiv:1608.06993 [Cs]</em>. <a href="http://arxiv.org/abs/1608.06993">http://arxiv.org/abs/1608.06993</a></p>
<p>[21]         Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross attention for semantic segmentation. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 603–612.</p>
<p>[22]         Jiang, Z., Yu, W., Zhou, D., Chen, Y., Feng, J., and Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. <em>ArXiv Preprint ArXiv:2008.02496</em>.</p>
<p>[23]         Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.), <em>Advances in Neural Information Processing Systems 25</em> (pp. 1097–1105). Curran Associates, Inc. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>
<p>[24]         Kumar, M., Weissenborn, D., and Kalchbrenner, N. (2020, September 28). <em>Colorization Transformer</em>. International Conference on Learning Representations. <a href="https://openreview.net/forum?id=5NA1PinlGFu">https://openreview.net/forum?id=5NA1PinlGFu</a></p>
<p>[25]         LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. <em>Neural Computation</em>, <em>1</em>(4), 541–551. <a href="https://doi.org/10.1162/neco.1989.1.4.541">https://doi.org/10.1162/neco.1989.1.4.541</a></p>
<p>[26]         Li, X., Wang, W., Hu, X., and Yang, J. (2019). Selective Kernel Networks. <em>ArXiv:1903.06586 [Cs]</em>. <a href="http://arxiv.org/abs/1903.06586">http://arxiv.org/abs/1903.06586</a></p>
<p>[27]         Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. (2020). <em>Fourier Neural Operator for Parametric Partial Differential Equations</em>. <a href="https://arxiv.org/abs/2010.08895v1">https://arxiv.org/abs/2010.08895v1</a></p>
<p>[28]         Lu Chi. (2020). Fast Fourier Convolution. <em>Neural Information Processing Systems</em>, 767–774. <a href="https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf">https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf</a></p>
<p>[29]         Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. <em>ArXiv:1508.04025 [Cs]</em>. <a href="http://arxiv.org/abs/1508.04025">http://arxiv.org/abs/1508.04025</a></p>
<p>[30]         Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. <em>Proceedings of the 27th International Conference on International Conference on Machine Learning</em>, 807–814.</p>
<p>[31]         Park, J., Woo, S., Lee, J.-Y., and Kweon, I. S. (2018). BAM: Bottleneck Attention Module. <em>ArXiv:1807.06514 [Cs]</em>. <a href="http://arxiv.org/abs/1807.06514">http://arxiv.org/abs/1807.06514</a></p>
<p>[32]         Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. <em>Undefined</em>. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe</p>
<p>[33]         Radford, Alec, Narasimhan, K., Salimans, T., and Sutskever, I. (2018). <em>Improving language understanding by generative pre-training</em>.</p>
<p>[34]         Redmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. <em>ArXiv:1804.02767 [Cs]</em>. <a href="http://arxiv.org/abs/1804.02767">http://arxiv.org/abs/1804.02767</a></p>
<p>[35]         Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. <em>ArXiv:1506.01497 [Cs]</em>. <a href="http://arxiv.org/abs/1506.01497">http://arxiv.org/abs/1506.01497</a></p>
<p>[36]         Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. <em>ArXiv:1409.0575 [Cs]</em>. <a href="http://arxiv.org/abs/1409.0575">http://arxiv.org/abs/1409.0575</a></p>
<p>[37]         Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>ArXiv:1409.1556 [Cs]</em>. <a href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a></p>
<p>[38]         Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. <em>ArXiv:2101.11605 [Cs]</em>. <a href="http://arxiv.org/abs/2101.11605">http://arxiv.org/abs/2101.11605</a></p>
<p>[39]         Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. <em>ArXiv E-Prints</em>, <em>1505</em>, arXiv:1505.00387.</p>
<p>[40]         Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. <em>ArXiv:1707.02968 [Cs]</em>. <a href="http://arxiv.org/abs/1707.02968">http://arxiv.org/abs/1707.02968</a></p>
<p>[41]         Sun, Z., Cao, S., Yang, Y., and Kitani, K. (2020). Rethinking Transformer-based Set Prediction for Object Detection. <em>ArXiv:2011.10881 [Cs]</em>. <a href="http://arxiv.org/abs/2011.10881">http://arxiv.org/abs/2011.10881</a></p>
<p>[42]         Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going Deeper with Convolutions. <em>ArXiv:1409.4842 [Cs]</em>. <a href="http://arxiv.org/abs/1409.4842">http://arxiv.org/abs/1409.4842</a></p>
<p>[43]         Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). <em>Rethinking the Inception Architecture for Computer Vision</em>. 2818–2826. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html">http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html</a></p>
<p>[44]         Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2021). Training data-efficient image transformers &amp; distillation through attention. <em>ArXiv:2012.12877 [Cs]</em>. <a href="http://arxiv.org/abs/2012.12877">http://arxiv.org/abs/2012.12877</a></p>
<p>[45]         Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), <em>Advances in Neural Information Processing Systems 30</em> (pp. 5998–6008). Curran Associates, Inc. <a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a></p>
<p>[46]         Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. (2020). MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. <em>ArXiv:2012.00759 [Cs]</em>. <a href="http://arxiv.org/abs/2012.00759">http://arxiv.org/abs/2012.00759</a></p>
<p>[47]         Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L.-C. (2020). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. <em>European Conference on Computer Vision</em>, 108–126.</p>
<p>[48]         Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., and Cottrell, G. (2018). Understanding Convolution for Semantic Segmentation. <em>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 1451–1460. <a href="https://doi.org/10.1109/WACV.2018.00163">https://doi.org/10.1109/WACV.2018.00163</a></p>
<p>[49]         Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. (2021). Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. <em>ArXiv:2102.12122 [Cs]</em>. <a href="http://arxiv.org/abs/2102.12122">http://arxiv.org/abs/2102.12122</a></p>
<p>[50]         Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local Neural Networks. <em>ArXiv:1711.07971 [Cs]</em>. <a href="http://arxiv.org/abs/1711.07971">http://arxiv.org/abs/1711.07971</a></p>
<p>[51]         Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H. (2020). End-to-End Video Instance Segmentation with Transformers. <em>ArXiv:2011.14503 [Cs]</em>. <a href="http://arxiv.org/abs/2011.14503">http://arxiv.org/abs/2011.14503</a></p>
<p>[52]         Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). <em>CBAM: Convolutional Block Attention Module</em>. 3–19. <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html">https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html</a></p>
<p>[53]         Yang, F., Yang, H., Fu, J., Lu, H., and Guo, B. (2020). Learning Texture Transformer Network for Image Super-Resolution. <em>ArXiv:2006.04139 [Cs]</em>. <a href="http://arxiv.org/abs/2006.04139">http://arxiv.org/abs/2006.04139</a></p>
<p>[54]         Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. <em>ArXiv:2101.11986 [Cs]</em>. <a href="http://arxiv.org/abs/2101.11986">http://arxiv.org/abs/2101.11986</a></p>
<p>[55]         Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., and Sun, Q. (2020). Feature Pyramid Transformer. <em>ArXiv:2007.09451 [Cs]</em>. <a href="http://arxiv.org/abs/2007.09451">http://arxiv.org/abs/2007.09451</a></p>
<p>[56]         Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V. (2020). Point Transformer. <em>ArXiv:2012.09164 [Cs]</em>. <a href="http://arxiv.org/abs/2012.09164">http://arxiv.org/abs/2012.09164</a></p>
<p>[57]         Zheng, M., Gao, P., Wang, X., Li, H., and Dong, H. (2020). End-to-End Object Detection with Adaptive Clustering Transformer. <em>ArXiv:2011.09315 [Cs]</em>. <a href="http://arxiv.org/abs/2011.09315">http://arxiv.org/abs/2011.09315</a></p>
<p>[58]         Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., and Zhang, L. (2020). Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. <em>ArXiv:2012.15840 [Cs]</em>. <a href="http://arxiv.org/abs/2012.15840">http://arxiv.org/abs/2012.15840</a></p>
<p>[59]         Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable DETR: Deformable Transformers for End-to-End Object Detection. <em>ArXiv:2010.04159 [Cs]</em>. <a href="http://arxiv.org/abs/2010.04159">http://arxiv.org/abs/2010.04159</a></p>
]]></content>
      <categories>
        <category>Course project</category>
      </categories>
      <tags>
        <tag>Survey</tag>
        <tag>Vision transformer</tag>
        <tag>Computer vision</tag>
        <tag>Course project</tag>
      </tags>
  </entry>
  <entry>
    <title>Xiaoyu Xie</title>
    <url>/about/index.html</url>
    <content><![CDATA[<!-- <img src="https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg" width = "300" height = "200" alt="" align=center /> -->

<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>I am Xiaoyu Xie, a first-year PhD student at Northwestern University. </p>
<h1 id="Research-Interests"><a href="#Research-Interests" class="headerlink" title="Research Interests:"></a>Research Interests:</h1><ul>
<li>Computer Vision;</li>
<li>Deep Learning in Manufacturing, Mechanics, and Finace;</li>
<li>Bayesian Deep Learning;</li>
</ul>
<h1 id="Machine-Learning-related-courses-that-I-have-studied"><a href="#Machine-Learning-related-courses-that-I-have-studied" class="headerlink" title="Machine Learning related courses that I have studied"></a>Machine Learning related courses that I have studied</h1><p>Northwestern University:</p>
<ul>
<li><a href="https://www.mccormick.northwestern.edu/electrical-computer/academics/courses/descriptions/375-475.html">Machine Learning: Foundations, Applications, and Algorithms</a></li>
<li><a href="https://www.mccormick.northwestern.edu/electrical-computer/academics/courses/descriptions/433.html">Statistical Pattern Recognition</a></li>
<li><a href="https://www.mccormick.northwestern.edu/mechanical/academics/courses/descriptions/395-mechanistic-data-science-for-engineering.html">Mechanistic Data Science for Engineering</a></li>
<li><a href="https://www.mccormick.northwestern.edu/mechanical/academics/courses/descriptions/441-engineering-optimization-for-product-design-and-manufacturing.html">Engineering Optimization for Product Design and Manufacturing</a></li>
<li><a href="https://www.mccormick.northwestern.edu/mechanical/academics/courses/descriptions/470-high-performance-computing-for-multiphysics-applications.html">High Performance Computing for Multiphysics Applications</a></li>
</ul>
<p>Coursera:</p>
<ul>
<li><a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a><ul>
<li><a href="https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning">Neural Networks and Deep Learning</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-network?specialization=deep-learning">Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</a></li>
<li><a href="https://www.coursera.org/learn/machine-learning-projects?specialization=deep-learning">Structuring Machine Learning Projects</a></li>
<li><a href="https://www.coursera.org/learn/convolutional-neural-networks?specialization=deep-learning">Convolutional Neural Networks</a></li>
<li><a href="https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning">Sequence Models</a></li>
</ul>
</li>
<li><a href="https://www.coursera.org/learn/bayesian-statistics">Bayesian Statistics: From Concept to Data Analysis</a></li>
<li><a href="https://www.coursera.org/specializations/generative-adversarial-networks-gans">Generative Adversarial Networks (GANs) Specialization</a><ul>
<li><a href="https://www.coursera.org/learn/build-better-generative-adversarial-networks-gans">Build Better Generative Adversarial Networks (GANs)</a></li>
<li><a href="https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans">Build Basic Generative Adversarial Networks (GANs)</a></li>
</ul>
</li>
</ul>
<p>Online courses:</p>
<ul>
<li><a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2017-fall.html">Machine Lerning by Hung-yi Lee</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>tags</title>
    <url>/tags/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>categories</title>
    <url>/categories/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Reading</title>
    <url>/reading/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
