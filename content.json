{"meta":{"title":"Xiaoyu Xie's Blog","subtitle":"Machine Learning, Deep Learning, Computer Vision","description":"Track The past. Organize the present. Design the future.","author":"Xiaoyu Xie","url":"http://example.com","root":"/"},"pages":[{"title":"Xiaoyu Xie","date":"2021-06-26T08:03:18.962Z","updated":"2021-06-26T08:03:18.938Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"BackgroundI am Xiaoyu Xie, a first-year PhD student at Northwestern University. Research Interests: Computer Vision; Deep Learning in Manufacturing, Mechanics, and Finace; Bayesian Deep Learning; Machine Learning related courses that I have studiedNorthwestern University: Machine Learning: Foundations, Applications, and Algorithms Statistical Pattern Recognition Mechanistic Data Science for Engineering Engineering Optimization for Product Design and Manufacturing High Performance Computing for Multiphysics Applications Coursera: Deep Learning Specialization Neural Networks and Deep Learning Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization Structuring Machine Learning Projects Convolutional Neural Networks Sequence Models Bayesian Statistics: From Concept to Data Analysis Generative Adversarial Networks (GANs) Specialization Build Better Generative Adversarial Networks (GANs) Build Basic Generative Adversarial Networks (GANs) Online courses: Machine Lerning by Hung-yi Lee"},{"title":"tags","date":"2021-01-07T04:08:10.118Z","updated":"2021-01-07T04:08:10.112Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2021-01-07T04:21:57.215Z","updated":"2021-01-07T04:21:57.207Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Reading","date":"2021-01-05T11:27:50.020Z","updated":"2021-01-05T11:27:50.007Z","comments":true,"path":"reading/index.html","permalink":"http://example.com/reading/index.html","excerpt":"","text":""}],"posts":[{"title":"GANs specialization Class 2 Week4 Notes and codes","slug":"GANs-specialization-Class-2-Week4-Notes-and-codes","date":"2021-06-19T13:08:27.000Z","updated":"2021-06-19T13:10:50.759Z","comments":true,"path":"2021/06/19/GANs-specialization-Class-2-Week4-Notes-and-codes/","link":"","permalink":"http://example.com/2021/06/19/GANs-specialization-Class-2-Week4-Notes-and-codes/","excerpt":"Class 2 week 1 assignmentEvaluating GANsGoalsIn this notebook, you’re going to gain a better understanding of some of the challenges that come with evaluating GANs and a response you can take to alleviate some of them called Fréchet Inception Distance (FID).","text":"Class 2 week 1 assignmentEvaluating GANsGoalsIn this notebook, you’re going to gain a better understanding of some of the challenges that come with evaluating GANs and a response you can take to alleviate some of them called Fréchet Inception Distance (FID). Learning Objectives Understand the challenges associated with evaluating GANs. Write code to evaluate the Fréchet Inception Distance. Challenges With Evaluating GANsLoss is Uninformative of PerformanceOne aspect that makes evaluating GANs challenging is that the loss tells us little about their performance. Unlike with classifiers, where a low loss on a test set indicates superior performance, a low loss for the generator or discriminator suggests that learning has stopped. No Clear Non-human MetricIf you define the goal of a GAN as “generating images which look real to people” then it’s technically possible to measure this directly: you can ask people to act as a discriminator. However, this takes significant time and money so ideally you can use a proxy for this. There is also no “perfect” discriminator that can differentiate reals from fakes - if there were, a lot of machine learning tasks would be solved ;) In this notebook, you will implement Fréchet Inception Distance, one method which aims to solve these issues. Getting StartedFor this notebook, you will again be using CelebA. You will start by loading a pre-trained generator which has been trained on CelebA. Here, you will import some useful libraries and packages. You will also be provided with the generator and noise code from earlier assignments. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import torchimport numpy as npfrom torch import nnfrom tqdm.auto import tqdmfrom torchvision import transformsfrom torchvision.datasets import CelebAfrom torchvision.utils import make_gridfrom torch.utils.data import DataLoaderimport matplotlib.pyplot as plttorch.manual_seed(0) # Set for our testing purposes, please do not change!class Generator(nn.Module): &#x27;&#x27;&#x27; Generator Class Values: z_dim: the dimension of the noise vector, a scalar im_chan: the number of channels in the images, fitted for the dataset used, a scalar (CelebA is rgb, so 3 is your default) hidden_dim: the inner dimension, a scalar &#x27;&#x27;&#x27; def __init__(self, z_dim=10, im_chan=3, hidden_dim=64): super(Generator, self).__init__() self.z_dim = z_dim # Build the neural network self.gen = nn.Sequential( self.make_gen_block(z_dim, hidden_dim * 8), self.make_gen_block(hidden_dim * 8, hidden_dim * 4), self.make_gen_block(hidden_dim * 4, hidden_dim * 2), self.make_gen_block(hidden_dim * 2, hidden_dim), self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True), ) def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False): &#x27;&#x27;&#x27; Function to return a sequence of operations corresponding to a generator block of DCGAN; a transposed convolution, a batchnorm (except in the final layer), and an activation. Parameters: input_channels: how many channels the input feature representation has output_channels: how many channels the output feature representation should have kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size) stride: the stride of the convolution final_layer: a boolean, true if it is the final layer and false otherwise (affects activation and batchnorm) &#x27;&#x27;&#x27; if not final_layer: return nn.Sequential( nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride), nn.BatchNorm2d(output_channels), nn.ReLU(inplace=True), ) else: return nn.Sequential( nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride), nn.Tanh(), ) def forward(self, noise): &#x27;&#x27;&#x27; Function for completing a forward pass of the generator: Given a noise tensor, returns generated images. Parameters: noise: a noise tensor with dimensions (n_samples, z_dim) &#x27;&#x27;&#x27; x = noise.view(len(noise), self.z_dim, 1, 1) return self.gen(x)def get_noise(n_samples, z_dim, device=&#x27;cpu&#x27;): &#x27;&#x27;&#x27; Function for creating noise vectors: Given the dimensions (n_samples, z_dim) creates a tensor of that shape filled with random numbers from the normal distribution. Parameters: n_samples: the number of samples to generate, a scalar z_dim: the dimension of the noise vector, a scalar device: the device type &#x27;&#x27;&#x27; return torch.randn(n_samples, z_dim, device=device) Loading the Pre-trained ModelNow, you can set the arguments for the model and load the dataset: z_dim: the dimension of the noise vector image_size: the image size of the input to Inception (more details in the following section) device: the device type 123456789101112131415161718z_dim = 64image_size = 299device = &#x27;cuda&#x27;transform = transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])in_coursera = True # Set this to false if you&#x27;re running this outside Courseraif in_coursera: import numpy as np data = torch.Tensor(np.load(&#x27;fid_images_tensor.npz&#x27;, allow_pickle=True)[&#x27;arr_0&#x27;]) dataset = torch.utils.data.TensorDataset(data, data)else: dataset = CelebA(&quot;.&quot;, download=True, transform=transform) Then, you can load and initialize the model with weights from a pre-trained model. This allows you to use the pre-trained model as if you trained it yourself. 123gen = Generator(z_dim).to(device)gen.load_state_dict(torch.load(f&quot;pretrained_celeba.pth&quot;, map_location=torch.device(device))[&quot;gen&quot;])gen = gen.eval() Inception-v3 NetworkInception-V3 is a neural network trained on ImageNet to classify objects. You may recall from the lectures that ImageNet has over 1 million images to train on. As a result, Inception-V3 does a good job detecting features and classifying images. Here, you will load Inception-V3 as inception_model. 12345from torchvision.models import inception_v3inception_model = inception_v3(pretrained=False)inception_model.load_state_dict(torch.load(&quot;inception_v3_google-1a9a5a14.pth&quot;))inception_model.to(device)inception_model = inception_model.eval() # Evaluation mode Fréchet Inception DistanceFréchet Inception Distance (FID) was proposed as an improvement over Inception Score and still uses the Inception-v3 network as part of its calculation. However, instead of using the classification labels of the Inception-v3 network, it uses the output from an earlier layer—the layer right before the labels. This is often called the feature layer. Research has shown that deep convolutional neural networks trained on difficult tasks, like classifying many classes, build increasingly sophisticated representations of features going deeper into the network. For example, the first few layers may learn to detect different kinds of edges and curves, while the later layers may have neurons that fire in response to human faces. To get the feature layer of a convolutional neural network, you can replace the final fully connected layer with an identity layer that simply returns whatever input it received, unchanged. This essentially removes the final classification layer and leaves you with the intermediate outputs from the layer before. Optional hint for inception_model.fc You may find torch.nn.Identity() helpful. 123456789# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED CELL: inception_model.fc# You want to replace the final fully-connected (fc) layer # with an identity function layer to cut off the classification# layer and get a feature extractor#### START CODE HERE ####inception_model.fc = nn.Identity()#### END CODE HERE #### 1234# UNIT TESTtest_identity_noise = torch.randn(100, 100)assert torch.equal(test_identity_noise, inception_model.fc(test_identity_noise))print(&quot;Success!&quot;) Success! Fréchet DistanceFréchet distance uses the values from the feature layer for two sets of images, say reals and fakes, and compares different statistical properties between them to see how different they are. Specifically, Fréchet distance finds the shortest distance needed to walk along two lines, or two curves, simultaneously. The most intuitive explanation of Fréchet distance is as the “minimum leash distance” between two points. Imagine yourself and your dog, both moving along two curves. If you walked on one curve and your dog, attached to a leash, walked on the other at the same pace, what is the least amount of leash that you can give your dog so that you never need to give them more slack during your walk? Using this, the Fréchet distance measures the similarity between these two curves. The basic idea is similar for calculating the Fréchet distance between two probability distributions. You’ll start by seeing what this looks like in one-dimensional, also called univariate, space. Univariate Fréchet DistanceYou can calculate the distance between two normal distributions $X$ and $Y$ with means $\\mu_X$ and $\\mu_Y$ and standard deviations $\\sigma_X$ and $\\sigma_Y$, as: $$d(X,Y) = (\\mu_X-\\mu_Y)^2 + (\\sigma_X-\\sigma_Y)^2 $$ Pretty simple, right? Now you can see how it can be converted to be used in multi-dimensional, which is also called multivariate, space. Multivariate Fréchet DistanceCovariance To find the Fréchet distance between two multivariate normal distributions, you first need to find the covariance instead of the standard deviation. The covariance, which is the multivariate version of variance (the square of standard deviation), is represented using a square matrix where the side length is equal to the number of dimensions. Since the feature vectors you will be using have 2048 values/weights, the covariance matrix will be 2048 x 2048. But for the sake of an example, this is a covariance matrix in a two-dimensional space: $\\Sigma = \\left(\\begin{array}{cc}1 &amp; 0\\0 &amp; 1\\end{array}\\right)$ The value at location $(i, j)$ corresponds to the covariance of vector $i$ with vector $j$. Since the covariance of $i$ with $j$ and $j$ with $i$ are equivalent, the matrix will always be symmetric with respect to the diagonal. The diagonal is the covariance of that element with itself. In this example, there are zeros everywhere except the diagonal. That means that the two dimensions are independent of one another, they are completely unrelated. The following code cell will visualize this matrix. 1234567891011121314#import os#os.environ[&#x27;KMP_DUPLICATE_LIB_OK&#x27;]=&#x27;True&#x27;from torch.distributions import MultivariateNormalimport seaborn as sns # This is for visualizationmean = torch.Tensor([0, 0]) # Center the mean at the origincovariance = torch.Tensor( # This matrix shows independence - there are only non-zero values on the diagonal [[1, 0], [0, 1]])independent_dist = MultivariateNormal(mean, covariance)samples = independent_dist.sample((10000,))res = sns.jointplot(samples[:, 0], samples[:, 1], kind=&quot;kde&quot;)plt.show() Now, here’s an example of a multivariate normal distribution that has covariance: $\\Sigma = \\left(\\begin{array}{cc}2 &amp; -1\\-1 &amp; 2\\end{array}\\right)$ And see how it looks: 123456789mean = torch.Tensor([0, 0])covariance = torch.Tensor( [[2, -1], [-1, 2]])covariant_dist = MultivariateNormal(mean, covariance)samples = covariant_dist.sample((10000,))res = sns.jointplot(samples[:, 0], samples[:, 1], kind=&quot;kde&quot;)plt.show() Formula Based on the paper, “The Fréchet distance between multivariate normal distributions“ by Dowson and Landau (1982), the Fréchet distance between two multivariate normal distributions $X$ and $Y$ is: $d(X, Y) = \\Vert\\mu_X-\\mu_Y\\Vert^2 + \\mathrm{Tr}\\left(\\Sigma_X+\\Sigma_Y - 2 \\sqrt{\\Sigma_X \\Sigma_Y}\\right)$ Similar to the formula for univariate Fréchet distance, you can calculate the distance between the means and the distance between the standard deviations. However, calculating the distance between the standard deviations changes slightly here, as it includes the matrix product and matrix square root. $\\mathrm{Tr}$ refers to the trace, the sum of the diagonal elements of a matrix. Now you can implement this! Optional hints for frechet_distance You want to implement the above equation in code. You might find the functions torch.norm and torch.trace helpful here. A matrix_sqrt function is defined for you above – you need to use it instead of torch.sqrt() which only gets the elementwise square root instead of the matrix square root. You can also use the @ symbol for matrix multiplication. 123456789101112import scipy# This is the matrix square root function you will be usingdef matrix_sqrt(x): &#x27;&#x27;&#x27; Function that takes in a matrix and returns the square root of that matrix. For an input matrix A, the output matrix B would be such that B @ B is the matrix A. Parameters: x: a matrix &#x27;&#x27;&#x27; y = x.cpu().detach().numpy() y = scipy.linalg.sqrtm(y) return torch.Tensor(y.real, device=x.device) 123456789101112131415# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: frechet_distancedef frechet_distance(mu_x, mu_y, sigma_x, sigma_y): &#x27;&#x27;&#x27; Function for returning the Fréchet distance between multivariate Gaussians, parameterized by their means and covariance matrices. Parameters: mu_x: the mean of the first Gaussian, (n_features) mu_y: the mean of the second Gaussian, (n_features) sigma_x: the covariance matrix of the first Gaussian, (n_features, n_features) sigma_y: the covariance matrix of the second Gaussian, (n_features, n_features) &#x27;&#x27;&#x27; #### START CODE HERE #### return torch.norm(mu_x-mu_y, 2) + torch.trace(sigma_x+sigma_y-2*matrix_sqrt(torch.matmul(sigma_x, sigma_y))) #### END CODE HERE #### 123456789101112131415161718192021222324252627282930# UNIT TESTmean1 = torch.Tensor([0, 0]) # Center the mean at the origincovariance1 = torch.Tensor( # This matrix shows independence - there are only non-zero values on the diagonal [[1, 0], [0, 1]])dist1 = MultivariateNormal(mean1, covariance1)mean2 = torch.Tensor([0, 0]) # Center the mean at the origincovariance2 = torch.Tensor( # This matrix shows dependence [[2, -1], [-1, 2]])dist2 = MultivariateNormal(mean2, covariance2)assert torch.isclose( frechet_distance( dist1.mean, dist2.mean, dist1.covariance_matrix, dist2.covariance_matrix ), 4 - 2 * torch.sqrt(torch.tensor(3.)))assert (frechet_distance( dist1.mean, dist1.mean, dist1.covariance_matrix, dist1.covariance_matrix ).item() == 0)print(&quot;Success!&quot;) Success! Putting it all together!Now, you can apply FID to your generator from earlier. You will start by defining a bit of helper code to preprocess the image for the Inception-v3 network: 123def preprocess(img): img = torch.nn.functional.interpolate(img, size=(299, 299), mode=&#x27;bilinear&#x27;, align_corners=False) return img Then, you’ll define a function to calculate the covariance of the features that returns a covariance matrix given a list of values: 123import numpy as npdef get_covariance(features): return torch.Tensor(np.cov(features.detach().numpy(), rowvar=False)) Finally, you can use the pre-trained Inception-v3 model to compute features of the real and fake images. With these features, you can then get the covariance and means of these features across many samples. First, you get the features of the real and fake images using the Inception-v3 model: 1234567891011121314151617181920212223242526272829fake_features_list = []real_features_list = []gen.eval()n_samples = 512 # The total number of samplesbatch_size = 4 # Samples per iterationdataloader = DataLoader( dataset, batch_size=batch_size, shuffle=True)cur_samples = 0with torch.no_grad(): # You don&#x27;t need to calculate gradients here, so you do this to save memory try: for real_example, _ in tqdm(dataloader, total=n_samples // batch_size): # Go by batch real_samples = real_example real_features = inception_model(real_samples.to(device)).detach().to(&#x27;cpu&#x27;) # Move features to CPU real_features_list.append(real_features) fake_samples = get_noise(len(real_example), z_dim).to(device) fake_samples = preprocess(gen(fake_samples)) fake_features = inception_model(fake_samples.to(device)).detach().to(&#x27;cpu&#x27;) fake_features_list.append(fake_features) cur_samples += len(real_samples) if cur_samples &gt;= n_samples: break except: print(&quot;Error in loop&quot;) HBox(children=(FloatProgress(value=0.0, max=128.0), HTML(value=&#39;&#39;))) Then, you can combine all of the values that you collected for the reals and fakes into large tensors: 1234# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# UNIT TEST COMMENT: Needed as is for autogradingfake_features_all = torch.cat(fake_features_list)real_features_all = torch.cat(real_features_list) 1fake_features_all.shape torch.Size([512, 2048]) And calculate the covariance and means of these real and fake features: 1234567891011# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED CELL# Calculate the covariance matrix for the fake and real features# and also calculate the means of the feature over the batch (for each feature dimension mean)#### START CODE HERE ####mu_fake = torch.mean(fake_features_all, 0)mu_real = torch.mean(real_features_all, 0)sigma_fake = get_covariance(fake_features_all)sigma_real = get_covariance(real_features_all)#### END CODE HERE #### 123456789assert tuple(sigma_fake.shape) == (fake_features_all.shape[1], fake_features_all.shape[1])assert torch.abs(sigma_fake[0, 0] - 2.5e-2) &lt; 1e-2 and torch.abs(sigma_fake[-1, -1] - 5e-2) &lt; 1e-2assert tuple(sigma_real.shape) == (real_features_all.shape[1], real_features_all.shape[1])assert torch.abs(sigma_real[0, 0] - 3.5768e-2) &lt; 1e-4 and torch.abs(sigma_real[0, 1] + 5.3236e-4) &lt; 1e-4assert tuple(mu_fake.shape) == (fake_features_all.shape[1],)assert tuple(mu_real.shape) == (real_features_all.shape[1],)assert torch.abs(mu_real[0] - 0.3099) &lt; 0.01 and torch.abs(mu_real[1] - 0.2721) &lt; 0.01assert torch.abs(mu_fake[0] - 0.37) &lt; 0.05 and torch.abs(mu_real[1] - 0.27) &lt; 0.05print(&quot;Success!&quot;) Success! At this point, you can also visualize what the pairwise multivariate distributions of the inception features look like! 12345678910111213indices = [2, 4, 5]fake_dist = MultivariateNormal(mu_fake[indices], sigma_fake[indices][:, indices])fake_samples = fake_dist.sample((5000,))real_dist = MultivariateNormal(mu_real[indices], sigma_real[indices][:, indices])real_samples = real_dist.sample((5000,))import pandas as pddf_fake = pd.DataFrame(fake_samples.numpy(), columns=indices)df_real = pd.DataFrame(real_samples.numpy(), columns=indices)df_fake[&quot;is_real&quot;] = &quot;no&quot;df_real[&quot;is_real&quot;] = &quot;yes&quot;df = pd.concat([df_fake, df_real])sns.pairplot(df, plot_kws=&#123;&#x27;alpha&#x27;: 0.1&#125;, hue=&#x27;is_real&#x27;) &lt;seaborn.axisgrid.PairGrid at 0x7f3ea941ed30&gt; Lastly, you can use your earlier frechet_distance function to calculate the FID and evaluate your GAN. You can see how similar/different the features of the generated images are to the features of the real images. The next cell might take five minutes or so to run in Coursera. 12with torch.no_grad(): print(frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake).item()) You’ll notice this model gets a pretty high FID, likely over 30. Since lower is better, and the best models on CelebA get scores in the single-digits, there’s clearly a long way to go with this model. You can use FID to compare different models, as well as different stages of training of the same model.","categories":[{"name":"Class notes","slug":"Class-notes","permalink":"http://example.com/categories/Class-notes/"}],"tags":[{"name":"Notes","slug":"Notes","permalink":"http://example.com/tags/Notes/"},{"name":"Online course","slug":"Online-course","permalink":"http://example.com/tags/Online-course/"},{"name":"GANs","slug":"GANs","permalink":"http://example.com/tags/GANs/"}]},{"title":"GANs specialization Week4-1: Notes and codes","slug":"GANs-specialization-Week4-Notes-and-codes","date":"2021-05-23T15:39:55.000Z","updated":"2021-05-23T15:45:13.438Z","comments":true,"path":"2021/05/23/GANs-specialization-Week4-Notes-and-codes/","link":"","permalink":"http://example.com/2021/05/23/GANs-specialization-Week4-Notes-and-codes/","excerpt":"Assignment 1: Build a Conditional GANGoalsIn this notebook, you’re going to make a conditional GAN in order to generate hand-written images of digits, conditioned on the digit to be generated (the class vector). This will let you choose what digit you want to generate. You’ll then do some exploration of the generated images to visualize what the noise and class vectors mean.","text":"Assignment 1: Build a Conditional GANGoalsIn this notebook, you’re going to make a conditional GAN in order to generate hand-written images of digits, conditioned on the digit to be generated (the class vector). This will let you choose what digit you want to generate. You’ll then do some exploration of the generated images to visualize what the noise and class vectors mean. Learning Objectives Learn the technical difference between a conditional and unconditional GAN. Understand the distinction between the class and noise vector in a conditional GAN. Getting StartedFor this assignment, you will be using the MNIST dataset again, but there’s nothing stopping you from applying this generator code to produce images of animals conditioned on the species or pictures of faces conditioned on facial characteristics. Note that this assignment requires no changes to the architectures of the generator or discriminator, only changes to the data passed to both. The generator will no longer take z_dim as an argument, but input_dim instead, since you need to pass in both the noise and class vectors. In addition to good variable naming, this also means that you can use the generator and discriminator code you have previously written with different parameters. You will begin by importing the necessary libraries and building the generator and discriminator. Packages and Visualization123456789101112131415161718192021import torchfrom torch import nnfrom tqdm.auto import tqdmfrom torchvision import transformsfrom torchvision.datasets import MNISTfrom torchvision.utils import make_gridfrom torch.utils.data import DataLoaderimport matplotlib.pyplot as plttorch.manual_seed(0) # Set for our testing purposes, please do not change!def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True): &#x27;&#x27;&#x27; Function for visualizing images: Given a tensor of images, number of images, and size per image, plots and prints the images in an uniform grid. &#x27;&#x27;&#x27; image_tensor = (image_tensor + 1) / 2 image_unflat = image_tensor.detach().cpu() image_grid = make_grid(image_unflat[:num_images], nrow=nrow) plt.imshow(image_grid.permute(1, 2, 0).squeeze()) if show: plt.show() Generator and Noise12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class Generator(nn.Module): &#x27;&#x27;&#x27; Generator Class Values: input_dim: the dimension of the input vector, a scalar im_chan: the number of channels in the images, fitted for the dataset used, a scalar (MNIST is black-and-white, so 1 channel is your default) hidden_dim: the inner dimension, a scalar &#x27;&#x27;&#x27; def __init__(self, input_dim=10, im_chan=1, hidden_dim=64): super(Generator, self).__init__() self.input_dim = input_dim # Build the neural network self.gen = nn.Sequential( self.make_gen_block(input_dim, hidden_dim * 4), self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1), self.make_gen_block(hidden_dim * 2, hidden_dim), self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True), ) def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False): &#x27;&#x27;&#x27; Function to return a sequence of operations corresponding to a generator block of DCGAN; a transposed convolution, a batchnorm (except in the final layer), and an activation. Parameters: input_channels: how many channels the input feature representation has output_channels: how many channels the output feature representation should have kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size) stride: the stride of the convolution final_layer: a boolean, true if it is the final layer and false otherwise (affects activation and batchnorm) &#x27;&#x27;&#x27; if not final_layer: return nn.Sequential( nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride), nn.BatchNorm2d(output_channels), nn.ReLU(inplace=True), ) else: return nn.Sequential( nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride), nn.Tanh(), ) def forward(self, noise): &#x27;&#x27;&#x27; Function for completing a forward pass of the generator: Given a noise tensor, returns generated images. Parameters: noise: a noise tensor with dimensions (n_samples, input_dim) &#x27;&#x27;&#x27; x = noise.view(len(noise), self.input_dim, 1, 1) return self.gen(x)def get_noise(n_samples, input_dim, device=&#x27;cpu&#x27;): &#x27;&#x27;&#x27; Function for creating noise vectors: Given the dimensions (n_samples, input_dim) creates a tensor of that shape filled with random numbers from the normal distribution. Parameters: n_samples: the number of samples to generate, a scalar input_dim: the dimension of the input vector, a scalar device: the device type &#x27;&#x27;&#x27; return torch.randn(n_samples, input_dim, device=device) Discriminator123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Discriminator(nn.Module): &#x27;&#x27;&#x27; Discriminator Class Values: im_chan: the number of channels in the images, fitted for the dataset used, a scalar (MNIST is black-and-white, so 1 channel is your default) hidden_dim: the inner dimension, a scalar &#x27;&#x27;&#x27; def __init__(self, im_chan=1, hidden_dim=64): super(Discriminator, self).__init__() self.disc = nn.Sequential( self.make_disc_block(im_chan, hidden_dim), self.make_disc_block(hidden_dim, hidden_dim * 2), self.make_disc_block(hidden_dim * 2, 1, final_layer=True), ) def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False): &#x27;&#x27;&#x27; Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer). Parameters: input_channels: how many channels the input feature representation has output_channels: how many channels the output feature representation should have kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size) stride: the stride of the convolution final_layer: a boolean, true if it is the final layer and false otherwise (affects activation and batchnorm) &#x27;&#x27;&#x27; if not final_layer: return nn.Sequential( nn.Conv2d(input_channels, output_channels, kernel_size, stride), nn.BatchNorm2d(output_channels), nn.LeakyReLU(0.2, inplace=True), ) else: return nn.Sequential( nn.Conv2d(input_channels, output_channels, kernel_size, stride), ) def forward(self, image): &#x27;&#x27;&#x27; Function for completing a forward pass of the discriminator: Given an image tensor, returns a 1-dimension tensor representing fake/real. Parameters: image: a flattened image tensor with dimension (im_chan) &#x27;&#x27;&#x27; disc_pred = self.disc(image) return disc_pred.view(len(disc_pred), -1) Class InputIn conditional GANs, the input vector for the generator will also need to include the class information. The class is represented using a one-hot encoded vector where its length is the number of classes and each index represents a class. The vector is all 0’s and a 1 on the chosen class. Given the labels of multiple images (e.g. from a batch) and number of classes, please create one-hot vectors for each label. There is a class within the PyTorch functional library that can help you. Optional hints for get_one_hot_labels This code can be done in one line. The documentation for F.one_hot may be helpful. 1234567891011121314# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: get_one_hot_labelsimport torch.nn.functional as Fdef get_one_hot_labels(labels, n_classes): &#x27;&#x27;&#x27; Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes). Parameters: labels: tensor of labels from the dataloader, size (?) n_classes: the total number of classes in the dataset, an integer scalar &#x27;&#x27;&#x27; #### START CODE HERE #### return F.one_hot(labels, n_classes).float() #### END CODE HERE #### 123456789101112assert ( get_one_hot_labels( labels=torch.Tensor([[0, 2, 1]]).long(), n_classes=3 ).tolist() == [[ [1, 0, 0], [0, 0, 1], [0, 1, 0] ]])print(&quot;Success!&quot;) Success! Next, you need to be able to concatenate the one-hot class vector to the noise vector before giving it to the generator. You will also need to do this when adding the class channels to the discriminator. To do this, you will need to write a function that combines two vectors. Remember that you need to ensure that the vectors are the same type: floats. Again, you can look to the PyTorch library for help. Optional hints for combine_vectors This code can also be written in one line. The documentation for torch.cat may be helpful. Specifically, you might want to look at what the dim argument of torch.cat does. 123456789101112131415161718# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: combine_vectorsdef combine_vectors(x, y): &#x27;&#x27;&#x27; Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?). Parameters: x: (n_samples, ?) the first vector. In this assignment, this will be the noise vector of shape (n_samples, z_dim), but you shouldn&#x27;t need to know the second dimension&#x27;s size. y: (n_samples, ?) the second vector. Once again, in this assignment this will be the one-hot class vector with the shape (n_samples, n_classes), but you shouldn&#x27;t assume this in your code. &#x27;&#x27;&#x27; # Note: Make sure this function outputs a float no matter what inputs it receives #### START CODE HERE #### combined = torch.cat((x, y), 1).float() #### END CODE HERE #### return combined 12345678910combined = combine_vectors(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]));# Check exact order of elementsassert torch.all(combined == torch.tensor([[1, 2, 5, 6], [3, 4, 7, 8]]))# Tests that items are of float typeassert (type(combined[0][0].item()) == float)# Check shapescombined = combine_vectors(torch.randn(1, 4, 5), torch.randn(1, 8, 5));assert tuple(combined.shape) == (1, 12, 5)assert tuple(combine_vectors(torch.randn(1, 10, 12).long(), torch.randn(1, 20, 12).long()).shape) == (1, 30, 12)print(&quot;Success!&quot;) Success! TrainingNow you can start to put it all together!First, you will define some new parameters: mnist_shape: the number of pixels in each MNIST image, which has dimensions 28 x 28 and one channel (because it’s black-and-white) so 1 x 28 x 28 n_classes: the number of classes in MNIST (10, since there are the digits from 0 to 9) 12mnist_shape = (1, 28, 28)n_classes = 10 And you also include the same parameters from previous assignments: criterion: the loss function n_epochs: the number of times you iterate through the entire dataset when training z_dim: the dimension of the noise vector display_step: how often to display/visualize the images batch_size: the number of images per forward/backward pass lr: the learning rate device: the device type 1234567891011121314151617criterion = nn.BCEWithLogitsLoss()n_epochs = 200z_dim = 64display_step = 500batch_size = 128lr = 0.0002device = &#x27;cuda&#x27;transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])dataloader = DataLoader( MNIST(&#x27;.&#x27;, download=False, transform=transform), batch_size=batch_size, shuffle=True) Then, you can initialize your generator, discriminator, and optimizers. To do this, you will need to update the input dimensions for both models. For the generator, you will need to calculate the size of the input vector; recall that for conditional GANs, the generator’s input is the noise vector concatenated with the class vector. For the discriminator, you need to add a channel for every class. 12345678910111213141516171819202122# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: get_input_dimensionsdef get_input_dimensions(z_dim, mnist_shape, n_classes): &#x27;&#x27;&#x27; Function for getting the size of the conditional input dimensions from z_dim, the image shape, and number of classes. Parameters: z_dim: the dimension of the noise vector, a scalar mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28) n_classes: the total number of classes in the dataset, an integer scalar (10 for MNIST) Returns: generator_input_dim: the input dimensionality of the conditional generator, which takes the noise and class vectors discriminator_im_chan: the number of input channels to the discriminator (e.g. C x 28 x 28 for MNIST) &#x27;&#x27;&#x27; #### START CODE HERE #### generator_input_dim = z_dim + n_classes discriminator_im_chan = mnist_shape[0] + n_classes #### END CODE HERE #### return generator_input_dim, discriminator_im_chan 123456def test_input_dims(): gen_dim, disc_dim = get_input_dimensions(23, (12, 23, 52), 9) assert gen_dim == 32 assert disc_dim == 21test_input_dims()print(&quot;Success!&quot;) Success! 123456789101112131415generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)gen = Generator(input_dim=generator_input_dim).to(device)gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)disc = Discriminator(im_chan=discriminator_im_chan).to(device)disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)def weights_init(m): if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d): torch.nn.init.normal_(m.weight, 0.0, 0.02) if isinstance(m, nn.BatchNorm2d): torch.nn.init.normal_(m.weight, 0.0, 0.02) torch.nn.init.constant_(m.bias, 0)gen = gen.apply(weights_init)disc = disc.apply(weights_init) Now to train, you would like both your generator and your discriminator to know what class of image should be generated. There are a few locations where you will need to implement code. For example, if you’re generating a picture of the number “1”, you would need to: Tell that to the generator, so that it knows it should be generating a “1” Tell that to the discriminator, so that it knows it should be looking at a “1”. If the discriminator is told it should be looking at a 1 but sees something that’s clearly an 8, it can guess that it’s probably fake There are no explicit unit tests here – if this block of code runs and you don’t change any of the other variables, then you’ve done it correctly! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED CELLcur_step = 0generator_losses = []discriminator_losses = []#UNIT TEST NOTE: Initializations needed for gradingnoise_and_labels = Falsefake = Falsefake_image_and_labels = Falsereal_image_and_labels = Falsedisc_fake_pred = Falsedisc_real_pred = Falsefor epoch in range(n_epochs): # Dataloader returns the batches and the labels for real, labels in tqdm(dataloader): cur_batch_size = len(real) # Flatten the batch of real images from the dataset real = real.to(device) one_hot_labels = get_one_hot_labels(labels.to(device), n_classes) image_one_hot_labels = one_hot_labels[:, :, None, None] image_one_hot_labels = image_one_hot_labels.repeat(1, 1, mnist_shape[1], mnist_shape[2]) ### Update discriminator ### # Zero out the discriminator gradients disc_opt.zero_grad() # Get noise corresponding to the current batch_size fake_noise = get_noise(cur_batch_size, z_dim, device=device) # Now you can get the images from the generator # Steps: 1) Combine the noise vectors and the one-hot labels for the generator # 2) Generate the conditioned fake images #### START CODE HERE #### noise_and_labels = torch.cat((fake_noise,one_hot_labels), 1) fake = gen(noise_and_labels) #### END CODE HERE #### # Make sure that enough images were generated assert len(fake) == len(real) # Check that correct tensors were combined assert tuple(noise_and_labels.shape) == (cur_batch_size, fake_noise.shape[1] + one_hot_labels.shape[1]) # It comes from the correct generator assert tuple(fake.shape) == (len(real), 1, 28, 28) # Now you can get the predictions from the discriminator # Steps: 1) Create the input for the discriminator # a) Combine the fake images with image_one_hot_labels, # remember to detach the generator (.detach()) so you do not backpropagate through it # b) Combine the real images with image_one_hot_labels # 2) Get the discriminator&#x27;s prediction on the fakes as disc_fake_pred # 3) Get the discriminator&#x27;s prediction on the reals as disc_real_pred #### START CODE HERE #### fake_image_and_labels = torch.cat((fake.detach(), image_one_hot_labels), 1) real_image_and_labels = torch.cat((real.detach(), image_one_hot_labels), 1) disc_fake_pred = disc(fake_image_and_labels) disc_real_pred = disc(real_image_and_labels) #### END CODE HERE #### # Make sure shapes are correct assert tuple(fake_image_and_labels.shape) == (len(real), fake.detach().shape[1] + image_one_hot_labels.shape[1], 28 ,28) assert tuple(real_image_and_labels.shape) == (len(real), real.shape[1] + image_one_hot_labels.shape[1], 28 ,28) # Make sure that enough predictions were made assert len(disc_real_pred) == len(real) # Make sure that the inputs are different assert torch.any(fake_image_and_labels != real_image_and_labels) # Shapes must match assert tuple(fake_image_and_labels.shape) == tuple(real_image_and_labels.shape) assert tuple(disc_fake_pred.shape) == tuple(disc_real_pred.shape) disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred)) disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred)) disc_loss = (disc_fake_loss + disc_real_loss) / 2 disc_loss.backward(retain_graph=True) disc_opt.step() # Keep track of the average discriminator loss discriminator_losses += [disc_loss.item()] ### Update generator ### # Zero out the generator gradients gen_opt.zero_grad() fake_image_and_labels = combine_vectors(fake, image_one_hot_labels) # This will error if you didn&#x27;t concatenate your labels to your image correctly disc_fake_pred = disc(fake_image_and_labels) gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) gen_loss.backward() gen_opt.step() # Keep track of the generator losses generator_losses += [gen_loss.item()] # if cur_step % display_step == 0 and cur_step &gt; 0: gen_mean = sum(generator_losses[-display_step:]) / display_step disc_mean = sum(discriminator_losses[-display_step:]) / display_step print(f&quot;Step &#123;cur_step&#125;: Generator loss: &#123;gen_mean&#125;, discriminator loss: &#123;disc_mean&#125;&quot;) show_tensor_images(fake) show_tensor_images(real) step_bins = 20 x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins) num_examples = (len(generator_losses) // step_bins) * step_bins plt.plot( range(num_examples // step_bins), torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1), label=&quot;Generator Loss&quot; ) plt.plot( range(num_examples // step_bins), torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1), label=&quot;Discriminator Loss&quot; ) plt.legend() plt.show() elif cur_step == 0: print(&quot;Congratulations! If you&#x27;ve gotten here, it&#x27;s working. Please let this train until you&#x27;re happy with how the generated numbers look, and then go on to the exploration!&quot;) cur_step += 1 Step 87500: Generator loss: 0.9753798842430115, discriminator loss: 0.6099519475102425 ExplorationYou can do a bit of exploration now! 1234# Before you explore, you should put the generator# in eval mode, both in general and so that batch norm# doesn&#x27;t cause you issues and is using its eval statisticsgen = gen.eval() Changing the Class VectorYou can generate some numbers with your new model! You can add interpolation as well to make it more interesting. So starting from a image, you will produce intermediate images that look more and more like the ending image until you get to the final image. Your’re basically morphing one image into another. You can choose what these two images will be using your conditional GAN. 123456789101112131415161718192021222324252627282930313233343536373839404142import math### Change me! ###n_interpolation = 9 # Choose the interpolation: how many intermediate images you want + 2 (for the start and end image)interpolation_noise = get_noise(1, z_dim, device=device).repeat(n_interpolation, 1)def interpolate_class(first_number, second_number): first_label = get_one_hot_labels(torch.Tensor([first_number]).long(), n_classes) second_label = get_one_hot_labels(torch.Tensor([second_number]).long(), n_classes) # Calculate the interpolation vector between the two labels percent_second_label = torch.linspace(0, 1, n_interpolation)[:, None] interpolation_labels = first_label * (1 - percent_second_label) + second_label * percent_second_label # Combine the noise and the labels noise_and_labels = combine_vectors(interpolation_noise, interpolation_labels.to(device)) fake = gen(noise_and_labels) show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=False)### Change me! ###start_plot_number = 1 # Choose the start digit### Change me! ###end_plot_number = 5 # Choose the end digitplt.figure(figsize=(8, 8))interpolate_class(start_plot_number, end_plot_number)_ = plt.axis(&#x27;off&#x27;)## Uncomment the following lines of code if you would like to visualize a set of pairwise class ## interpolations for a collection of different numbers, all in a single grid of interpolations.## You&#x27;ll also see another visualization like this in the next code block!plot_numbers = [2, 3, 4, 5, 7]n_numbers = len(plot_numbers)plt.figure(figsize=(8, 8))for i, first_plot_number in enumerate(plot_numbers): for j, second_plot_number in enumerate(plot_numbers): plt.subplot(n_numbers, n_numbers, i * n_numbers + j + 1) interpolate_class(first_plot_number, second_plot_number) plt.axis(&#x27;off&#x27;)plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)plt.show()plt.close() Changing the Noise VectorNow, what happens if you hold the class constant, but instead you change the noise vector? You can also interpolate the noise vector and generate an image at each step. 12345678910111213141516171819202122232425262728n_interpolation = 9 # How many intermediate images you want + 2 (for the start and end image)# This time you&#x27;re interpolating between the noise instead of the labelsinterpolation_label = get_one_hot_labels(torch.Tensor([5]).long(), n_classes).repeat(n_interpolation, 1).float()def interpolate_noise(first_noise, second_noise): # This time you&#x27;re interpolating between the noise instead of the labels percent_first_noise = torch.linspace(0, 1, n_interpolation)[:, None].to(device) interpolation_noise = first_noise * percent_first_noise + second_noise * (1 - percent_first_noise) # Combine the noise and the labels again noise_and_labels = combine_vectors(interpolation_noise, interpolation_label.to(device)) fake = gen(noise_and_labels) show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=False)# Generate noise vectors to interpolate between### Change me! ###n_noise = 5 # Choose the number of noise examples in the gridplot_noises = [get_noise(1, z_dim, device=device) for i in range(n_noise)]plt.figure(figsize=(8, 8))for i, first_plot_noise in enumerate(plot_noises): for j, second_plot_noise in enumerate(plot_noises): plt.subplot(n_noise, n_noise, i * n_noise + j + 1) interpolate_noise(first_plot_noise, second_plot_noise) plt.axis(&#x27;off&#x27;)plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)plt.show()plt.close() 1","categories":[{"name":"Class notes","slug":"Class-notes","permalink":"http://example.com/categories/Class-notes/"}],"tags":[{"name":"Notes","slug":"Notes","permalink":"http://example.com/tags/Notes/"},{"name":"Online course","slug":"Online-course","permalink":"http://example.com/tags/Online-course/"},{"name":"GANs","slug":"GANs","permalink":"http://example.com/tags/GANs/"}]},{"title":"GANs specialization Week2: Notes and codes","slug":"GANs-specialization-Week2-Notes-and-codes","date":"2021-05-06T05:35:19.000Z","updated":"2021-05-06T05:38:49.579Z","comments":true,"path":"2021/05/06/GANs-specialization-Week2-Notes-and-codes/","link":"","permalink":"http://example.com/2021/05/06/GANs-specialization-Week2-Notes-and-codes/","excerpt":"NotesIn this week, I learnt how to implement DCGAN and TGA. The code are very helpful due to a lot of suggestions. There is an very useful interactive blog to illustrate the checkboard effects in transposed convolution. The link is here.","text":"NotesIn this week, I learnt how to implement DCGAN and TGA. The code are very helpful due to a lot of suggestions. There is an very useful interactive blog to illustrate the checkboard effects in transposed convolution. The link is here. Transposed convolution Checkboard pattern Week 2 - Assignment - Deep Convolutional GAN (DCGAN)GoalIn this notebook, you’re going to create another GAN using the MNIST dataset. You will implement a Deep Convolutional GAN (DCGAN), a very successful and influential GAN model developed in 2015. Note: here is the paper if you are interested! It might look dense now, but soon you’ll be able to understand many parts of it :) Learning Objectives Get hands-on experience making a widely used GAN: Deep Convolutional GAN (DCGAN). Train a powerful generative model. Figure: Architectural drawing of a generator from DCGAN from Radford et al (2016). Getting StartedDCGANHere are the main features of DCGAN (don’t worry about memorizing these, you will be guided through the implementation!): Since in DCGAN the activation function will be different for the output layer, you will need to check what layer is being created. You are supplied with some tests following the code cell so you can see if you’re on the right track! At the end of the generator class, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network. You are also given a function to create a noise vector. These functions are the same as the ones from the last assignment. Optional hint for make_gen_block You’ll find nn.ConvTranspose2d and nn.BatchNorm2d useful! 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: Generatorclass Generator(nn.Module): &#x27;&#x27;&#x27; Generator Class Values: z_dim: the dimension of the noise vector, a scalar im_chan: the number of channels in the images, fitted for the dataset used, a scalar (MNIST is black-and-white, so 1 channel is your default) hidden_dim: the inner dimension, a scalar &#x27;&#x27;&#x27; def __init__(self, z_dim=10, im_chan=1, hidden_dim=64): super(Generator, self).__init__() self.z_dim = z_dim # Build the neural network self.gen = nn.Sequential( self.make_gen_block(z_dim, hidden_dim * 4), self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1), self.make_gen_block(hidden_dim * 2, hidden_dim), self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True), ) def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False): &#x27;&#x27;&#x27; Function to return a sequence of operations corresponding to a generator block of DCGAN, corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation. Parameters: input_channels: how many channels the input feature representation has output_channels: how many channels the output feature representation should have kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size) stride: the stride of the convolution final_layer: a boolean, true if it is the final layer and false otherwise (affects activation and batchnorm) &#x27;&#x27;&#x27; # Steps: # 1) Do a transposed convolution using the given parameters. # 2) Do a batchnorm, except for the last layer. # 3) Follow each batchnorm with a ReLU activation. # 4) If its the final layer, use a Tanh activation after the deconvolution. # Build the neural block if not final_layer: return nn.Sequential( #### START CODE HERE #### # step 1 nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride), # step 2 nn.BatchNorm2d(output_channels), # step 3 nn.ReLU() #### END CODE HERE #### ) else: # Final Layer return nn.Sequential( #### START CODE HERE #### # step 4 nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride), nn.Tanh() #### END CODE HERE #### ) def unsqueeze_noise(self, noise): &#x27;&#x27;&#x27; Function for completing a forward pass of the generator: Given a noise tensor, returns a copy of that noise with width and height = 1 and channels = z_dim. Parameters: noise: a noise tensor with dimensions (n_samples, z_dim) &#x27;&#x27;&#x27; return noise.view(len(noise), self.z_dim, 1, 1) def forward(self, noise): &#x27;&#x27;&#x27; Function for completing a forward pass of the generator: Given a noise tensor, returns generated images. Parameters: noise: a noise tensor with dimensions (n_samples, z_dim) &#x27;&#x27;&#x27; x = self.unsqueeze_noise(noise) return self.gen(x)def get_noise(n_samples, z_dim, device=&#x27;cpu&#x27;): &#x27;&#x27;&#x27; Function for creating noise vectors: Given the dimensions (n_samples, z_dim) creates a tensor of that shape filled with random numbers from the normal distribution. Parameters: n_samples: the number of samples to generate, a scalar z_dim: the dimension of the noise vector, a scalar device: the device type &#x27;&#x27;&#x27; return torch.randn(n_samples, z_dim, device=device) 12345678910111213141516171819202122232425# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)&#x27;&#x27;&#x27;Test your make_gen_block() function&#x27;&#x27;&#x27;gen = Generator()num_test = 100# Test the hidden blocktest_hidden_noise = get_noise(num_test, gen.z_dim)test_hidden_block = gen.make_gen_block(10, 20, kernel_size=4, stride=1)test_uns_noise = gen.unsqueeze_noise(test_hidden_noise)hidden_output = test_hidden_block(test_uns_noise)# Check that it works with other stridestest_hidden_block_stride = gen.make_gen_block(20, 20, kernel_size=4, stride=2)test_final_noise = get_noise(num_test, gen.z_dim) * 20test_final_block = gen.make_gen_block(10, 20, final_layer=True)test_final_uns_noise = gen.unsqueeze_noise(test_final_noise)final_output = test_final_block(test_final_uns_noise)# Test the whole thing:test_gen_noise = get_noise(num_test, gen.z_dim)test_uns_gen_noise = gen.unsqueeze_noise(test_gen_noise)gen_output = gen(test_uns_gen_noise) Here’s the test for your generator block: 1234567891011121314151617# UNIT TESTSassert tuple(hidden_output.shape) == (num_test, 20, 4, 4)assert hidden_output.max() &gt; 1assert hidden_output.min() == 0assert hidden_output.std() &gt; 0.2assert hidden_output.std() &lt; 1assert hidden_output.std() &gt; 0.5assert tuple(test_hidden_block_stride(hidden_output).shape) == (num_test, 20, 10, 10)assert final_output.max().item() == 1assert final_output.min().item() == -1assert tuple(gen_output.shape) == (num_test, 1, 28, 28)assert gen_output.std() &gt; 0.5assert gen_output.std() &lt; 0.8print(&quot;Success!&quot;) Success! DiscriminatorThe second component you need to create is the discriminator. You will use 3 layers in your discriminator’s neural network. Like with the generator, you will need create the function to create a single neural network block for the discriminator. There are also tests at the end for you to use. Optional hint for make_disc_block You’ll find nn.Conv2d, nn.BatchNorm2d, and nn.LeakyReLU useful! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: Discriminatorclass Discriminator(nn.Module): &#x27;&#x27;&#x27; Discriminator Class Values: im_chan: the number of channels in the images, fitted for the dataset used, a scalar (MNIST is black-and-white, so 1 channel is your default) hidden_dim: the inner dimension, a scalar &#x27;&#x27;&#x27; def __init__(self, im_chan=1, hidden_dim=16): super(Discriminator, self).__init__() self.disc = nn.Sequential( self.make_disc_block(im_chan, hidden_dim), self.make_disc_block(hidden_dim, hidden_dim * 2), self.make_disc_block(hidden_dim * 2, 1, final_layer=True), ) def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False): &#x27;&#x27;&#x27; Function to return a sequence of operations corresponding to a discriminator block of DCGAN, corresponding to a convolution, a batchnorm (except for in the last layer), and an activation. Parameters: input_channels: how many channels the input feature representation has output_channels: how many channels the output feature representation should have kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size) stride: the stride of the convolution final_layer: a boolean, true if it is the final layer and false otherwise (affects activation and batchnorm) &#x27;&#x27;&#x27; # Steps: # 1) Add a convolutional layer using the given parameters. # 2) Do a batchnorm, except for the last layer. # 3) Follow each batchnorm with a LeakyReLU activation with slope 0.2. # Build the neural block if not final_layer: return nn.Sequential( #### START CODE HERE #### # # step 1 nn.Conv2d(input_channels, output_channels, kernel_size, stride), # step 2 nn.BatchNorm2d(output_channels), # step 3 nn.LeakyReLU(0.2), #### END CODE HERE #### ) else: # Final Layer return nn.Sequential( #### START CODE HERE #### # nn.Conv2d(input_channels, output_channels, kernel_size, stride), #### END CODE HERE #### ) def forward(self, image): &#x27;&#x27;&#x27; Function for completing a forward pass of the discriminator: Given an image tensor, returns a 1-dimension tensor representing fake/real. Parameters: image: a flattened image tensor with dimension (im_dim) &#x27;&#x27;&#x27; disc_pred = self.disc(image) return disc_pred.view(len(disc_pred), -1) 1234567891011121314151617181920# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)&#x27;&#x27;&#x27;Test your make_disc_block() function&#x27;&#x27;&#x27;num_test = 100gen = Generator()disc = Discriminator()test_images = gen(get_noise(num_test, gen.z_dim))# Test the hidden blocktest_hidden_block = disc.make_disc_block(1, 5, kernel_size=6, stride=3)hidden_output = test_hidden_block(test_images)# Test the final blocktest_final_block = disc.make_disc_block(1, 10, kernel_size=2, stride=5, final_layer=True)final_output = test_final_block(test_images)# Test the whole thing:disc_output = disc(test_images) Here’s a test for your discriminator block: 12345678910111213141516171819202122# Test the hidden blockassert tuple(hidden_output.shape) == (num_test, 5, 8, 8)# Because of the LeakyReLU slopeassert -hidden_output.min() / hidden_output.max() &gt; 0.15assert -hidden_output.min() / hidden_output.max() &lt; 0.25assert hidden_output.std() &gt; 0.5assert hidden_output.std() &lt; 1# Test the final blockassert tuple(final_output.shape) == (num_test, 10, 6, 6)assert final_output.max() &gt; 1.0assert final_output.min() &lt; -1.0assert final_output.std() &gt; 0.3assert final_output.std() &lt; 0.6# Test the whole thing:assert tuple(disc_output.shape) == (num_test, 1)assert disc_output.std() &gt; 0.25assert disc_output.std() &lt; 0.5print(&quot;Success!&quot;) Success! TrainingNow you can put it all together!Remember that these are your parameters: criterion: the loss function n_epochs: the number of times you iterate through the entire dataset when training z_dim: the dimension of the noise vector display_step: how often to display/visualize the images batch_size: the number of images per forward/backward pass lr: the learning rate beta_1, beta_2: the momentum term device: the device type 1234567891011121314151617181920212223criterion = nn.BCEWithLogitsLoss()z_dim = 64display_step = 500batch_size = 128# A learning rate of 0.0002 works well on DCGANlr = 0.0002# These parameters control the optimizer&#x27;s momentum, which you can read more about here:# https://distill.pub/2017/momentum/ but you don’t need to worry about it for this course!beta_1 = 0.5 beta_2 = 0.999device = &#x27;cuda&#x27;# You can tranform the image values to be between -1 and 1 (the range of the tanh activation)transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])dataloader = DataLoader( MNIST(&#x27;.&#x27;, download=False, transform=transform), batch_size=batch_size, shuffle=True) Then, you can initialize your generator, discriminator, and optimizers. 123456789101112131415gen = Generator(z_dim).to(device)gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))disc = Discriminator().to(device) disc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))# You initialize the weights to the normal distribution# with mean 0 and standard deviation 0.02def weights_init(m): if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d): torch.nn.init.normal_(m.weight, 0.0, 0.02) if isinstance(m, nn.BatchNorm2d): torch.nn.init.normal_(m.weight, 0.0, 0.02) torch.nn.init.constant_(m.bias, 0)gen = gen.apply(weights_init)disc = disc.apply(weights_init) Finally, you can train your GAN!For each epoch, you will process the entire dataset in batches. For every batch, you will update the discriminator and generator. Then, you can see DCGAN’s results! Here’s roughly the progression you should be expecting. On GPU this takes about 30 seconds per thousand steps. On CPU, this can take about 8 hours per thousand steps. You might notice that in the image of Step 5000, the generator is disproprotionately producing things that look like ones. If the discriminator didn’t learn to detect this imbalance quickly enough, then the generator could just produce more ones. As a result, it may have ended up tricking the discriminator so well that there would be no more improvement, known as mode collapse: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748n_epochs = 50cur_step = 0mean_generator_loss = 0mean_discriminator_loss = 0for epoch in range(n_epochs): # Dataloader returns the batches for real, _ in tqdm(dataloader): cur_batch_size = len(real) real = real.to(device) ## Update discriminator ## disc_opt.zero_grad() fake_noise = get_noise(cur_batch_size, z_dim, device=device) fake = gen(fake_noise) disc_fake_pred = disc(fake.detach()) disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred)) disc_real_pred = disc(real) disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred)) disc_loss = (disc_fake_loss + disc_real_loss) / 2 # Keep track of the average discriminator loss mean_discriminator_loss += disc_loss.item() / display_step # Update gradients disc_loss.backward(retain_graph=True) # Update optimizer disc_opt.step() ## Update generator ## gen_opt.zero_grad() fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device) fake_2 = gen(fake_noise_2) disc_fake_pred = disc(fake_2) gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) gen_loss.backward() gen_opt.step() # Keep track of the average generator loss mean_generator_loss += gen_loss.item() / display_step ## Visualization code ## if cur_step % display_step == 0 and cur_step &gt; 0: print(f&quot;Step &#123;cur_step&#125;: Generator loss: &#123;mean_generator_loss&#125;, discriminator loss: &#123;mean_discriminator_loss&#125;&quot;) show_tensor_images(fake) show_tensor_images(real) mean_generator_loss = 0 mean_discriminator_loss = 0 cur_step += 1 HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;))) ​ HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;))) Step 500: Generator loss: 0.9676521250009537, discriminator loss: 0.5025659155845642 HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;))) Step 23000: Generator loss: 0.7005814965963376, discriminator loss: 0.6960778114795685","categories":[{"name":"Class-notes","slug":"Class-notes","permalink":"http://example.com/categories/Class-notes/"}],"tags":[{"name":"Notes","slug":"Notes","permalink":"http://example.com/tags/Notes/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://example.com/tags/Deep-Learning/"},{"name":"Online course","slug":"Online-course","permalink":"http://example.com/tags/Online-course/"},{"name":"GANs","slug":"GANs","permalink":"http://example.com/tags/GANs/"}]},{"title":"GANs specialization Week1: Notes and codes","slug":"GANs-specialization-Week1-Notes-and-codes","date":"2021-04-26T04:30:40.000Z","updated":"2021-04-26T05:35:05.065Z","comments":true,"path":"2021/04/26/GANs-specialization-Week1-Notes-and-codes/","link":"","permalink":"http://example.com/2021/04/26/GANs-specialization-Week1-Notes-and-codes/","excerpt":"This is week 1 notes for the first course (Build Basic Generative Adversarial Networks (GANs)) in the GANs specialization. I actually leant a lot about some pratical things in doing the assignment, like how to set some hyperparameters and the usage of truncation to balance the diversity and quality. I really like DeepLearning.AI’s courses. The lectures and assignments are always well-designed.","text":"This is week 1 notes for the first course (Build Basic Generative Adversarial Networks (GANs)) in the GANs specialization. I actually leant a lot about some pratical things in doing the assignment, like how to set some hyperparameters and the usage of truncation to balance the diversity and quality. I really like DeepLearning.AI’s courses. The lectures and assignments are always well-designed. Notes Generator needs more time steps to train This can be a much harder task than discrimination. Typically, you will need the generator to take multiple steps to improve itself for every step the discriminator takes. Noise vector 𝑧 The noise vector 𝑧 has the important role of making sure the images generated from the same class 𝑦 don’t all look the same—think of it as a random seed. You generate it randomly, usually by sampling random numbers either between 0 and 1 uniformly, or from the normal distribution, which you can denote 𝑧 ~ 𝑁(0,1). The zero means the normal distribution has a mean of zero, and the 1 means that the normal distribution has a variance of 1. In reality, 𝑧 is usually larger than just 1 value to allow for more combinations of what 𝑧 could be. There’s no special number that determines what works, but 100 is standard. Some researchers might use a power of 2, like 128 or 512, but again, nothing special about the number itself, just that it’s large enough to contain a lot of possibilities. As a result, you would sample 𝑧 from that many different dimensions (constituting multiple normal distributions). Fun Fact: this is also called a spherical normal and denoted 𝑧 ~ 𝑁(0,𝐼) where the 𝐼 represents the identity matrix and means the variance is 1 in all dimensions.* Truncation trick So now that you’re a bit familiar with noise vectors, here’s another cool concept that people use to tune their outputs. It’s called the truncation trick. I like to think of the truncation trick as a way of trading off fidelity (quality) and diversity in the samples. It works like this: when you randomly sample your noise vector 𝑧, you can choose to keep that random 𝑧 or you can sample another one. Why would you want to sample another one? Well, since I’m sampling 𝑧 from a normal distribution, my model will see more of those 𝑧 values within a standard deviation from the mean than those at the tails of the distribution—and this happens during training. This means that while the model is training, it’s likely to be familiar with certain noise vectors and as a result model those areas coming from familiar noise vector regions. In these areas, my model will likely have much more realistic results, but nothing too funky, it’s not taking as many risks in those regions mapped from those familiar noise vectors. This is the trade-off between fidelity (realistic, high quality images) and diversity (variety in images). Image Credit: Modelica What the truncation trick does is resamples the noise vector 𝑧 until it falls within some bounds of the normal distribution. In fact, it samples 𝑧 from a truncated normal distribution where the tails are cut off at different values (red line in graph is truncated normal, blue is original). You can tune these values and thus tune fidelity/diversity. Recall that having a lot of fidelity is not always the goal—one failure mode of that is that you get one really real image but nothing else (no diversity), and that’s not very interesting or successful from a model that’s supposed to model the realm of all possible human faces or that of all possible coconuts—including that of a cat pouncing after a flying coconut (but with extremely low probability). truncation: The positive truncation value. 1 is low truncation (high diversity) and 0 is all truncation except for the mean (high quality/fidelity). A lower value increases fidelity and decreases diversity, and vice versa. Playing with code interpolation 12345z_dim = Gs.input_shape[1]first_noise = rnd.randn(1, z_dim)second_noise = rnd.randn(1, z_dim)percent_first_noise = np.linspace(0, 1, n_interpolation)[:, None]interpolation_noise = first_noise * percent_first_noise + second_noise * (1 - percent_first_noise) Random vector Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. torch.ones(3, 3, device=device), or move it onto the target device using torch.ones(3, 3).to(device). You do not need to do this if you’re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as torch.ones_like. In general, use torch.ones_like and torch.zeros_like instead of torch.ones or torch.zeros where possible. HW Since the generator is needed when calculating the discriminator’s loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated! Week 1 Assignment: Your First GANGoalIn this notebook, you’re going to create your first generative adversarial network (GAN) for this course! Specifically, you will build and train a GAN that can generate hand-written images of digits (0-9). You will be using PyTorch in this specialization, so if you’re not familiar with this framework, you may find the PyTorch documentation useful. The hints will also often include links to relevant documentation. Learning Objectives Build the generator and discriminator components of a GAN from scratch. Create generator and discriminator loss functions. Train your GAN and visualize the generated images. Getting StartedYou will begin by importing some useful packages and the dataset you will use to build and train your GAN. You are also provided with a visualizer function to help you investigate the images your GAN will create. 12345678910111213141516171819import torchfrom torch import nnfrom tqdm.auto import tqdmfrom torchvision import transformsfrom torchvision.datasets import MNIST # Training datasetfrom torchvision.utils import make_gridfrom torch.utils.data import DataLoaderimport matplotlib.pyplot as plttorch.manual_seed(0) # Set for testing purposes, please do not change!def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)): &#x27;&#x27;&#x27; Function for visualizing images: Given a tensor of images, number of images, and size per image, plots and prints the images in a uniform grid. &#x27;&#x27;&#x27; image_unflat = image_tensor.detach().cpu().view(-1, *size) image_grid = make_grid(image_unflat[:num_images], nrow=5) plt.imshow(image_grid.permute(1, 2, 0).squeeze()) plt.show() MNIST DatasetThe training images your discriminator will be using is from a dataset called MNIST. It contains 60,000 images of handwritten digits, from 0 to 9, like these: You may notice that the images are quite pixelated – this is because they are all only 28 x 28! The small size of its images makes MNIST ideal for simple training. Additionally, these images are also in black-and-white so only one dimension, or “color channel”, is needed to represent them (more on this later in the course). TensorYou will represent the data using tensors. Tensors are a generalization of matrices: for example, a stack of three matrices with the amounts of red, green, and blue at different locations in a 64 x 64 pixel image is a tensor with the shape 3 x 64 x 64. Tensors are easy to manipulate and supported by PyTorch, the machine learning library you will be using. Feel free to explore them more, but you can imagine these as multi-dimensional matrices or vectors! BatchesWhile you could train your model after generating one image, it is extremely inefficient and leads to less stable training. In GANs, and in machine learning in general, you will process multiple images per training step. These are called batches. This means that your generator will generate an entire batch of images and receive the discriminator’s feedback on each before updating the model. The same goes for the discriminator, it will calculate its loss on the entire batch of generated images as well as on the reals before the model is updated. GeneratorThe first step is to build the generator component. You will start by creating a function to make a single layer/block for the generator’s neural network. Each block should include a linear transformation to map to another shape, a batch normalization for stabilization, and finally a non-linear activation function (you use a ReLU here) so the output can be transformed in complex ways. You will learn more about activations and batch normalization later in the course. 1234567891011121314151617181920212223# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: get_generator_blockdef get_generator_block(input_dim, output_dim): &#x27;&#x27;&#x27; Function for returning a block of the generator&#x27;s neural network given input and output dimensions. Parameters: input_dim: the dimension of the input vector, a scalar output_dim: the dimension of the output vector, a scalar Returns: a generator neural network layer, with a linear transformation followed by a batch normalization and then a relu activation &#x27;&#x27;&#x27; return nn.Sequential( # Hint: Replace all of the &quot;None&quot; with the appropriate dimensions. # The documentation may be useful if you&#x27;re less familiar with PyTorch: # https://pytorch.org/docs/stable/nn.html. #### START CODE HERE #### nn.Linear(input_dim, output_dim), nn.BatchNorm1d(output_dim), nn.ReLU(inplace=True), #### END CODE HERE #### ) 1234567891011121314151617181920# Verify the generator block functiondef test_gen_block(in_features, out_features, num_test=1000): block = get_generator_block(in_features, out_features) # Check the three parts assert len(block) == 3 assert type(block[0]) == nn.Linear assert type(block[1]) == nn.BatchNorm1d assert type(block[2]) == nn.ReLU # Check the output shape test_input = torch.randn(num_test, in_features) test_output = block(test_input) assert tuple(test_output.shape) == (num_test, out_features) assert test_output.std() &gt; 0.55 assert test_output.std() &lt; 0.65test_gen_block(25, 12)test_gen_block(15, 28)print(&quot;Success!&quot;) Success! Now you can build the generator class. It will take 3 values: The noise vector dimension The image dimension The initial hidden dimension Using these values, the generator will build a neural network with 5 layers/blocks. Beginning with the noise vector, the generator will apply non-linear transformations via the block function until the tensor is mapped to the size of the image to be outputted (the same size as the real images from MNIST). You will need to fill in the code for final layer since it is different than the others. The final layer does not need a normalization or activation function, but does need to be scaled with a sigmoid function. Finally, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network. Optional hints for Generator The output size of the final linear transformation should be im_dim, but remember you need to scale the outputs between 0 and 1 using the sigmoid function. nn.Linear and nn.Sigmoid will be useful here. 1234567891011121314151617181920212223242526272829303132333435363738394041# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: Generatorclass Generator(nn.Module): &#x27;&#x27;&#x27; Generator Class Values: z_dim: the dimension of the noise vector, a scalar im_dim: the dimension of the images, fitted for the dataset used, a scalar (MNIST images are 28 x 28 = 784 so that is your default) hidden_dim: the inner dimension, a scalar &#x27;&#x27;&#x27; def __init__(self, z_dim=10, im_dim=784, hidden_dim=128): super(Generator, self).__init__() # Build the neural network self.gen = nn.Sequential( get_generator_block(z_dim, hidden_dim), get_generator_block(hidden_dim, hidden_dim * 2), get_generator_block(hidden_dim * 2, hidden_dim * 4), get_generator_block(hidden_dim * 4, hidden_dim * 8), # There is a dropdown with hints if you need them! #### START CODE HERE #### nn.Linear(hidden_dim * 8, im_dim), nn.Sigmoid() #### END CODE HERE #### ) def forward(self, noise): &#x27;&#x27;&#x27; Function for completing a forward pass of the generator: Given a noise tensor, returns generated images. Parameters: noise: a noise tensor with dimensions (n_samples, z_dim) &#x27;&#x27;&#x27; return self.gen(noise) # Needed for grading def get_gen(self): &#x27;&#x27;&#x27; Returns: the sequential model &#x27;&#x27;&#x27; return self.gen 1234567891011121314151617181920# Verify the generator classdef test_generator(z_dim, im_dim, hidden_dim, num_test=10000): gen = Generator(z_dim, im_dim, hidden_dim).get_gen() # Check there are six modules in the sequential part assert len(gen) == 6 test_input = torch.randn(num_test, z_dim) test_output = gen(test_input) # Check that the output shape is correct assert tuple(test_output.shape) == (num_test, im_dim) assert test_output.max() &lt; 1, &quot;Make sure to use a sigmoid&quot; assert test_output.min() &gt; 0, &quot;Make sure to use a sigmoid&quot; assert test_output.min() &lt; 0.5, &quot;Don&#x27;t use a block in your solution&quot; assert test_output.std() &gt; 0.05, &quot;Don&#x27;t use batchnorm here&quot; assert test_output.std() &lt; 0.15, &quot;Don&#x27;t use batchnorm here&quot;test_generator(5, 10, 20)test_generator(20, 8, 24)print(&quot;Success!&quot;) Success! NoiseTo be able to use your generator, you will need to be able to create noise vectors. The noise vector z has the important role of making sure the images generated from the same class don’t all look the same – think of it as a random seed. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, you will generate all the noise vectors at once. Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. torch.ones(3, 3, device=device), or move it onto the target device using torch.ones(3, 3).to(device). You do not need to do this if you’re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as torch.ones_like. In general, use torch.ones_like and torch.zeros_like instead of torch.ones or torch.zeros where possible. Optional hint for get_noise You will probably find torch.randn useful here. 12345678910111213141516# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: get_noisedef get_noise(n_samples, z_dim, device=&#x27;cpu&#x27;): &#x27;&#x27;&#x27; Function for creating noise vectors: Given the dimensions (n_samples, z_dim), creates a tensor of that shape filled with random numbers from the normal distribution. Parameters: n_samples: the number of samples to generate, a scalar z_dim: the dimension of the noise vector, a scalar device: the device type &#x27;&#x27;&#x27; # NOTE: To use this on GPU with device=&#x27;cuda&#x27;, make sure to pass the device # argument to the function you use to generate the noise. #### START CODE HERE #### return torch.randn(n_samples, z_dim, device=device) #### END CODE HERE #### 12345678910111213# Verify the noise vector functiondef test_get_noise(n_samples, z_dim, device=&#x27;cpu&#x27;): noise = get_noise(n_samples, z_dim, device) # Make sure a normal distribution was used assert tuple(noise.shape) == (n_samples, z_dim) assert torch.abs(noise.std() - torch.tensor(1.0)) &lt; 0.01 assert str(noise.device).startswith(device)test_get_noise(1000, 100, &#x27;cpu&#x27;)if torch.cuda.is_available(): test_get_noise(1000, 32, &#x27;cuda&#x27;)print(&quot;Success!&quot;) Success! DiscriminatorThe second component that you need to construct is the discriminator. As with the generator component, you will start by creating a function that builds a neural network block for the discriminator. Note: You use leaky ReLUs to prevent the “dying ReLU” problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient. You will learn more about this in the following lectures! 1234567891011121314151617181920# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: get_discriminator_blockdef get_discriminator_block(input_dim, output_dim): &#x27;&#x27;&#x27; Discriminator Block Function for returning a neural network of the discriminator given input and output dimensions. Parameters: input_dim: the dimension of the input vector, a scalar output_dim: the dimension of the output vector, a scalar Returns: a discriminator neural network layer, with a linear transformation followed by an nn.LeakyReLU activation with negative slope of 0.2 (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html) &#x27;&#x27;&#x27; return nn.Sequential( #### START CODE HERE #### nn.Linear(input_dim, output_dim), nn.LeakyReLU(negative_slope=0.2) #### END CODE HERE #### ) 123456789101112131415161718192021# Verify the discriminator block functiondef test_disc_block(in_features, out_features, num_test=10000): block = get_discriminator_block(in_features, out_features) # Check there are two parts assert len(block) == 2 test_input = torch.randn(num_test, in_features) test_output = block(test_input) # Check that the shape is right assert tuple(test_output.shape) == (num_test, out_features) # Check that the LeakyReLU slope is about 0.2 assert -test_output.min() / test_output.max() &gt; 0.1 assert -test_output.min() / test_output.max() &lt; 0.3 assert test_output.std() &gt; 0.3 assert test_output.std() &lt; 0.5test_disc_block(25, 12)test_disc_block(15, 28)print(&quot;Success!&quot;) Success! Now you can use these blocks to make a discriminator! The discriminator class holds 2 values: The image dimension The hidden dimension The discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator’s neural network you are given a forward pass function that takes in an image tensor to be classified. 123456789101112131415161718192021222324252627282930313233343536373839# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: Discriminatorclass Discriminator(nn.Module): &#x27;&#x27;&#x27; Discriminator Class Values: im_dim: the dimension of the images, fitted for the dataset used, a scalar (MNIST images are 28x28 = 784 so that is your default) hidden_dim: the inner dimension, a scalar &#x27;&#x27;&#x27; def __init__(self, im_dim=784, hidden_dim=128): super(Discriminator, self).__init__() self.disc = nn.Sequential( get_discriminator_block(im_dim, hidden_dim * 4), get_discriminator_block(hidden_dim * 4, hidden_dim * 2), get_discriminator_block(hidden_dim * 2, hidden_dim), # Hint: You want to transform the final output into a single value, # so add one more linear map. #### START CODE HERE #### nn.Linear(hidden_dim, 1) #### END CODE HERE #### ) def forward(self, image): &#x27;&#x27;&#x27; Function for completing a forward pass of the discriminator: Given an image tensor, returns a 1-dimension tensor representing fake/real. Parameters: image: a flattened image tensor with dimension (im_dim) &#x27;&#x27;&#x27; return self.disc(image) # Needed for grading def get_disc(self): &#x27;&#x27;&#x27; Returns: the sequential model &#x27;&#x27;&#x27; return self.disc 12345678910111213141516171819# Verify the discriminator classdef test_discriminator(z_dim, hidden_dim, num_test=100): disc = Discriminator(z_dim, hidden_dim).get_disc() # Check there are three parts assert len(disc) == 4 # Check the linear layer is correct test_input = torch.randn(num_test, z_dim) test_output = disc(test_input) assert tuple(test_output.shape) == (num_test, 1) # Don&#x27;t use a block assert not isinstance(disc[-1], nn.Sequential)test_discriminator(5, 10)test_discriminator(20, 8)print(&quot;Success!&quot;) Success! TrainingNow you can put it all together!First, you will set your parameters: criterion: the loss function n_epochs: the number of times you iterate through the entire dataset when training z_dim: the dimension of the noise vector display_step: how often to display/visualize the images batch_size: the number of images per forward/backward pass lr: the learning rate device: the device type, here using a GPU (which runs CUDA), not CPU Next, you will load the MNIST dataset as tensors using a dataloader. 1234567891011121314151617181920212223# Set your parameters&quot;&quot;&quot;Notes for nn.BCEWithLogitsLoss():This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.&quot;&quot;&quot;criterion = nn.BCEWithLogitsLoss()n_epochs = 200z_dim = 64display_step = 500batch_size = 128lr = 0.00001# Load MNIST dataset as tensorsdataloader = DataLoader( MNIST(&#x27;.&#x27;, download=False, transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True)### DO NOT EDIT ###device = &#x27;cuda&#x27; Now, you can initialize your generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models. 1234gen = Generator(z_dim).to(device)gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)disc = Discriminator().to(device) disc_opt = torch.optim.Adam(disc.parameters(), lr=lr) Before you train your GAN, you will need to create functions to calculate the discriminator’s loss and the generator’s loss. This is how the discriminator and generator will know how they are doing and improve themselves. Since the generator is needed when calculating the discriminator’s loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated! Remember that you have already defined a loss function earlier (criterion) and you are encouraged to use torch.ones_like and torch.zeros_like instead of torch.ones or torch.zeros. If you use torch.ones or torch.zeros, you’ll need to pass device=device to them. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: get_disc_lossdef get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device): &#x27;&#x27;&#x27; Return the loss of the discriminator given inputs. Parameters: gen: the generator model, which returns an image given z-dimensional noise disc: the discriminator model, which returns a single-dimensional prediction of real/fake criterion: the loss function, which should be used to compare the discriminator&#x27;s predictions to the ground truth reality of the images (e.g. fake = 0, real = 1) real: a batch of real images num_images: the number of images the generator should produce, which is also the length of the real images z_dim: the dimension of the noise vector, a scalar device: the device type Returns: disc_loss: a torch scalar loss value for the current batch &#x27;&#x27;&#x27; # These are the steps you will need to complete: # 1) Create noise vectors and generate a batch (num_images) of fake images. # Make sure to pass the device argument to the noise. # 2) Get the discriminator&#x27;s prediction of the fake image # and calculate the loss. Don&#x27;t forget to detach the generator! # (Remember the loss function you set earlier -- criterion. You need a # &#x27;ground truth&#x27; tensor in order to calculate the loss. # For example, a ground truth tensor for a fake image is all zeros.) # 3) Get the discriminator&#x27;s prediction of the real image and calculate the loss. # 4) Calculate the discriminator&#x27;s loss by averaging the real and fake loss # and set it to disc_loss. # Note: Please do not use concatenation in your solution. The tests are being updated to # support this, but for now, average the two losses as described in step (4). # *Important*: You should NOT write your own loss function here - use criterion(pred, true)! #### START CODE HERE #### # step 1 noise = get_noise(num_images, z_dim, device) fake_imgs = gen(noise) # step 2 disc_output_fake = disc(fake_imgs.detach()) ground_truth_fake = torch.zeros_like(disc_output_fake) loss_fake = criterion(disc_output_fake, ground_truth_fake) # step 3 disc_output_real = disc(real) ground_truth_real = torch.ones_like(disc_output_real) loss_real = criterion(disc_output_real, ground_truth_real) # 3tep 4 disc_loss = (loss_fake + loss_real) / 2.0 #### END CODE HERE #### return disc_loss 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475def test_disc_reasonable(num_images=10): # Don&#x27;t use explicit casts to cuda - use the device argument import inspect, re lines = inspect.getsource(get_disc_loss) assert (re.search(r&quot;to\\(.cuda.\\)&quot;, lines)) is None assert (re.search(r&quot;\\.cuda\\(\\)&quot;, lines)) is None z_dim = 64 gen = torch.zeros_like disc = lambda x: x.mean(1)[:, None] criterion = torch.mul # Multiply real = torch.ones(num_images, z_dim) disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, &#x27;cpu&#x27;) assert torch.all(torch.abs(disc_loss.mean() - 0.5) &lt; 1e-5) gen = torch.ones_like criterion = torch.mul # Multiply real = torch.zeros(num_images, z_dim) assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, &#x27;cpu&#x27;)) &lt; 1e-5) gen = lambda x: torch.ones(num_images, 10) disc = lambda x: x.mean(1)[:, None] + 10 criterion = torch.mul # Multiply real = torch.zeros(num_images, 10) assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, &#x27;cpu&#x27;).mean() - 5) &lt; 1e-5) gen = torch.ones_like disc = nn.Linear(64, 1, bias=False) real = torch.ones(num_images, 64) * 0.5 disc.weight.data = torch.ones_like(disc.weight.data) * 0.5 disc_opt = torch.optim.Adam(disc.parameters(), lr=lr) criterion = lambda x, y: torch.sum(x) + torch.sum(y) disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, &#x27;cpu&#x27;).mean() disc_loss.backward() assert torch.isclose(torch.abs(disc.weight.grad.mean() - 11.25), torch.tensor(3.75)) def test_disc_loss(max_tests = 10): z_dim = 64 gen = Generator(z_dim).to(device) gen_opt = torch.optim.Adam(gen.parameters(), lr=lr) disc = Discriminator().to(device) disc_opt = torch.optim.Adam(disc.parameters(), lr=lr) num_steps = 0 for real, _ in dataloader: cur_batch_size = len(real) real = real.view(cur_batch_size, -1).to(device) ### Update discriminator ### # Zero out the gradient before backpropagation disc_opt.zero_grad() # Calculate discriminator loss disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device) assert (disc_loss - 0.68).abs() &lt; 0.05 # Update gradients disc_loss.backward(retain_graph=True) # Check that they detached correctly assert gen.gen[0][0].weight.grad is None # Update optimizer old_weight = disc.disc[0][0].weight.data.clone() disc_opt.step() new_weight = disc.disc[0][0].weight.data # Check that some discriminator weights changed assert not torch.all(torch.eq(old_weight, new_weight)) num_steps += 1 if num_steps &gt;= max_tests: breaktest_disc_reasonable()test_disc_loss()print(&quot;Success!&quot;) Success! 12345678910111213141516171819202122232425262728293031323334353637383940# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: get_gen_lossdef get_gen_loss(gen, disc, criterion, num_images, z_dim, device): &#x27;&#x27;&#x27; Return the loss of the generator given inputs. Parameters: gen: the generator model, which returns an image given z-dimensional noise disc: the discriminator model, which returns a single-dimensional prediction of real/fake criterion: the loss function, which should be used to compare the discriminator&#x27;s predictions to the ground truth reality of the images (e.g. fake = 0, real = 1) num_images: the number of images the generator should produce, which is also the length of the real images z_dim: the dimension of the noise vector, a scalar device: the device type Returns: gen_loss: a torch scalar loss value for the current batch &#x27;&#x27;&#x27; # These are the steps you will need to complete: # 1) Create noise vectors and generate a batch of fake images. # Remember to pass the device argument to the get_noise function. # 2) Get the discriminator&#x27;s prediction of the fake image. # 3) Calculate the generator&#x27;s loss. Remember the generator wants # the discriminator to think that its fake images are real # *Important*: You should NOT write your own loss function here - use criterion(pred, true)! #### START CODE HERE #### # step 1 noise = get_noise(num_images, z_dim, device) gen_output_fake = gen(noise) # step 2 disc_output_fake = disc(gen_output_fake) # step 3 ground_truth_fake = torch.ones_like(disc_output_fake) gen_loss = criterion(disc_output_fake, ground_truth_fake) #### END CODE HERE #### return gen_loss 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def test_gen_reasonable(num_images=10): # Don&#x27;t use explicit casts to cuda - use the device argument import inspect, re lines = inspect.getsource(get_gen_loss) assert (re.search(r&quot;to\\(.cuda.\\)&quot;, lines)) is None assert (re.search(r&quot;\\.cuda\\(\\)&quot;, lines)) is None z_dim = 64 gen = torch.zeros_like disc = nn.Identity() criterion = torch.mul # Multiply gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, &#x27;cpu&#x27;) assert torch.all(torch.abs(gen_loss_tensor) &lt; 1e-5) #Verify shape. Related to gen_noise parametrization assert tuple(gen_loss_tensor.shape) == (num_images, z_dim) gen = torch.ones_like disc = nn.Identity() criterion = torch.mul # Multiply real = torch.zeros(num_images, 1) gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, &#x27;cpu&#x27;) assert torch.all(torch.abs(gen_loss_tensor - 1) &lt; 1e-5) #Verify shape. Related to gen_noise parametrization assert tuple(gen_loss_tensor.shape) == (num_images, z_dim) def test_gen_loss(num_images): z_dim = 64 gen = Generator(z_dim).to(device) gen_opt = torch.optim.Adam(gen.parameters(), lr=lr) disc = Discriminator().to(device) disc_opt = torch.optim.Adam(disc.parameters(), lr=lr) gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device) # Check that the loss is reasonable assert (gen_loss - 0.7).abs() &lt; 0.1 gen_loss.backward() old_weight = gen.gen[0][0].weight.clone() gen_opt.step() new_weight = gen.gen[0][0].weight assert not torch.all(torch.eq(old_weight, new_weight))test_gen_reasonable(10)test_gen_loss(18)print(&quot;Success!&quot;) Success! Finally, you can put everything together! For each epoch, you will process the entire dataset in batches. For every batch, you will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. It’s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It’s important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments. After you’ve submitted a working version with the original architecture, feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. For example, consider changing the size of the hidden dimension, or making the networks shallower or deeper by changing the number of layers. But remember, don’t expect anything spectacular: this is only the first lesson. The results will get better with later lessons as you learn methods to help keep your generator and discriminator at similar levels. You should roughly expect to see this progression. On a GPU, this should take about 15 seconds per 500 steps, on average, while on CPU it will take roughly 1.5 minutes: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)# GRADED FUNCTION: cur_step = 0mean_generator_loss = 0mean_discriminator_loss = 0test_generator = True # Whether the generator should be testedgen_loss = Falseerror = Falsefor epoch in range(n_epochs): # Dataloader returns the batches for real, _ in tqdm(dataloader): cur_batch_size = len(real) # Flatten the batch of real images from the dataset real = real.view(cur_batch_size, -1).to(device) ### Update discriminator ### # Zero out the gradients before backpropagation disc_opt.zero_grad() # Calculate discriminator loss disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device) # Update gradients disc_loss.backward(retain_graph=True) # Update optimizer disc_opt.step() # For testing purposes, to keep track of the generator weights if test_generator: old_generator_weights = gen.gen[0][0].weight.detach().clone() ### Update generator ### # Hint: This code will look a lot like the discriminator updates! # These are the steps you will need to complete: # 1) Zero out the gradients. # 2) Calculate the generator loss, assigning it to gen_loss. # 3) Backprop through the generator: update the gradients and optimizer. #### START CODE HERE #### # step 1 gen_opt.zero_grad() # step 2 gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device) # step 3 gen_loss.backward(retain_graph=True) gen_opt.step() #### END CODE HERE #### # For testing purposes, to check that your code changes the generator weights if test_generator: try: assert lr &gt; 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() &lt; 0.0005 and epoch == 0) assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights) except: error = True print(&quot;Runtime tests have failed&quot;) # Keep track of the average discriminator loss mean_discriminator_loss += disc_loss.item() / display_step # Keep track of the average generator loss mean_generator_loss += gen_loss.item() / display_step ### Visualization code ### if cur_step % display_step == 0 and cur_step &gt; 0: print(f&quot;Epoch &#123;epoch&#125;, step &#123;cur_step&#125;: Generator loss: &#123;mean_generator_loss&#125;, discriminator loss: &#123;mean_discriminator_loss&#125;&quot;) fake_noise = get_noise(cur_batch_size, z_dim, device=device) fake = gen(fake_noise) show_tensor_images(fake) show_tensor_images(real) mean_generator_loss = 0 mean_discriminator_loss = 0 cur_step += 1 HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;))) HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;))) Epoch 1, step 500: Generator loss: 1.537445880770684, discriminator loss: 0.4010176688432697 Notes that there are only two epoch’s results. HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;))) Epoch 199, step 93500: Generator loss: 1.095844237446786, discriminator loss: 0.5419520480632782","categories":[{"name":"Class notes","slug":"Class-notes","permalink":"http://example.com/categories/Class-notes/"}],"tags":[{"name":"Notes","slug":"Notes","permalink":"http://example.com/tags/Notes/"},{"name":"Online course","slug":"Online-course","permalink":"http://example.com/tags/Online-course/"},{"name":"GANs","slug":"GANs","permalink":"http://example.com/tags/GANs/"}]},{"title":"A Survey for Vision Transformers (By Mar. 2021)","slug":"Survey-vision-transformer","date":"2021-04-25T05:02:11.000Z","updated":"2021-04-25T05:54:50.539Z","comments":true,"path":"2021/04/25/Survey-vision-transformer/","link":"","permalink":"http://example.com/2021/04/25/Survey-vision-transformer/","excerpt":"ABSTRACT: Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback.","text":"ABSTRACT: Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback. KEYWORDS: Self-attention, transformer, convolution neural networks, deep neural networks. 1. IntroductionConvolutional neural networks (CNN) has dominated computer vision field since the impressive success of a ImageNet Large Scale Visual Recognition Challenge in 2014 (Russakovsky et al., 2015). Various CNN architectures (He et al., 2015; Szegedy et al., 2016; G. Huang et al., 2016; Hu et al., 2017) springs up like mushrooms since that time. These carefully designed CNN architectures have created significant progress in various visual fields, like image classification (Howard et al., 2017; Szegedy et al., 2014; X. Wang et al., 2018), object detection (Redmon and Farhadi, 2018; Ren et al., 2016), semantic segmentation (Z. Huang et al., 2019; P. Wang et al., 2018; S. Zheng et al., 2020), image generation(X. Chen et al., 2016; Goodfellow, 2016), etc. Most of CNN architectures (He et al., 2015; Simonyan and Zisserman, 2014) heavily benefit from different convolution operations like 3×3 convolutions. These convolution operations can learn local information from visual inputs. Deep stacked convolutions then help to gather high level information (Goodfellow et al., 2016), which help to do different image understanding tasks. In about less than a decade, CNN has become the main design paradigm in many modern deep learning systems. Despite the great success of CNN, there are some limitations. First, while convolution operations can process a local neighbor in space and time, convolution is hard to capture long-distance dependencies. These dependencies are vital to semantic concepts. For example, a bird break in different angles have to be treated with different convolution operations due to the changes of break position. Generally, we can use different convolution operations, increase kernel sizes, increase model depth, or dilated convolutions to solve this problem. But the computation cost can be high and require careful design. Second, convolution treats all information as equal, but in fact not all pixels should be regarded as equal. For example, when we classify dogs and cats, we pay more attention on their ears and noses and ignore some useless background information. That is to say, image classification should give priority to important foreground objects instead of background. But convolution operations process all local neighborhood information with equal weights and do not have a concept of importance. This also causes inefficiency in computation. At the same time, transformer shows its power in natural language processing (NLP) tasks and almost become the standard architecture in NLP tasks. One of important reason is that transformer is good at modeling long distance information. In 2017, Vaswani et at. (Vaswani et al., 2017) first replaced convolutions and recurrence with transformer which only based on attention modules. The architecture allows to train model in a more parallelizable way and significantly decrease the training time. In 2019, Devlin et al. (Devlin et al., 2019) proposed a new language representation model called BERT, which aims to train a deep learning model from unlabeled text by jointly conditioning on all context in all layers. One of the most impressive things is BERT model can be easily transferred to other tasks by just fine tuning one additional output layer. In 2020, Brown et al. (Brown et al., 2020) proposed an autoregressive language model GPT-3 which achieves impressive performance on various NLP tasks, like machine translation and question-answering system. Inspired by the significant improvement of transformer in natural language processing (NLP) tasks, a growing number of researchers are trying to take advantage of transformer in computer vision (Carion et al., 2020; Dosovitskiy et al., 2020; W. Wang et al., 2021; Srinivas et al., 2021). Chen et al. (M. Chen et al., 2020) firstly tried to explore the application of transformer in images. They trained a sequence transformer model to predict pixels and found that the model can learn strong image representations. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed a fully-transformer model called Vision Transformer (ViT) that can be directly used in image classification. Vit treats each image as a set of fixed-size patches (14×14 or 16×16) that will be feed into a transformer encoder to make classification. Though ViT achieves excellent results compared to stat-of-the-art CNN models, it heavily relies on a large-size dataset like JFT-300M (C. Sun et al., 2017) (300 million images) and have relative worse performance on a medium size dataset like ImageNet (Deng et al., 2009). To tackle this problem and make models train effectively on a medium size dataset, Touvron et al. (Touvron et al., 2021) also proposed an efficient convolution-free transformer model called DeiT. DeiT can be trained on a single 8-GPU node in about three days by leveraging a new distillation token and a specific teacher-student strategy. In the early of 2021, various explorations in vision transformer has been proposed, including Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), Transformer in Transformer (TiT) (Han et al., 2021), and Tokens-to-token ViT (Yuan et al., 2021). Even though the application of transformer has dominated a variety of NLP tasks since 2017, the application of transformer in CV is just rapidly developed especially since the late 2020. There is still no standard architecture in vision transformer and different models have their own advantages and disadvantages. Almost in every month, there are new proposed architectures about vision transformer. Thus, this paper tries to do a comprehensive survey about the recent vision transformer and compare different architectures in different applications. Specifically, this paper tries to answer four questions in vision transformer: 1) what is transformer? 2) why we need transformer in CV? 3) how transformer is applied in CV? 4) what are existing difficulties and potential development? The structure of the paper is as follows. In section 2, foundations and preliminary knowledge about transformer will be introduced. Some close related ideas like conventional CNN architectures, channel attention, self-attention, multi-head self-attention, and non-local network will be discussed. This section tries to discuss the evolution of the design of vision models from convolution to transformer. The first aforementioned question will be discussed in this section. In section 3, the core topic transformer will be discussed in detailed. The structure of transformer and different applications of transformer will be introduced, including image classification, object detection, segmentation, etc. This section will show the power of transformer in vision fields, which will also clarify the second and third question in depth. In section 4, vision transformer will be classified into three categories, including transformer with global attention, transformer plus CNN, and transformer with efficient attention. Section 3 and section 4 will help to answer the third aforementioned question. In section 5, existing problems and difficulties in vision transformer will be discussed. Some potential and valuable future work will also be discussed in this section. This section will answer the fourth aforementioned question. In the last section, I will conclude the development of vision transformer and summary the paper. 2. Related workVisio transformer closely relates to convolution operation and self-attention. Thus, in this section I will firstly go through the milestone vision architectures since 2012 to explain how researchers designed models and improved performance. Then, I will give an introduction about self-attention, which is a core module in transformer. I will also introduce non-local network and two Fourier-based networks to explain there are other ways to capture long-range dependencies instead of self-attention. 2.1 CNNThere are a lot of CNN model taking advantage from a deeper neural network. Going deeper for a neural network is always the first choice for complex tasks. In 1989, LeNet (LeCun et al., 1989) , one of the earliest CNN, was proposed by LeCun et al. It only has 7 layers, including 3 convolutional layers, 2 pooling layers, and 1 fully-connected layer. It defines the fundamental modules for CNN. Trained with backpropagation algorithms, it can be successfully identified handwritten zip code numbers. In 2012, AlexNet (Krizhevsky et al., 2012) increased the model depth to 8 layers. It achieved a great improvement (a top-5 error of 15.3%, more than 10.8 percentage points than second one) in ImageNet (Deng et al., 2009). AlexNet shows the power of increase model depth and makes it feasible when use GPU during training. After AlexNet, there are countless number of researchers trying to use Deep CNN to improve performance in their fields. This also greatly improve the development of deep learning in all fields. In 2015, VGG (Simonyan and Zisserman, 2014) achieved the state-of-the-art performance on ILSVRC classiﬁcation and localisation tasks. It heavily uses 3×3 convolutions and increases the depth by adding more convolutional layers. Considering the significance of the depth, Residual Network (ResNet) (He et al., 2015) tried to explore how to increase the depth and avoid vanishing/exploding gradients (Bengio et al., 1994; Glorot and Bengio, 2010). This is because He et al. found that directly stacked more layers cannot have a better performance. They proposed a residual block and let these layers fit a residual mapping. They increased the depth from 18 layers to 152 layers and increase performance, which is the first time to surpass 100-layer barrier. ResNet has also been a popular backbone in various vision tasks since then. In 2016, Densely Connected Convolutional Networks (DenseNet) not only creates short paths from early layers to later layers, it also connects all layers directly with each other. So, it uses direct connections between any two layers with the same feature-map size to implement this idea. This method also helps to solve the gradient vanishing problem. The maximum depth of DenseNet reached 264. The architecture of DenseNet is shown in Figure 1. Figure 1. DenseNet (G. Huang et al., 2016) It is needed to note that convolution operations have dominated various visual recognition tasks with a deep neural network since 2012. There are several reasons behind its popular. First, convolution layers can capture favorable visual context using filters. These filters shared weights over the entire image space, so convolution operations have translation equivariance. Second, convolution operation can take advantage of the rapid development of computational resource like GPU because it is inherent parallelable. And GPU allows to train a very deep CNN with fewer time, which helps to train on large-scale image dataset, like ImageNet. Third, multiple kernel paths help to achieve a competitive performance. GoogleNet (Szegedy et al., 2014) carefully designs a Inception module with 1×1 convolution, 3×3 convolution, and 5×5 convolution to increasing the depth and width of the network while keeping the computational budget constant. Fourth, CNN can use hierarchical structure to learn high-level information. This is because spatial and channel-wise information can be fused in a local receptive filed in convolutional layers. The fused information can then be passed to non-linear activation functions or/and down-sampling layers to learn a better representation that is vital to capture global theoretical receptive ﬁelds. 2.2 Attention in CNNConsidering the importance of channel relationship, SENet (Hu et al., 2017) uses a Squeeze-and-Excitation (SE) block to explicitly model the interdependencies between different channels. The architecture can learn different weights for different channels in SE block. It is an effective and lightweight gating mechanism to self-recalibrate the feature map. There are three important operations. First, in the squeeze operation, SE block will shrink features through spatial dimensions. Second, in the excitation operation, it will learn weights to explicitly model channel association. Third, it reweights the feature maps by the learned weights. The SE block is shown in Figure 2. Figure 2. A Squeeze-and-Excitation block. (Hu et al., 2017) Inspired the SENet’s channel recalibration mechanism and multi-branch convolutional networks (He et al., 2015; Srivastava et al., 2015), SKNet uses a non-linear approach to aggregate multiple scale feature and constructs a dynamic selection mechanism in CNN instead of fixed receptive fields in standard CNN. It has three basic operators – Split, Fuse, and Select, which are shown in Figure 3. Split operator means that they use two filters with different filter sizes. Fuse operator aims to enable neurons to adaptively adjust receptive fields based on the stimulus content. Select operator is a soft attention across channels, which is similar to SENet. This architecture improves the efﬁciency and effectiveness of object recognition. Figure 3. Selective Kernel Convolution. (X. Li et al., 2019) Some researchers tried to not only use channel attention, but also take into account spatial attention as well. For example, Bottleneck Attention Module (BAM) (Park et al., 2018) and Convolutional Block Attention Module (CBAM) (Woo et al., 2018) were proposed to infer an attention map along two separate dimensions, channel and spatial. But BAM’s attention computes in parallel, while CBAM computes attention sequentially. Other researchers only used spatial attention and did not consider channel attention. For example, Non-local Network (NLNet) (X. Wang et al., 2018) tries to capture long-range dependencies by avoiding messages delivered back and forth between distant positions. They proposed non-local operations to have spatial attention. This operation can use a weighted sum of the features to compute the response at a position. They applied NLNet in video classification and achieved competitive performance. The non-local block is shown in Figure 4. Figure 4. Non-local block. (X. Wang et al., 2018) Although NLNet presents a way to capture long-range information, global contexts are almost the same for different query positions (Cao et al., 2019). So, construct a global context network (GCNet) is proposed to solve this problem by unify the basic idea of SNNet and NLNet, which means it takes into account of spatial and channel information at the same time. To summary the application of attention in CNN, the comparison of NLNet, SENet, SKNet, and GCNet, etc. are shown in Table 1. Table 1. Model comparison for the application of attention in CNN. Model name Characteristics SENet Spatial aggregation, channel attention SKNet Adaptive receptive filed, channel attention BAM Channel attention and spatial attention in parallel CBAM Channel attention and spatial attention sequentially NLNet Spatial weighted sum per pixel GCNet Spatial weighted sum, channel attention 2.3 Self-attentionSelf-attention (Vaswani et al., 2017) can transform an input to a better representation by allowing inputs to interact with each other. It will compute three hidden representations called query, key, value firstly by three learnable matrixes. After that, it will compute attention weights by the dot products of query and key. The weights will be divided by the sqrt root of the key’s dimension and obtain the weighted value as output. So, the output of self-attention operation is aggregates of the interactions between all inputs. Figure 5 shows how self-attention works. Figure 5. Self-attention operation. The calculation of self-attention can be formulated as a single function: (1) The first important thing to understand the mechanism of self-attention is remembering the dimension of the input and output of self-attention is the same. That is to say, our goal is to have a new and better representation for our inputs, and the output needs to be the same dimension. So, what is a good representation for the input? When we are doing machine translation, we understand that even for a same word it can have different or even opposite meaning. What make the word has different meaning? The answer is context. So, we want to consider the whole context and capture the different relationships between different words. Self-attention is designed to do a similar thing. The dot product of and aims to calculate the similarity/distance between two different vectors. The score of can be used to determine how much weights need to be put in a certain input. But before we calculate a weighted value as the output, we need to do some normalization, which is shown in Equation 1. It is also need to note that the difference of the encoder-decoder attention layer (Luong et al., 2015) between self-attention layer is where the key matrix ,value matrix and query matrix come from. For the encoder-decoder attention layer, and come from the encoder module and comes from the previous layer. Other operations are the same. The limitation for self-attention layer is that it cannot give different weights for different parts of the input. One way to solve that is to use multi-head attention instead of one single-head attention. The structure of multi-head attention is shown in Figure 6. It helps to boost the performance of the vanilla self-attention layer by giving attention layers different representation subspace. Different query, key, and value matrix are used for different heads So, they can learn different representation in training. For example, several heads are good at learning local information. Others are good at learning global information. Specifically, the operation of multi-head attention can be formulated as below: (2) where , , , and are the concatenation of . Figure 6. Multi-head attention. (Vaswani et al., 2017) 2.4 Fourier-based networkConvolution and attention are not the only choice to improve model performance. Fourier analysis tells us that any signal can be represented as a linear combination of sin and cos functions with different coefficients regardless of signal’s dimension. Some researcher recently tried to integrate Short Time Fourier Transformation (STFT) in to vision architectures. For example, Fast Fourier Convolution (FFC) (Lu Chi, 2020) adds a global branch that manipulates image-level spectrum and a semi-global branch to process stacked image patches. This design help to capture non-local receptive fields and cross-scale fusion. Another network is call Fourier Neural Operator (FNO) (Z. Li et al., 2020). Although FNO is designed to solve partial differential equations in engineering and science, it is a pure neural network and does not have close relationship with some numerical methods, like Finite Element Analysis or Finite Difference Method. The basic idea is that they build a Fourier Layer/ Block and stacked many Fourier Layers to get a better performance. Specifically, it has two branches. The first branch uses STFT to change hidden representations in spatial domain to frequency domain. Then high frequency modes are removed, which means only low frequency modes remains. The coefficients for each low frequency are learned by the Neural Network. And the frequency information is inversed to the original spatial domain by inverse STFT. The second is 1×1 convolution operation which is designed to let the model learn the residual and help to alleviate the gradient vanishing problem. Figure 7 show the structure of Fourier Layer. Figure 7. Fourier Layer. (Z. Li et al., 2020) There are several reasons for researchers to use Fourier Analysis in Neural Network. First, the computation is very efficient in a quasi-linear level due to the use of STFT. Second, it helps to capture coarse and global in a more efficient way by using learnable coefficients for different frequencies. Thus, in some applications, we may not need to use stacked convolutions with different size. We can use Fourier Layer to learn a good representation in frequency domain. 3. Topic and its applicationsIn this section, I will firstly introduce the details of transformer and then provide a comprehensive review of its applications in different visual tasks, including image classification, object detection, segmentation, etc. 3.1 TransformerIn the previous section, I have introduced the core module in transformer, single-head and multi-head attention layer. I will go through other modules and introduce the whole structure of transformer, which is shown in Figure 8. Figure 8. The architecture for transformer. (Vaswani et al., 2017) The first thing is the positional encoding. Self-attention operation is irrelevant to the position of each input. So, it cannot capture the positional information of inputs. To solve this potential problem, they proposed a positional encoding that is added to the input embedding. The second thing is the Feed-forward neural network (FFNN). A FFNN is used after a self-attention layer in each encoder and decoder. This FFNN has two linear transformation layers and a ReLU activation function (Nair and Hinton, 2010). It can be expressed as: , (3) where and are weights for the two linear transformation layers, is ReLU activation function. The third thing is Add &amp; Norm module. To strength the information flow in a deep neural network, a residual connection is added in each module, including FFNN and self-attention. This is a similar design in ResNet (He et al., 2015). To boost the training, a layer-normalization (Cheng et al., 2016) is followed by the residual connection. The last thing is the output layer. After passing through decoders, the output will be passed to a linear layer and a softmax layer, which help to transform logits vector into word probabilities. To summarize, Transformer, an encoder-decoder architecture, is designed to handle sequential data. 3.2 ApplicationsSince 2017, Transformer has achieved great success in various NLP tasks. For example, after pretraining, BERT (Devlin et al., 2019) can be easily used in a wide range of downstream tasks by fine-tuning one newly added output layer. Generative Pre-Trained Transformer models (GPT (Alec Radford et al., 2018), GPT-2 (A. Radford et al., 2019), and GPT-3 (Brown et al., 2020)) are another popular pretrained models in NLP. GPT-3 is designed to directly do multiple tasks without fine-tuning, but it has 175 billion parameters and requires 45 TB compressed plaintext data to train. Inspired by the success of transformer in NLP, there are growing number of vision transformers being proposed (Beal et al., 2020; Dosovitskiy et al., 2020; Yang et al., 2020; Zhang et al., 2020; S. Zheng et al., 2020). Researchers also applied vision transformer in different fields. 3.2.1 Image classificationViT (Dosovitskiy et al., 2020) is a pure transformer applied directly to image patches. The architecture is shown in Figure 9. They reshape an image as a sequence of flattened 2D patches , where H = 14 or 16. Then, a 1D positional embedding added with patch embedding are fed into transformer encoder. The feature obtained from the first position is regarded as the global image representation and is used to do classification. ViT only uses the encoder of transformer and only has minimal changes compared to a standard transformer in NLP. Figure 9. Vision Transformer (ViT). (Dosovitskiy et al., 2020) ViT achieved excellent results when pre-trained on a large dataset JFT-300M (300 million images), but it does not generalize well in mid-sized datasets like ImageNet (1.2 million images). One of the reasons is that transformer lack inductive biases inherent to CNN, such as translation equivariance and locality. So, vision transformer needs sufficient data to have a good generalization, which is similar in NLP transformer. Moreover, they compared ViT with a hybrid model that feeds intermediate feature maps into ViT by leveraging CNN model. They found that hybrid model only has better performance than ViT at small computational budgets and the difference vanishes given sufficient data. This shows that CNN may not be a necessary part for a large model. Furthermore, they analyzed the relationship between network depth and mean attention distance, which is shown in Figure 10. They found that some heads can attend at global information even at the lowest layer. Figure 10. Attention distance in ViT. (Dosovitskiy et al., 2020) ViT needs to be trained on millions of images, which severely hinders its application in different fields. In the early of 2021, DeiT (Touvron et al., 2021) uses a teacher-student strategy and add a distillation token. It is a convolution-free architecture and similar to ViT, but it uses CNN as a teacher and transformer as a student. It is much more efficient and only needs to be trained about 3 days on a single 8-GPU node. To facilitate the training, it uses data augmentation and regularization strategies. These helps to greatly reduces the requirement of data and make it feasible to train on ImageNet instead of JFT-300M. They achieved better performance than ViT and EfficientNet. Another vision transformer called Tokens-to-Token ViT (T2T) (Yuan et al., 2021) was proposed in 2021. The architecture of T2T is shown in Figure 11 (a). T2T wants to train model in a more efficient way and avoid the reliance on a large-size dataset. They pointed out two problems in ViT. First, splitting each image into a sequence of tokens fails to capture key local structures like edges or lines. Second, the backbone of ViT is not suitable for vision tasks due to the redundancy in the attention module. From Figure 11 (b) , we can find that T2T can capture local structure features (green box), but ViT capture invalid feature maps (red box). Thus, they proposed a progressive tokenization module to replace the tokenization method in ViT. Their method helps to capture local structure information from neighbor tokens and achieve length reduction of tokens. They also compared different backbone from existing CNN architectures and found that “deep-narrow” architecture design with fewer channels but more layers in ViT is the best one. Figure 11. T2T architecture and feature visualization. (Yuan et al., 2021) Viewing images as a sequence of patches ignore the structure information between patches. To solve this problem, Transformer in Transformer (TNT), a pure transformer network, handles images in patch-level and pixel level. The architecture of TNT is shown in Figure 12. TNT uses two transformer blocks. The first one is an outer transformer block which is used to process patch embedding. The other one is an inner transformer block which is designed to caoture local features from pixel level. In a similar computational cost, TNT’s performance is 1.5% higher than that of DeiT on ImageNet. They also compared visualization of learned features between DeiT and TNT. Compared to DeiT, TNT can preserve better local information and have more diverse features. Figure 12. TNT block and TNT framework. (Han et al., 2021) Bottleneck Transformers Network (BoTNet) is also a backbone architecture. It only modifies the final three bottleneck blocks of ResNet with global self-attention, which is shown in Figure 13. It has two main stages. In the first stage, they use convolution operations to learn high-level feature maps. In the second stage, they use global self-attention to aggregate information. Based on such design, it is easy to handle large images (1024×1024) in an efficient way. They pointed out that the designed block can be regarded as transformer block. BoTNet is quite similar to NLNet. The first difference is that BoTNet uses multi-head self-attention but NLNet does not always use it in the implementation. The second difference is that non-local block is used as an additional block, while bottleneck block is used to replace convolution block. Figure 13. A comparison of ResNet bottleneck and bottleneck Transformer. (Srinivas et al., 2021) 3.2.2 Object detectionCompared to image classification, object detection requires models to predict object labels and their corresponding bounding boxes. Detection Transformer (DETR) is the first end-to-end transformer architecture used in object detection. It removes the many traditional hand-designed components like region proposal network (RPN) and non-maximal suppression (NMS). It also can predict all objects at once, which is different to RNN that predicts sequence elements one by one. DETR uses a set-based global loss which performs bipartite matching between predicted and ground-truth objects to train the model. It does not require any customized layers, which makes it easily reproduced in any framework. It achieves comparable results to Faster R-CNN (Ren et al., 2016). But it has two main limitations. First, it takes a long time to converge. This is because the attention module has uniform attention at the start of training and needs more epochs to update to optimal attention weights. Second, it easily fails to detect small objects due to limited feature spatial resolution. Figure 14. The architecture for DETR. (Carion et al., 2020) To solve the drawbacks of DETR, Deformable DETR uses more efficient attention module that only attend on some key sampling points around the reference point. In this way, it greatly reduces the computational cost. Also, the deformable attention module cam fuse multi-scale features. It achieves better performance with 10 times less training epochs compared to the original DETR model. Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), a recent pure transformer backbone, also aims to capture multi-scale features maps for dense predictions. Compared to ViT, PVT is trained on dense partition of images (the patch size is 4) to get high resolution outputs. To prevent the high computational costs, PVT uses a progressive shrinking pyramid and adopts a spatial -reduction attention layer. They applied PVT in image classification, object detection, and sematic segmentation and achieved a competitive performance. PVT also can be easily combined with DETR. The combination model is called PVT+DETR, which is an entirely convolution-free network. It has better performance than DETR plus ResNet50. Adaptive Clustering Transformer (ACT) (M. Zheng et al., 2020) is also designed to solve DETR’s high computational cost in training and testing. ACT cluster the query features using Locality Sensitive Hashing (LSH) and use the prototype-key interaction to approximate the query-key relationship. ACT greatly reduce the computation cost while just reduce little accuracy. Sun et al. (Z. Sun et al., 2020) pointed out that the slow convergence of the original DETR model comes from the Hungarian loss and the Transformer cross-attention. They proposed two possible solutions: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). These two models achieve better performance than the original DETR model. 3.2.3 SegmentationSegmentation requires model to have pixel-level understanding, which is different from image classification and object detection. Video Instance Segmentation with Transformers (VisTR) (Y. Wang et al., 2020) is an end-to-end parallel sequence prediction network. The pipeline of VisTR is shown in Figure 15. VisTR takes a sequence of images from a video as input and make instance segmentation prediction for each image directly. VisTR is designed to learn pixel-level and instance-level similarity. It can track instances seamlessly and naturally. A CNN model is used to extract feature maps from a sequence of images. After adding with positional encoding, extracted feature maps will be fed into encoder and decoder. Finally, VisTR will output the final mask sequences by an instance sequence segmentation. Figure 15. The architecture for VisTR. (Y. Wang et al., 2020) Point Transformer (Zhao et al., 2020) is a backbone architecture for 3D point cloud understanding tasks, including semantic scene segmentation, object part segmentation, and object classiﬁcation. They designed a Point Transformer layer which is invariant to permutation and cardinality. It outperforms the state-of-the-art model in S3DIS dataset for large -scale semantic scene segmentation. Axial-DeepLab (H. Wang, Zhu, Green, et al., 2020) is proposed to convert 2D self-attention as two 1D axial-attention layers which are applied to height and width sequentially. This design greatly reduces the computation and allow to capture non-local interactions easily. There is also a model called Attention-Based Transformers that is designed to instance segmentation of cells in microstructures. It uses long skip connections between the CNN encoder and CNN decoder. It achieves fast and accurate cell instance segmentation. 4. Classification and comparison of various methodsTransformer structure uses self-attention module to long-range information, while convolutions use hierarchical structure to learn high-level features. Generally, researchers try to explore architectures in these two different ways and take advantage from each other. Another important category is efficient transformer because transformer normally requires more data than standard CNN. Task Category Method Characteristics Image classification Transformer + global attention ViT (Dosovitskiy et al., 2020) Only transformer encoder, patching embedding, and positional encoding; High computational cost, heavily rely on large-size dataset Image classification Transformer + CNN DeiT (Touvron et al., 2021) Teacher-student strategy, distillation token, data augmentation and regularization strategies; Rely on the CNN model to be a teacher Image classification Transformer + global attention T2T-ViT (Yuan et al., 2021) Progressively structurize images to tokens, deep-narrow structure, train from scratch on ImageNet Image classification Transformer + global attention TNT (Han et al., 2021) Operate on patch-level and pixel-level, more diverse feature extraction Object detection Transformer + CNN DETR (Carion et al., 2020) End-to-end learning, set-based prediction, remove hand-design modules; Struggle on small objects, slow to converge Object detection Transformer + CNN + efficient attention Deformable DETR (Zhu et al., 2020) Better performance on small objects, faster convergence than DETR; Two stage detector architecture, use data augmentation in test time Segmentation Transformer + CNN MaX-DeepLab (H. Wang, Zhu, Adam, et al., 2020) ﬁrst end-to-end model for panoptic segmentation, mask transformer, bipartite matching, dual-path architecture Segmentation Transformer + CNN VisTR (Y. Wang et al., 2020) instance sequence matching and segmentation, direct end-to-end parallel sequence prediction Image processing Transformer + CNN Image processing transformer (IPT) (H. Chen et al., 2020) The multi-head and multi-trail structure, generate a large amount of corrupted image pairs images by add noising, rain streak, or down-sampling; can be used in different tasks in one model after ﬁne-tuning Image colorization Transformer + global attention Colorization Transformer (ColTran) (Kumar et al., 2020) Conditional transformer layers, two parallel networks to upsample low resolution images Segmentation Transformer + efficient attention Criss-Cross Network (CCNet) Criss-Cross Attention block, high efficiency and low computation, The state-of-the-art performance, NLP, architecture exploration Transformer + efficient attention ConvBERT (Jiang et al., 2020) Span-based dynamic convolution, mixed attention block to capture both global and local dependencies 5. Future workTransformer is still a young field and has different architectures proposed in almost every week. There are several important problems or difficulties that are worth to be noticed. The first one is to discover the mechanism of transformer. Since Google proposed transformer architecture in 2017, countless papers have been using this idea in different fields, from NLP to CV or to medical researches. But recently, Google also published a paper which points out that “Attention is not all you need: pure attention loses rank doubly exponentially with depth”. They decompose the outputs of self-attention layer into a sum of smaller terms and found that skip connections or multi-layer perceptron are vital to the final performance. We need to understand why and how transformer works. Only when we have a better understanding about that, we can know the direction of improvement and development. The second one is to build efficient vision transformer. There are several ways to accomplish that. For example, we can take advantages from convolution operations which has inductive biases, including translation invariance, weight sharing, scale invariance. Or we can build effective attention block to capture different scale information in a better way. Or we can discover the efficient to use training strategies (data augmentation or regularization) to avoid the reliance on large-size dataset. This problem is the core research direction for vision transformer for a long time since in practical we generally cannot find such large dataset. The third one is to reduce model complexity. Small and efficient model can be easily extended in different fields. Distillation maybe a choice to make transformer models more feasible in practice. Some useful techniques such as selective or sparse attention, low-rank matrix in attention, and efficient cluster attention may give some insights to solve this problem. The fourth one is to explore more backbone architecture for visual inputs instead of directly using transformer structure in NLP tasks. The last one is to discover more models that can do multiple tasks in one model. 6. ConclusionIn this survey, I give a comprehensive review about vision transformer. I introduce the development of the depth exploration in CNN and go through the researches in channel and spatial attention. I also introduce the all related modules in transformer structure, including self-attention, MLP, FFNN, and layer normalization. Then the application of vision transformer and their classification and comparison are discussed. I also list some potential and useful future research directions at the end. 7. Course feedbacksThis course is a great graduate-level class. There are several reasons for that: \\1. Rich course content. From PCA to Bayesian, all important topics are covered in this 9-week course. It is amazing and really give me a good overview in this field. \\2. Excellent lectures. Even though I toke class by watching video, I can feel that professor try to explain concept in detail rather quickly go through various concepts and equations. For some hard concepts or math-heavy part, professor is always willing to draw some lines or circles in patient, which really help me to understand them. \\3. Novel and creative illustration. Even though I think I have learned PCA from other course or videos, I am surprised to learn new things from professor’s explanation. More importantly, most of other courses only cover PCA and do not introduce LDA. Only after this class, I finally understand that when do I need to choose PCA or LDA. Another example is backpropagation. This is my first time to learn sensitivity in backpropagation, which really help me understand more than other courses. I never heard it, although I have learned some course or read some books before. \\4. Excellent reading recommendation. Professor have kindly given us some reading list in each lecture to help us learn more things. They are really helpful and help me to understand deeply. \\5. For every lecture, professor always pointed out the “take away home message”. Personally, I really love this part. This short message really helps me to focus on the most important part and avoid loss from some relative trivial concepts. \\6. The slides are clear and well-organized. I really love this class’s slides. The have a lot of sentence and organized very well, which help to in the lecture and review. 8. Reference[1] Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., and Kislyuk, D. (2020). Toward Transformer-Based Object Detection. ArXiv:2012.09958 [Cs]. http://arxiv.org/abs/2012.09958 [2] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2), 157–166. https://doi.org/10.1109/72.279181 [3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). Language Models are Few-Shot Learners. https://arxiv.org/abs/2005.14165v4 [4] Cao, Y., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond. 0–0. https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html [5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-End Object Detection with Transformers. ArXiv:2005.12872 [Cs]. http://arxiv.org/abs/2005.12872 [6] Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. (2020). Pre-Trained Image Processing Transformer. ArXiv:2012.00364 [Cs]. http://arxiv.org/abs/2012.00364 [7] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative Pretraining From Pixels. International Conference on Machine Learning, 1691–1703. http://proceedings.mlr.press/v119/chen20s.html [8] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. ArXiv:1606.03657 [Cs, Stat]. http://arxiv.org/abs/1606.03657 [9] Cheng, J., Dong, L., and Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. ArXiv:1601.06733 [Cs]. http://arxiv.org/abs/1601.06733 [10] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248–255. [11] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv:1810.04805 [Cs]. http://arxiv.org/abs/1810.04805 [12] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ArXiv:2010.11929 [Cs]. http://arxiv.org/abs/2010.11929 [13] Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 249–256. http://proceedings.mlr.press/v9/glorot10a.html [14] Goodfellow, I. (2016). Generative Adversarial Networks (GANs). 86. [15] Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press. [16] Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in Transformer. ArXiv:2103.00112 [Cs]. http://arxiv.org/abs/2103.00112 [17] He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. ArXiv:1512.03385 [Cs]. http://arxiv.org/abs/1512.03385 [18] Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. ArXiv:1704.04861 [Cs]. http://arxiv.org/abs/1704.04861 [19] Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2017). Squeeze-and-Excitation Networks. ArXiv:1709.01507 [Cs]. http://arxiv.org/abs/1709.01507 [20] Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. ArXiv:1608.06993 [Cs]. http://arxiv.org/abs/1608.06993 [21] Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross attention for semantic segmentation. Proceedings of the IEEE/CVF International Conference on Computer Vision, 603–612. [22] Jiang, Z., Yu, W., Zhou, D., Chen, Y., Feng, J., and Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. ArXiv Preprint ArXiv:2008.02496. [23] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems 25 (pp. 1097–1105). Curran Associates, Inc. http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf [24] Kumar, M., Weissenborn, D., and Kalchbrenner, N. (2020, September 28). Colorization Transformer. International Conference on Learning Representations. https://openreview.net/forum?id=5NA1PinlGFu [25] LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation, 1(4), 541–551. https://doi.org/10.1162/neco.1989.1.4.541 [26] Li, X., Wang, W., Hu, X., and Yang, J. (2019). Selective Kernel Networks. ArXiv:1903.06586 [Cs]. http://arxiv.org/abs/1903.06586 [27] Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. (2020). Fourier Neural Operator for Parametric Partial Differential Equations. https://arxiv.org/abs/2010.08895v1 [28] Lu Chi. (2020). Fast Fourier Convolution. Neural Information Processing Systems, 767–774. https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf [29] Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. ArXiv:1508.04025 [Cs]. http://arxiv.org/abs/1508.04025 [30] Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. Proceedings of the 27th International Conference on International Conference on Machine Learning, 807–814. [31] Park, J., Woo, S., Lee, J.-Y., and Kweon, I. S. (2018). BAM: Bottleneck Attention Module. ArXiv:1807.06514 [Cs]. http://arxiv.org/abs/1807.06514 [32] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. Undefined. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe [33] Radford, Alec, Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving language understanding by generative pre-training. [34] Redmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. ArXiv:1804.02767 [Cs]. http://arxiv.org/abs/1804.02767 [35] Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. ArXiv:1506.01497 [Cs]. http://arxiv.org/abs/1506.01497 [36] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. ArXiv:1409.0575 [Cs]. http://arxiv.org/abs/1409.0575 [37] Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. ArXiv:1409.1556 [Cs]. http://arxiv.org/abs/1409.1556 [38] Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. ArXiv:2101.11605 [Cs]. http://arxiv.org/abs/2101.11605 [39] Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. ArXiv E-Prints, 1505, arXiv:1505.00387. [40] Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. ArXiv:1707.02968 [Cs]. http://arxiv.org/abs/1707.02968 [41] Sun, Z., Cao, S., Yang, Y., and Kitani, K. (2020). Rethinking Transformer-based Set Prediction for Object Detection. ArXiv:2011.10881 [Cs]. http://arxiv.org/abs/2011.10881 [42] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going Deeper with Convolutions. ArXiv:1409.4842 [Cs]. http://arxiv.org/abs/1409.4842 [43] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). Rethinking the Inception Architecture for Computer Vision. 2818–2826. http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html [44] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2021). Training data-efficient image transformers &amp; distillation through attention. ArXiv:2012.12877 [Cs]. http://arxiv.org/abs/2012.12877 [45] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Advances in Neural Information Processing Systems 30 (pp. 5998–6008). Curran Associates, Inc. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf [46] Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. (2020). MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. ArXiv:2012.00759 [Cs]. http://arxiv.org/abs/2012.00759 [47] Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L.-C. (2020). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. European Conference on Computer Vision, 108–126. [48] Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., and Cottrell, G. (2018). Understanding Convolution for Semantic Segmentation. 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), 1451–1460. https://doi.org/10.1109/WACV.2018.00163 [49] Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. (2021). Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. ArXiv:2102.12122 [Cs]. http://arxiv.org/abs/2102.12122 [50] Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local Neural Networks. ArXiv:1711.07971 [Cs]. http://arxiv.org/abs/1711.07971 [51] Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H. (2020). End-to-End Video Instance Segmentation with Transformers. ArXiv:2011.14503 [Cs]. http://arxiv.org/abs/2011.14503 [52] Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). CBAM: Convolutional Block Attention Module. 3–19. https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html [53] Yang, F., Yang, H., Fu, J., Lu, H., and Guo, B. (2020). Learning Texture Transformer Network for Image Super-Resolution. ArXiv:2006.04139 [Cs]. http://arxiv.org/abs/2006.04139 [54] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. ArXiv:2101.11986 [Cs]. http://arxiv.org/abs/2101.11986 [55] Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., and Sun, Q. (2020). Feature Pyramid Transformer. ArXiv:2007.09451 [Cs]. http://arxiv.org/abs/2007.09451 [56] Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V. (2020). Point Transformer. ArXiv:2012.09164 [Cs]. http://arxiv.org/abs/2012.09164 [57] Zheng, M., Gao, P., Wang, X., Li, H., and Dong, H. (2020). End-to-End Object Detection with Adaptive Clustering Transformer. ArXiv:2011.09315 [Cs]. http://arxiv.org/abs/2011.09315 [58] Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., and Zhang, L. (2020). Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. ArXiv:2012.15840 [Cs]. http://arxiv.org/abs/2012.15840 [59] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable DETR: Deformable Transformers for End-to-End Object Detection. ArXiv:2010.04159 [Cs]. http://arxiv.org/abs/2010.04159","categories":[{"name":"Course project","slug":"Course-project","permalink":"http://example.com/categories/Course-project/"}],"tags":[{"name":"Survey","slug":"Survey","permalink":"http://example.com/tags/Survey/"},{"name":"Vision transformer","slug":"Vision-transformer","permalink":"http://example.com/tags/Vision-transformer/"},{"name":"Computer vision","slug":"Computer-vision","permalink":"http://example.com/tags/Computer-vision/"},{"name":"Course project","slug":"Course-project","permalink":"http://example.com/tags/Course-project/"}]},{"title":"CS61B-Week2-Notes","slug":"CS61B-Week2","date":"2021-04-24T14:26:44.000Z","updated":"2021-06-20T12:54:39.093Z","comments":true,"path":"2021/04/24/CS61B-Week2/","link":"","permalink":"http://example.com/2021/04/24/CS61B-Week2/","excerpt":"Week 2 - NotesExercise B Level Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method deleteFirst, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above. Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants.","text":"Week 2 - NotesExercise B Level Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method deleteFirst, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above. Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123// Code for the question 1 and 2import edu.princeton.cs.algs4.In;//import java.util.Arrays;// Using sentinel to replace the firstpublic class SLList2 &#123; private static class IntNode &#123; public int item; public IntNode next; public IntNode(int i, IntNode n) &#123; item = i; next = n; &#125; &#125; private IntNode sentinel; private int size; public SLList2() &#123; // initialize with no inputs sentinel = new IntNode(63, null); size = 1; &#125; public SLList2(int x) &#123; // initialize with a integer sentinel = new IntNode(63, null); sentinel.next = new IntNode(x, null); size = 1; &#125; public SLList2(int[] x) &#123; // initialize with an array size = 0; sentinel = new IntNode(63, null); for (int i = 0; i &lt; x.length; i++) &#123; // get the item from array inversely sentinel.next = new IntNode(x[x.length-i-1], null); size += 1; &#125; &#125; /** Add the first item in the list */ public void addFirst(int x) &#123; sentinel.next = new IntNode(x, sentinel.next); size += 1; &#125; /** Returns the first item in the list */ public int getFirst() &#123; return sentinel.next.item; &#125; /** * Returns the last item in the list * @return the last item */ public int getLast() &#123; if (sentinel.next == null) &#123; return sentinel.item; &#125; sentinel = sentinel.next; return getLast(); &#125; /** * Add an item to a list * @param args int x */ public void addLast(int x) &#123; size += 1; IntNode p = sentinel; /* Advance p to the end of the list. */ while (p.next != null) &#123; p = p.next; &#125; p.next = new IntNode(x, null); &#125; public int size() &#123; return size; &#125; public int deleteFirst() &#123; /* sentinel.next or sentinel.next.next could be null when size == 0 */ if (sentinel.next == null) &#123; return -1; &#125; IntNode deleteNode = sentinel.next; if (sentinel.next.next == null) &#123; sentinel.next = new IntNode(-1, null); return deleteNode.item; &#125; sentinel.next = sentinel.next.next; return deleteNode.item; &#125; public static void main(String[] args) &#123; /** Test the constructor that takes in an array of integers*/ int[] arr = new int[]&#123;1,2,3&#125;; SLList2 L = new SLList2(arr); System.out.println(L.getFirst()); SLList2 L = new SLList2(1); L.addFirst(2); System.out.println(L.getFirst()); L.addFirst(3); L.deleteFirst(); System.out.print(&quot;Final: &quot;); System.out.println(L.getFirst());// L.addLast(100);// System.out.println(L.getLast());// System.out.println(L.size()); &#125;&#125; Exercise A Level Problem Link: Osmosis 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class IntList2 &#123; public int first; public IntList2 rest; public IntList2(int f, IntList2 r)&#123; first = f; rest = r; &#125; // Iterative // Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html public void addAdjacentIter(IntList2 p)&#123; /* if p == null, p.rest will no longer execute */ if (p.rest == null) &#123; /* size &lt;= 1 */ return; &#125; /** * p.rest != null * p ends at the last node finally * loop through 1st ~ last 2nd node */ while (p.rest != null) &#123; /* p ends at the last node */ if (p.first == p.rest.first) &#123; /* merge */ p.first *= 2; p.rest = p.rest.rest; /* it&#x27;s okay if it is null */ &#125; else &#123; p = p.rest; &#125; &#125; &#125; // recursion // Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html public void addAdjacentRec(IntList2 p) &#123; if (p == null) return; adj(p, p.rest); &#125; // helper function - pass previous node recursively private void adj(IntList2 prev, IntList2 current) &#123; if (current == null) return; if (prev.first == current.first) &#123; prev.first *= 2; prev.rest = current.rest; // maybe null adj(prev, prev.rest); // I fixed this part that is wrong in the reference link. &#125; else &#123; adj(current, current.rest); &#125; &#125; // Display an IntList public void display(IntList2 L) &#123; while (L.rest != null) &#123; System.out.print(L.first); System.out.print(&quot;, &quot;); L.first = L.rest.first; L.rest = L.rest.rest; &#125; System.out.println(L.first); &#125; public static void main(String[] args) &#123; IntList2 L = new IntList2(3, null); L =new IntList2(2, L); L =new IntList2(1, L); L =new IntList2(1, L); // There are two methods. // Method 1: Recursive L.addAdjacentRec(L); // Method 2: Iterative// L.addAdjacentIter(L); System.out.print(&quot;Final: &quot;); L.display(L); &#125;&#125; Reading 2.3: The DLListaddLast123456789101112public class SLList &#123; private IntNode sentinel; private IntNode last; private int size; public void addLast(int x) &#123; last.next = new IntNode(x, null); last = last.next; size += 1; &#125; ...&#125; Exercise 2.3.1: Consider the box and pointer diagram representing the SLList implementation above, which includes the last pointer. Suppose that we’d like to support addLast, getLast, and removeLast operations. Will the structure shown support rapid addLast, getLast, and removeLast operations? If not, which operations are slow? Answer 2.3.1: addLast and getLast will be fast, but removeLast will be slow. That’s because we have no easy way to get the second-to-last node, to update the last pointer, after removing the last node. Week 2 - discussion 1Implement square and squareMutative which are static methods that both take in an IntList L and return an IntList with its integer values all squared. square does this non-mutatively with recursion by creating new IntLists while squareMutative uses a recursive approach to change the instance variables of the input IntList L. 123456789public static IntList square(IntList L) &#123; if (L == null) &#123; return L; &#125; else &#123; IntList rest = square(L.rest); IntList M = new IntList(L.first * L.first, rest); return M; &#125;&#125; 123456789public static IntList squareMutative(IntList L) &#123; IntList B = L; while (B != null) &#123; B.first *= B.first; B = B.rest &#125; return L;&#125; Reference: CS 61B | Part 1 | List (Linked List &amp; Array List) CS 61B week discussion 1 solution","categories":[{"name":"Class notes","slug":"Class-notes","permalink":"http://example.com/categories/Class-notes/"}],"tags":[{"name":"Online Course","slug":"Online-Course","permalink":"http://example.com/tags/Online-Course/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://example.com/tags/Algorithm/"},{"name":"Data structure","slug":"Data-structure","permalink":"http://example.com/tags/Data-structure/"},{"name":"Notes","slug":"Notes","permalink":"http://example.com/tags/Notes/"}]},{"title":"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers","slug":"Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers","date":"2021-02-17T03:38:03.000Z","updated":"2021-02-17T13:13:40.346Z","comments":true,"path":"2021/02/17/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers/","link":"","permalink":"http://example.com/2021/02/17/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers/","excerpt":"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) SolversRecently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field. Note that please check the lastest verison in github.","text":"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) SolversRecently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field. Note that please check the lastest verison in github. Model Zoo Model Relevant Papers Link Notes HiDeNN Saha, Sourav, et al. “Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.“ Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452. Paper HiTSs Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. “Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.“ arXiv preprint arXiv:2008.09768 (2020). Paper, Code, Video Kochkov, Dmitrii, et al. “Machine learning accelerated computational fluid dynamics.“ arXiv preprint arXiv:2102.01010 (2021). Paper Google Fourier Neural Operator Li, Zongyi, et al. “Fourier neural operator for parametric partial differential equations.“ arXiv preprint arXiv:2010.08895 (2020). Paper, Code, Video PINNs Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. “Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.“ Journal of Computational Physics 378 (2019): 686-707; Paper, Code, Video Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.“ Journal of Fluid Mechanics 807 (2016): 155-166. Paper, Code Pure data K. Duraisamy, G. Iaccarino, and H. Xiao, Turbulence modeling in the age of data, Annual Review of Fluid Mechanics 51, 357 (2019). Paper Pure data, Review Maulik, Romit, et al. “Subgrid modelling for two-dimensional turbulence using neural networks.“ Journal of Fluid Mechanics 858 (2019): 122-144. Paper Beck, Andrea, David Flad, and Claus-Dieter Munz. “Deep neural networks for data-driven LES closure models.“ Journal of Computational Physics 398 (2019): 108910. Paper Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. “Deep learning for universal linear embeddings of nonlinear dynamics.“ Nature communications 9.1 (2018): 1-10. Paper Nature communications Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. “DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).“ arXiv preprint arXiv:1911.09145 (2019). Paper Um, Kiwon, et al. “Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.“ arXiv preprint arXiv:2007.00016 (2020). Paper Videos 2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations Steve Brunton: Machine Learning for Fluid Dynamics Petros Koumoutsakos: “Machine Learning for Fluid Mechanics” Blogs Fourier Neural Operator Research Groups Brunton Lab: Data-driven dynamics and control Animashree Anandkumar Wing Kam Liu Group ContactIf you like, please star or fork. Welcome any comments or feedbacks! Email: &#x78;&#105;&#97;&#111;&#x79;&#x75;&#120;&#105;&#101;&#50;&#48;&#x32;&#x30;&#64;&#117;&#x2e;&#x6e;&#111;&#114;&#x74;&#104;&#x77;&#101;&#115;&#116;&#x65;&#114;&#110;&#x2e;&#101;&#100;&#117;","categories":[{"name":"Notes","slug":"Notes","permalink":"http://example.com/categories/Notes/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://example.com/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://example.com/tags/Deep-Learning/"},{"name":"Fluid Mechanics","slug":"Fluid-Mechanics","permalink":"http://example.com/tags/Fluid-Mechanics/"},{"name":"Partial Differential Equations","slug":"Partial-Differential-Equations","permalink":"http://example.com/tags/Partial-Differential-Equations/"}]},{"title":"Applications of Machine Learning for Fluid Mechanics","slug":"notes-ML-FM","date":"2021-02-14T12:50:17.000Z","updated":"2021-02-17T13:13:26.433Z","comments":true,"path":"2021/02/14/notes-ML-FM/","link":"","permalink":"http://example.com/2021/02/14/notes-ML-FM/","excerpt":"This blog is the notes for a video called “Machine Learning for Fluid Mechanics“, which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. “Machine learning for fluid mechanics.” Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.","text":"This blog is the notes for a video called “Machine Learning for Fluid Mechanics“, which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. “Machine learning for fluid mechanics.” Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper. What is Machine Learning (ML)?ML: Models from Data via Optimization Any sufficiently advanced technology is indistinguishable from magic. – Arthur C. Clarke Fluid dynamics tasks: Reduction Modeling Control Sensing Closure Optimization problems: High-dimensional Non-linear Non-convex Multiscale What kind of ML is needed in science and engineering?We need Interpretable and Generalizable Machine Learning in science and engineering field. Everything should be made as simple as possible, but not simpler. – Albert Einstein How to build a model like $F=ma$? Features for ML in science and engineering: Sparse Low-dimensional Robust Schematic: ML + CFD Why ML could work?Because patterns exist in fluid flow. ApplicationsFluid flow decompositionPCA (Shallow, linear) -&gt; Autoencoder (Deep) Denoise for Fluid Flow Turbulence modelingPaper: Schlatter, Philipp, et al. “The structure of a turbulent boundary layer studied by numerical simulation.” arXiv preprint arXiv:1010.4000 (2010). Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. “Turbulence modeling in the age of data.” Annual Review of Fluid Mechanics 51 (2019): 357-377. ML_CFD solverPaper: Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.” Journal of Fluid Mechanics 807 (2016): 155-166. Add physical constraints and achieve accurate and pyhsical. Super-resolutionPaper: Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. “Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.” arXiv preprint arXiv:1905.10866 (2019). Interpolation and Extrapolation Solve PDEs Beyond understanding: control Inspiration from biology","categories":[{"name":"Notes","slug":"Notes","permalink":"http://example.com/categories/Notes/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://example.com/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://example.com/tags/Deep-Learning/"},{"name":"Fluid Mechanics","slug":"Fluid-Mechanics","permalink":"http://example.com/tags/Fluid-Mechanics/"}]},{"title":"Robust and Explainable Image Classification Based on Logits Kernel Density Estimation","slug":"Robust-Image-Classification-with-Kernel-Density-Function","date":"2021-01-05T10:51:00.000Z","updated":"2021-04-25T05:54:12.303Z","comments":true,"path":"2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/","link":"","permalink":"http://example.com/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/","excerpt":"Project DescriptionFor a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.","text":"Project DescriptionFor a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results. There are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are. DatasetThe cat and dog dataset used in this project is downloaded from Kaggel. Several images are shown in the below: The data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation. The batch size is 32 and the dataset will be shuﬄed. Model TrainingDetailed information about model training can be found in trainer_cat_dog.ipynb. The accuracy in the training set and the test set are 0.9809 and 0.9812 respectively. Error analysis (test set)Althought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high. True label is dog, but predicted label is cat and the score is high. The Grad-CAM results for these wrong predicted images are: We can ﬁnd that: Apparent misclassiﬁcations tend to have a larger attention area; Understandable misclassiﬁcations tend to have a smaller attention area; Distribution analysisTo solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes. Below, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project. Kernel density estimation for logits is: Analyze the robustness of classiﬁcation using logits kernel density estimationIt is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations. For the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization. If we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not “seen” these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model. In the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions. Conclusion The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; The proposed approach is explainable and can be understand visually; More resources Github Youtube Explanation Note that this project is my final project for EE475 at Northwestern University in 2020 Fall.","categories":[{"name":"Course project","slug":"Course-project","permalink":"http://example.com/categories/Course-project/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://example.com/tags/Deep-Learning/"},{"name":"Class Project","slug":"Class-Project","permalink":"http://example.com/tags/Class-Project/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://example.com/tags/Computer-Vision/"}]}],"categories":[{"name":"Class notes","slug":"Class-notes","permalink":"http://example.com/categories/Class-notes/"},{"name":"Class-notes","slug":"Class-notes","permalink":"http://example.com/categories/Class-notes/"},{"name":"Course project","slug":"Course-project","permalink":"http://example.com/categories/Course-project/"},{"name":"Notes","slug":"Notes","permalink":"http://example.com/categories/Notes/"}],"tags":[{"name":"Notes","slug":"Notes","permalink":"http://example.com/tags/Notes/"},{"name":"Online course","slug":"Online-course","permalink":"http://example.com/tags/Online-course/"},{"name":"GANs","slug":"GANs","permalink":"http://example.com/tags/GANs/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://example.com/tags/Deep-Learning/"},{"name":"Survey","slug":"Survey","permalink":"http://example.com/tags/Survey/"},{"name":"Vision transformer","slug":"Vision-transformer","permalink":"http://example.com/tags/Vision-transformer/"},{"name":"Computer vision","slug":"Computer-vision","permalink":"http://example.com/tags/Computer-vision/"},{"name":"Course project","slug":"Course-project","permalink":"http://example.com/tags/Course-project/"},{"name":"Online Course","slug":"Online-Course","permalink":"http://example.com/tags/Online-Course/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://example.com/tags/Algorithm/"},{"name":"Data structure","slug":"Data-structure","permalink":"http://example.com/tags/Data-structure/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://example.com/tags/Machine-Learning/"},{"name":"Fluid Mechanics","slug":"Fluid-Mechanics","permalink":"http://example.com/tags/Fluid-Mechanics/"},{"name":"Partial Differential Equations","slug":"Partial-Differential-Equations","permalink":"http://example.com/tags/Partial-Differential-Equations/"},{"name":"Class Project","slug":"Class-Project","permalink":"http://example.com/tags/Class-Project/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://example.com/tags/Computer-Vision/"}]}