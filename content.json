{"meta":{"title":"Xiaoyu Xie's Blog","subtitle":"Machine Learning, Deep Learning, Computer Vision","description":"Track The past. Organize the present. Design the future.","author":"Xiaoyu Xie","url":"http://example.com","root":"/"},"pages":[{"title":"Xiaoyu Xie","date":"2021-02-09T01:03:29.308Z","updated":"2021-02-09T01:03:29.300Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"BackgroundI am a PhD student at Northwestern University. Research Interests: Bayesian Deep Learning; Computer Vision; Deep Learning in Manufacturing, Mechanics, and Finace;"},{"title":"categories","date":"2021-01-07T04:21:57.215Z","updated":"2021-01-07T04:21:57.207Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Reading","date":"2021-01-05T11:27:50.020Z","updated":"2021-01-05T11:27:50.007Z","comments":true,"path":"reading/index.html","permalink":"http://example.com/reading/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-01-07T04:08:10.118Z","updated":"2021-01-07T04:08:10.112Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers","slug":"Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers","date":"2021-02-17T03:38:03.000Z","updated":"2021-02-17T13:13:40.346Z","comments":true,"path":"2021/02/17/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers/","link":"","permalink":"http://example.com/2021/02/17/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers/","excerpt":"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) SolversRecently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field. Note that please check the lastest verison in github.","text":"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) SolversRecently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field. Note that please check the lastest verison in github. Model Zoo Model Relevant Papers Link Notes HiDeNN Saha, Sourav, et al. “Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.“ Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452. Paper HiTSs Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. “Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.“ arXiv preprint arXiv:2008.09768 (2020). Paper, Code, Video Kochkov, Dmitrii, et al. “Machine learning accelerated computational fluid dynamics.“ arXiv preprint arXiv:2102.01010 (2021). Paper Google Fourier Neural Operator Li, Zongyi, et al. “Fourier neural operator for parametric partial differential equations.“ arXiv preprint arXiv:2010.08895 (2020). Paper, Code, Video PINNs Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. “Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.“ Journal of Computational Physics 378 (2019): 686-707; Paper, Code, Video Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.“ Journal of Fluid Mechanics 807 (2016): 155-166. Paper, Code Pure data K. Duraisamy, G. Iaccarino, and H. Xiao, Turbulence modeling in the age of data, Annual Review of Fluid Mechanics 51, 357 (2019). Paper Pure data, Review Maulik, Romit, et al. “Subgrid modelling for two-dimensional turbulence using neural networks.“ Journal of Fluid Mechanics 858 (2019): 122-144. Paper Beck, Andrea, David Flad, and Claus-Dieter Munz. “Deep neural networks for data-driven LES closure models.“ Journal of Computational Physics 398 (2019): 108910. Paper Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. “Deep learning for universal linear embeddings of nonlinear dynamics.“ Nature communications 9.1 (2018): 1-10. Paper Nature communications Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. “DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).“ arXiv preprint arXiv:1911.09145 (2019). Paper Um, Kiwon, et al. “Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.“ arXiv preprint arXiv:2007.00016 (2020). Paper Videos 2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations Steve Brunton: Machine Learning for Fluid Dynamics Petros Koumoutsakos: “Machine Learning for Fluid Mechanics” Blogs Fourier Neural Operator Research Groups Brunton Lab: Data-driven dynamics and control Animashree Anandkumar Wing Kam Liu Group ContactIf you like, please star or fork. Welcome any comments or feedbacks! Email: &#x78;&#105;&#x61;&#111;&#x79;&#x75;&#120;&#105;&#101;&#x32;&#x30;&#50;&#48;&#x40;&#x75;&#46;&#x6e;&#111;&#x72;&#116;&#104;&#119;&#x65;&#x73;&#116;&#x65;&#x72;&#110;&#46;&#101;&#x64;&#117;","categories":[{"name":"Notes","slug":"Notes","permalink":"http://example.com/categories/Notes/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://example.com/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://example.com/tags/Deep-Learning/"},{"name":"Fluid Mechanics","slug":"Fluid-Mechanics","permalink":"http://example.com/tags/Fluid-Mechanics/"},{"name":"Partial Differential Equations","slug":"Partial-Differential-Equations","permalink":"http://example.com/tags/Partial-Differential-Equations/"}]},{"title":"Applications of Machine Learning for Fluid Mechanics","slug":"notes-ML-FM","date":"2021-02-14T12:50:17.000Z","updated":"2021-02-17T13:13:26.433Z","comments":true,"path":"2021/02/14/notes-ML-FM/","link":"","permalink":"http://example.com/2021/02/14/notes-ML-FM/","excerpt":"This blog is the notes for a video called “Machine Learning for Fluid Mechanics“, which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. “Machine learning for fluid mechanics.” Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.","text":"This blog is the notes for a video called “Machine Learning for Fluid Mechanics“, which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. “Machine learning for fluid mechanics.” Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper. What is Machine Learning (ML)?ML: Models from Data via Optimization Any sufficiently advanced technology is indistinguishable from magic. – Arthur C. Clarke Fluid dynamics tasks: Reduction Modeling Control Sensing Closure Optimization problems: High-dimensional Non-linear Non-convex Multiscale What kind of ML is needed in science and engineering?We need Interpretable and Generalizable Machine Learning in science and engineering field. Everything should be made as simple as possible, but not simpler. – Albert Einstein How to build a model like $F=ma$? Features for ML in science and engineering: Sparse Low-dimensional Robust Schematic: ML + CFD Why ML could work?Because patterns exist in fluid flow. ApplicationsFluid flow decompositionPCA (Shallow, linear) -&gt; Autoencoder (Deep) Denoise for Fluid Flow Turbulence modelingPaper: Schlatter, Philipp, et al. “The structure of a turbulent boundary layer studied by numerical simulation.” arXiv preprint arXiv:1010.4000 (2010). Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. “Turbulence modeling in the age of data.” Annual Review of Fluid Mechanics 51 (2019): 357-377. ML_CFD solverPaper: Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.” Journal of Fluid Mechanics 807 (2016): 155-166. Add physical constraints and achieve accurate and pyhsical. Super-resolutionPaper: Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. “Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.” arXiv preprint arXiv:1905.10866 (2019). Interpolation and Extrapolation Solve PDEs Beyond understanding: control Inspiration from biology","categories":[{"name":"Notes","slug":"Notes","permalink":"http://example.com/categories/Notes/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://example.com/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://example.com/tags/Deep-Learning/"},{"name":"Fluid Mechanics","slug":"Fluid-Mechanics","permalink":"http://example.com/tags/Fluid-Mechanics/"}]},{"title":"Robust and Explainable Image Classification Based on Logits Kernel Density Estimation","slug":"Robust-Image-Classification-with-Kernel-Density-Function","date":"2021-01-05T10:51:00.000Z","updated":"2021-01-07T04:16:56.724Z","comments":true,"path":"2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/","link":"","permalink":"http://example.com/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/","excerpt":"Project DescriptionFor a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.","text":"Project DescriptionFor a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results. There are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are. DatasetThe cat and dog dataset used in this project is downloaded from Kaggel. Several images are shown in the below: The data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation. The batch size is 32 and the dataset will be shuﬄed. Model TrainingDetailed information about model training can be found in trainer_cat_dog.ipynb. The accuracy in the training set and the test set are 0.9809 and 0.9812 respectively. Error analysis (test set)Althought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high. True label is dog, but predicted label is cat and the score is high. The Grad-CAM results for these wrong predicted images are: We can ﬁnd that: Apparent misclassiﬁcations tend to have a larger attention area; Understandable misclassiﬁcations tend to have a smaller attention area; Distribution analysisTo solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes. Below, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project. Kernel density estimation for logits is: Analyze the robustness of classiﬁcation using logits kernel density estimationIt is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations. For the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization. If we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not “seen” these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model. In the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions. Conclusion The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; The proposed approach is explainable and can be understand visually; More resources Github Youtube Explanation Note that this project is my final project for EE475 at Northwestern University in 2020 Fall.","categories":[{"name":"Research","slug":"Research","permalink":"http://example.com/categories/Research/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://example.com/tags/Deep-Learning/"},{"name":"Class Project","slug":"Class-Project","permalink":"http://example.com/tags/Class-Project/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://example.com/tags/Computer-Vision/"}]}],"categories":[{"name":"Notes","slug":"Notes","permalink":"http://example.com/categories/Notes/"},{"name":"Research","slug":"Research","permalink":"http://example.com/categories/Research/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://example.com/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://example.com/tags/Deep-Learning/"},{"name":"Fluid Mechanics","slug":"Fluid-Mechanics","permalink":"http://example.com/tags/Fluid-Mechanics/"},{"name":"Partial Differential Equations","slug":"Partial-Differential-Equations","permalink":"http://example.com/tags/Partial-Differential-Equations/"},{"name":"Class Project","slug":"Class-Project","permalink":"http://example.com/tags/Class-Project/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://example.com/tags/Computer-Vision/"}]}}]}